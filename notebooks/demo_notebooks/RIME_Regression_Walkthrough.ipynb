{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Robust Intelligence Logo](https://www.dropbox.com/s/jzx24dtt6d1wdwz/RI%20Logo%20Stacked%20-%20White%20Transparent.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2juKKWiBUqM3"
   },
   "source": [
    "# RIME: NYC Taxi and Limousine Data Walkthrough üöñ\n",
    "In this walkthrough, we'll run AI Stress Testing and AI Continuous Testing on public NYC Taxi and Limousine Commission data (https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page) to demonstrate how RIME can be used with regression models. This data consists of information such as the pickup and dropoff locations, pickup and dropoff times, fare amounts and the number of passengers for every taxi trip that happens in New York City. We'll be predicting the duration of each trip given a bunch of other information about the trip.\n",
    "\n",
    "As you might imagine, the COVID-19 pandemic caused a significant change in the number and nature of the taxi rides that occur in New York City. We've included data from 2018 to 2021 for this walkthrough to demonstrate how AI Continuous Testing can help you identify and understand such distribution drifts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHB5JnkFNdOZ"
   },
   "source": [
    "To get started, provide the API credentials and link to the backend of RIME to connect the instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nqhw1HbENlgz"
   },
   "outputs": [],
   "source": [
    "API_TOKEN = '' # PASTE API_KEY RECIEVED IN EMAIL\n",
    "CLUSTER_URL = '' # PASTE DEDICATED BACKEND ENDPOINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZXhwB6bMCDt"
   },
   "source": [
    "## Libraries üìï\n",
    "Run the cell below to install libraries to receive data, install our SDK, and load analysis libraries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAlM-eMxL9W5"
   },
   "outputs": [],
   "source": [
    "!pip install rime-sdk~=2.0.0b &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAlM-eMxL9W5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from rime_sdk import Client "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kN0xMjsRMlkx"
   },
   "source": [
    "## Data and Model ‚òÅÔ∏è\n",
    "Run the cell below to download and unzip a preprocessed dataset and pretrained model based on public NYC Taxi and Limousine Commission data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/RobustIntelligence/ri-public-examples.git\n",
    "from ri_public_examples.download_files import download_files\n",
    "download_files('tabular/nyc_tlc', 'nyc_tlc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sep_preds(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df.drop(\"Prediction\", axis=1).to_csv(path, index=False)\n",
    "    preds_path = path.replace(\".csv\", \"_preds.csv\")\n",
    "    df[\"Prediction\"].to_csv(preds_path, index=False)\n",
    "    \n",
    "for dataset in [\"ref\", \"eval\", \"test\"]:\n",
    "    path = f\"nyc_tlc/data/{dataset}.csv\"\n",
    "    sep_preds(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s59pzJdnbFBQ"
   },
   "source": [
    "Next, let's take a quick look at the reference data (in this case, this was the data used to train the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "id": "mWr4uIoNau8N",
    "outputId": "47944289-584a-41d1-9597-a3e371b2d743"
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\"nyc_tlc/data/ref.csv\", nrows=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RjdT3Jv-b6U9"
   },
   "source": [
    "The key columns to look at above are the `TripDuration`, the duration of the trip in seconds, and `Prediction`, our model's estimate of the duration of the trip. The other columns are features used by the model to help predict the trip duration. We'll now proceed to run RIME Stress Testing on our data and model! We'll start by creating a project and uploading our datasets and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(CLUSTER_URL, API_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y0E0wSyHMhMj",
    "outputId": "24123ad6-eb99-4541-9975-8077ccaef9c3"
   },
   "outputs": [],
   "source": [
    "description = (\n",
    "    \"Run Stress Testing, Continuous Testing and AI Firewall on a\"\n",
    "    \" tabular regression model and dataset. Demonstration uses the\"\n",
    "    \" NYC Taxi and Limousine Commission trip duration dataset\"\n",
    "    \" (https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page).\"\n",
    ")\n",
    "project = client.create_project(\n",
    "    'Tabular Regression Demo', \n",
    "    description,\n",
    "    \"MODEL_TASK_REGRESSION\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Note: All registered models and datasets need to have unique names.\n",
    "dt = str(datetime.now())\n",
    "upload_path = \"ri_public_examples_nyc_tlc\"\n",
    "\n",
    "model_s3_path = client.upload_directory(Path(\"nyc_tlc/models\"), upload_path=upload_path)\n",
    "model_id = project.register_model_from_path(f\"model_{dt}\", model_s3_path + \"/model.py\")\n",
    "\n",
    "def upload_and_register_data(dataset, **kwargs):\n",
    "    s3_path = client.upload_file(Path(f\"nyc_tlc/data/{dataset}.csv\"), upload_path=upload_path)\n",
    "    dataset_id = project.register_dataset_from_file(f\"{dataset}_{dt}\", s3_path, {\"label_col\": \"TripDuration\", **kwargs})\n",
    "    preds_s3_path = client.upload_file(Path(f\"nyc_tlc/data/{dataset}_preds.csv\"), upload_path=upload_path)\n",
    "    project.register_predictions_from_file(dataset_id, model_id, preds_s3_path)\n",
    "    return dataset_id\n",
    "\n",
    "ref_id = upload_and_register_data(\"ref\")\n",
    "eval_id = upload_and_register_data(\"eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Stress Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lf1ylzMJGKJ0"
   },
   "source": [
    "Next, we'll create a stress testing configuration specifying relevant metadata for our datasets and model and run stress testing! When running stress testing, the reference data should be data used to train the model, and the evaluation data should be data used to evaluate the model. In this case, the reference and evaluation datasets are random splits of the NYC TLC data collected from 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YA4OxHAiO9g7",
    "outputId": "e768bc61-8aeb-460e-8dec-497f84ffe6ba"
   },
   "outputs": [],
   "source": [
    "stress_test_config = {\n",
    "  \"run_name\": \"NYC TLC\",\n",
    "  \"data_info\": {\n",
    "    \"ref_dataset_id\": ref_id,\n",
    "    \"eval_dataset_id\": eval_id,\n",
    "  },\n",
    "  \"model_id\": model_id\n",
    "}\n",
    "stress_test_job = client.start_stress_test(\n",
    "    stress_test_config, \n",
    "    project.project_id,\n",
    ")\n",
    "stress_test_job.get_status(verbose=True, wait_until_finish=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view the detailed results in the UI by running the below cell and redirecting to the generated link. This page shows granular results for a given AI Stress Test run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run = stress_test_job.get_test_run()\n",
    "test_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_lMRyPxQJX7"
   },
   "source": [
    "Stress testing should be used during model development to inform us about various issues with the data and model that we might want to address before the model is deployed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nidRlmx8Po9G"
   },
   "source": [
    "![img_1](https://drive.google.com/uc?id=1fVdk5_KJzMXNT1YLsmCfu_V2UQqahFmD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Firewall and Continuous Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7WJksWqRFge"
   },
   "source": [
    "In this walkthrough, we'll be focusing on the _production_ setting where we've deployed a model and would like to ensure that it continues to perform well as the underlying data drifts and evolves. For this we'll need to create an AI Firewall that will allow us to perform AI Continuous Testing. Run the following snippet to create an AI Firewall and set up continuous testing to split the data into 4 week bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "firewall = project.create_firewall(model_id, ref_id, timedelta(weeks=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRgljOaeRa5X"
   },
   "source": [
    "Our firewall is ready to go! Next, we'll upload some incoming production data. The data we're uploading here is from 2019 through to 2021, which will look substantially different from what the model saw in its training data from 2018 (the data will be automatically split into pieces based on the timestamps specified and the bin size set for this AI Firewall). We'll use AI Continuous Testing to identify the differences and understand how they're impacting our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = upload_and_register_data(\"test\", timestamp_col=\"PickupDatetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jUD1ZBNyg0LM",
    "outputId": "26393984-96d0-42d0-898c-432f9e5479d8"
   },
   "outputs": [],
   "source": [
    "ct_job = firewall.start_continuous_test(test_id)\n",
    "ct_job.get_status(verbose=True, wait_until_finish=True)\n",
    "firewall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFkrep_aUswK"
   },
   "source": [
    "Time to see how our model is doing! Navigate to the \"Continuous Tests\" tab on the left nav. Here, you'll see some of the key metrics that are being tracked over time along with active monitoring alerts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_f7cTHfWcEv"
   },
   "source": [
    "![img_2](https://drive.google.com/uc?id=1wdr1UFy6CN7RR0UCGp9MUQJNcZ1mREqe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZV8fW1Ywb3_v"
   },
   "source": [
    "If we head to the \"Operational Risk\" page we can see all of the metrics and alerting related to the operational health of our model and data pipelines. In the \"Model Performance\" category (available when labels are provided) we can take a look at the Mean Absolute Error plotted over time and we see a significant increase in this error around the spring of 2020 when the COVID-19 pandemic caused a substantial decrease in the number of taxi rides in New York City. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbzPy5MQfJvt"
   },
   "source": [
    "![img_5](https://drive.google.com/uc?id=1Pnm_rRvaICRn2j3htbhIC1KrnBLBHb_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If labels weren't available (as is often the case with production data) we could look at changes in prediction distributions as a proxy for changes in model performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSFyoP4bfZN3"
   },
   "source": [
    "![img_6](https://drive.google.com/uc?id=10r8M99urYXRlao0gFVT8Fk0cFAMaIOht)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N939LDZ_fmna"
   },
   "source": [
    "We can also inspect the input data for abnormal values. The overall abnormality rate shows the percent of abnormal inputs (for all types of abnormalities including outliers, missing values, unseen categories) over time. We can see that there's a spike in the abnormality rate in November of 2019 after which it remains high. Further exploration reveals that much of this comes from Numeric Outlier values in the `TripDistance` feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![abnormality_rate](https://drive.google.com/uc?id=19sAuug-L5aJv2xlhpQF2CNvYRs7-aTiX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxApkNpRj7tN"
   },
   "source": [
    "![img_7](https://drive.google.com/uc?id=14GKLjAxr0N1wSVMUVNhZU_SQUN5cFVVV)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RIME_Regression_Walkthrough.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "39-venv",
   "language": "python",
   "name": "39-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
