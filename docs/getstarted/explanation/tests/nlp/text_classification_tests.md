# Text Classification Tests
## Adversarial

### Invisible Character Attack
<p>This test measures the robustness of your model to invisible character attacks. It does this by taking a sample input, inserting zero-width unicode characters, and measuring the performance of the model on the perturbed input. See the paper  "Fall of Giants: How Popular Text-Based MLaaS Fall against a Simple Evasion Attack" by Pajola and Conti (https://arxiv.org/abs/2104.05996) for more details.</p><p><b>Why it matters:</b> Malicious actors can perturb natural language input sequences to  alter model behavior in unexpected ways. It is important that your NLP models are robust to such attacks.</p><p><b>Configuration:</b> By default, this test runs in adversarial mode.</p><p><b>Example:</b> Given the input sequence "RIME is helpful.", this test measures the performance of the model when imperceptibly perturbed (e.g.,  when changed to "RIM‌E is hel​p‍ful.")</p>

### Deletion Control Character Attack
<p>This test measures the robustness of your model to deletion control character attacks. It does this by taking a sample input, inserting deletion control characters, and measuring the performance of the model on the perturbed input. See the paper  "Bad Characters: Imperceptible NLP Attacks" by Boucher, Shumailov, et al. (https://arxiv.org/abs/2106.09898) for more details.</p><p><b>Why it matters:</b> Malicious actors can perturb natural language input sequences to  alter model behavior in unexpected ways. It is important that your NLP models are robust to such attacks.</p><p><b>Configuration:</b> By default, this test runs in adversarial mode.</p><p><b>Example:</b> Given the input sequence "RIME is helpful.", this test measures the performance of the model when imperceptibly perturbed (e.g.,  when changed to "RIM‌E is hel​p‍ful.")</p>

### Intentional Homoglyph Attack
<p>This test measures the robustness of your model to intentional homoglyph attacks. It does this by taking a sample input, substituting homoglyphs designed to look like other characters, and measuring the performance of the model on the perturbed input. See the paper  "Bad Characters: Imperceptible NLP Attacks" by Boucher, Shumailov, et al. (https://arxiv.org/abs/2106.09898) for more details.</p><p><b>Why it matters:</b> Malicious actors can perturb natural language input sequences to  alter model behavior in unexpected ways. It is important that your NLP models are robust to such attacks.</p><p><b>Configuration:</b> By default, this test runs in adversarial mode.</p><p><b>Example:</b> Given the input sequence "RIME is helpful.", this test measures the performance of the model when imperceptibly perturbed (e.g.,  when changed to "RIM‌E is hel​p‍ful.")</p>

### Confusable Homoglyph Attack
<p>This test measures the robustness of your model to confusable homoglyph attacks. It does this by taking a sample input, substituting homoglyphs that are easily confused with other characters, and measuring the performance of the model on the perturbed input. See the paper  "Bad Characters: Imperceptible NLP Attacks" by Boucher, Shumailov, et al. (https://arxiv.org/abs/2106.09898) for more details.</p><p><b>Why it matters:</b> Malicious actors can perturb natural language input sequences to  alter model behavior in unexpected ways. It is important that your NLP models are robust to such attacks.</p><p><b>Configuration:</b> By default, this test runs in adversarial mode.</p><p><b>Example:</b> Given the input sequence "RIME is helpful.", this test measures the performance of the model when imperceptibly perturbed (e.g.,  when changed to "RIM‌E is hel​p‍ful.")</p>

### Character Substitution
<p>This test measures the robustness of your model to character substitution attacks. It does this by randomly substituting characters in the input string and measuring your model's performance on the attacked string.</p><p><b>Why it matters:</b> Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p><p><b>Configuration:</b> By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p><p><b>Example:</b> Given an input sequence "The quick brown fox jumped over the lazy dog", this test measures the performance of the model when given the attacked input of "Tie quick brorn fox tumped over the lyzy dog".</p>

### Character Deletion
<p>This test measures the robustness of your model to character deletion attacks. It does this by randomly deleting characters in the input string and measuring your model's performance on the attacked string.</p><p><b>Why it matters:</b> Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p><p><b>Configuration:</b> By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p><p><b>Example:</b> Given an input sequence "The quick brown fox jumped over the lazy dog", this test measures the performance of the model when given the attacked input of "Th quick brwn fox jumpd over the lazy dog".</p>

### Character Insertion
<p>This test measures the robustness of your model to character insertion attacks. It does this by randomly adding characters to the input string and measuring your model's performance on the attacked string.</p><p><b>Why it matters:</b> Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p><p><b>Configuration:</b> By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p><p><b>Example:</b> Given an input sequence "The quick brown fox jumped over the lazy dog", this test measures the performance of the model when given the attacked input of "Thew quick broqwn fox jumqped over the lazy dog".</p>

### Character Swap
<p>This test measures the robustness of your model to character swap attacks. It does this by randomly swapping characters in the input string and measuring your model's performance on the attacked string.</p><p><b>Why it matters:</b> Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p><p><b>Configuration:</b> By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p><p><b>Example:</b> Given an input sequence "The quick brown fox jumped over the lazy dog", this test measures the performance of the model when given the attacked input of "Teh quick bornw fox ujmpde over the lazy dog".</p>

### Keyboard Augmentation
<p>This test measures the robustness of your model to keyboard augmentation attacks. It does this by adding common typos based on keyboard distance to the input string and measuring your model's performance on the attacked string.</p><p><b>Why it matters:</b> Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p><p><b>Configuration:</b> By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p><p><b>Example:</b> Given an input sequence "The quick brown fox jumped over the lazy dog", this test measures the performance of the model when given the attacked input of "Thr quick browb fox jumled over the lazy dog".</p>

### Common Misspellings
<p>This test measures the robustness of your model to common misspellings attacks. It does this by adding common misspellings to the input string and measuring your model's performance on the attacked string.</p><p><b>Why it matters:</b> Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p><p><b>Configuration:</b> By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p><p><b>Example:</b> Given an input sequence "The quick brown fox jumped over the lazy dog", this test measures the performance of the model when given the attacked input of "Thee quik brown focks jumped over the lasy dog".</p>

### OCR Error Simulation
<p>This test measures the robustness of your model to ocr error simulation attacks. It does this by adding common OCR errors to the input string and measuring your model's performance on the attacked string.</p><p><b>Why it matters:</b> Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p><p><b>Configuration:</b> By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p><p><b>Example:</b> Given an input sequence "The quick brown fox jumped over the lazy dog", this test measures the performance of the model when given the attacked input of "Th3 quick br0wn fox jumped over the 1azy d0g".</p>

### Synonym Swap
<p>This test measures the robustness of your model to synonym swap attacks. It does this by randomly swapping synonyms in the input string and measuring your model's performance on the attacked string.</p><p><b>Why it matters:</b> Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p><p><b>Configuration:</b> By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p><p><b>Example:</b> Given an input sequence "The quick brown fox jumped over the lazy dog", this test measures the performance of the model when given the attacked input of "The fast brown fox leaped over the lazy dog".</p>

### Contextual Word Swap
<p>This test measures the robustness of your model to contextual word swap attacks. It does this by replacing words with those close in embedding space and measuring your model's performance on the attacked string.</p><p><b>Why it matters:</b> Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p><p><b>Configuration:</b> By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p><p><b>Example:</b> Given an input sequence "The quick brown fox jumped over the lazy dog", this test measures the performance of the model when given the attacked input of "the fast brown pigeon leaped over the white dog".</p>

### Contextual Word Insertion
<p>This test measures the robustness of your model to contextual word insertion attacks. It does this by inserting words generated from a language model and measuring your model's performance on the attacked string.</p><p><b>Why it matters:</b> Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p><p><b>Configuration:</b> By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p><p><b>Example:</b> Given an input sequence "The quick brown fox jumped over the lazy dog", this test measures the performance of the model when given the attacked input of "the fast brown fox leaped away over the lazy dog".</p>

### Universal Prefix Attack
<p>This test measures the robustness of your model to 'universal' adversarial prefix injections. It does this by sampling a batch of inputs, and searching over the model vocabulary to find a prefix that is nonsensical to a reader but that, when prepended to the batch of inputs, will cause the model to output a different prediction. See the paper  "Universal Adversarial Triggers for Attacking and Analyzing NLP" by Wallace, Feng, Kandpal, et al. (https://arxiv.org/abs/1908.07125) for more details.</p><p><b>Why it matters:</b> Malicious actors can perturb natural language input sequences to  alter model behavior in unexpected ways. 'Universal triggers'  pose a particularly large threat since they easily transfer between models and data points to permit an adversary to make large-scale, cost-efficient attacks. It is important that your NLP models are robust to such threat vectors.</p><p><b>Configuration:</b> By default, this test runs when the 'Adversarial' category is specified.</p><p><b>Example:</b> Given a target class of 0, this test selects a batch of inputs for which the model predicts a different class (e.g., 1). It then searches for an adversarial prefix that maximizes the probability assigned to the target class. The severity of this test is based on the difference in the average probability assigned to the target class before and after the prefix is prepended to the batch. For instance, given two inputs "I am happy!" and "I like ice cream!", the attack finds an example prefix, e.g., "the why y could", and measures the new probability assigned by the model to the target class for inputs "the why y could I am happy!" and "the why y could I like ice cream!".</p>

## Abnormal Inputs

### Numeric Outliers
<p>This test measures the number of failing rows in your data with outliers and their impact on the model. Outliers are values which may not necessarily be outside of an allowed range for a feature, but are extreme values that are unusual and may be indicative of abnormality. The model impact is the difference in model performance between passing and failing rows with outliers. If labels are not provided, prediction change is used instead of model performance change.</p><p><b>Why it matters:</b> Outliers can be a sign of corrupted or otherwise erroneous data, and can degrade model performance if used in the training data, or lead to unexpected behaviour if input at inference time.</p><p><b>Configuration:</b> By default this test is run over each numeric feature that is neither unique nor ascending.</p><p><b>Example:</b> Suppose there is a feature <span>age</span> for which in the reference set the values <span>103</span> and <span>114</span> each appear once but every other value (with substantial sample size) is contained within the range <span>[0, 97]</span>. Then we would infer a lower outlier threshold of <span>0</span> and an upper outlier threshold of <span>97</span>. This test raises a warning if we observe any values in the evaluation set outside these thresholds or if model performance decreases on observed datapoints with outliers.</p>

### Unseen Categorical
<p>This test measures the number of failing rows in your data with unseen categorical values and their impact on the model. The model impact is the difference in model performance between passing and failing rows with unseen categorical values. If labels are not provided, prediction change is used instead of model performance change.</p><p><b>Why it matters:</b> Unseen categorical values are a common failure point in machine learning systems; since these models are trained over a reference set, they may yield uninterpretable or undefined behavior when interacting with an unseen categorical value. In addition, such errors may expose gaps or errors in data collection.</p><p><b>Configuration:</b> By default, this test runs over all categorical features.</p><p><b>Example:</b> Say that the feature <span>Animal</span> contains the values <span>['Cat', 'Dog']</span> from the reference set. This test raises a warning if we observe any unseen values in the evaluation set such as <span>'Mouse'</span> that causes a significant change in model performance. If labels/predictions are provided in the run, then a severity would be raised if the Average Prediction changed by 0.03. If labels/predictions were not provided but <span>'Mouse'</span> appeared in 3% of the evaluation dataset, an severity would be raised due to the significant increase in presence of an unseen feature.</p>

### Unseen Domain
<p>This test measures the number of failing rows in your data with unseen domain values and their impact on the model. The model impact is the difference in model performance between passing and failing rows with unseen domain values. If labels are not provided, prediction change is used instead of model performance change.</p><p><b>Why it matters:</b> Unseen categorical values are a common failure point in machine learning systems; since these models are trained over a reference set, they may yield uninterpretable or undefined behavior when interacting with an unseen categorical value. In addition, such errors may expose gaps or errors in data collection.</p><p><b>Configuration:</b> By default, this test runs over all features inferred to contain domains.</p><p><b>Example:</b> Say that the feature <span>WebDomain</span> contains the values <span>['gmail.com', 'hotmail.com']</span> from the reference set. This test raises a warning if we observe any unseen values in the evaluation set such as <span>'xyzabc.com'</span> that causes a significant change in model performance. If labels/predictions are provided in the run, then a severity issue would be raised if the Average Prediction changed by 0.03. If labels/predictions were not provided but <span>'xyzabc.com'</span> appeared in 3% of the evaluation dataset, an severity issue would be raised due to the significant increase in presence of an unseen feature.</p>

### Unseen Email
<p>This test measures the number of failing rows in your data with unseen email values and their impact on the model. The model impact is the difference in model performance between passing and failing rows with unseen email values. If labels are not provided, prediction change is used instead of model performance change.</p><p><b>Why it matters:</b> Unseen categorical values are a common failure point in machine learning systems; since these models are trained over a reference set, they may yield uninterpretable or undefined behavior when interacting with an unseen categorical value. In addition, such errors may expose gaps or errors in data collection.</p><p><b>Configuration:</b> By default, this test runs over all features inferred to contain emails.</p><p><b>Example:</b> Say that the feature <span>Email</span> contains the values <span>['user1@gmail.com', 'user2@yahoo.com']</span> from the reference set. This test raises a warning if we observe any unseen values in the evaluation set such as <span>'xyz@xyzabc.com'</span> that causes a significant change in model performance. If labels/predictions are provided in the run, then a severity issue would be raised if the Average Prediction changed by 0.03. If labels/predictions were not provided but <span>'xyz@xyzabc.com'</span> appeared in 3% of the evaluation dataset, a severity issue would be raised due to the significant increase in presence of an unseen feature.</p>

### Unseen URL
<p>This test measures the number of failing rows in your data with unseen URL values and their impact on the model. The model impact is the difference in model performance between passing and failing rows with unseen URL values. If labels are not provided, prediction change is used instead of model performance change.</p><p><b>Why it matters:</b> Unseen categorical values are a common failure point in machine learning systems; since these models are trained over a reference set, they may yield uninterpretable or undefined behavior when interacting with an unseen categorical value. In addition, such errors may expose gaps or errors in data collection.</p><p><b>Configuration:</b> By default, this test runs over all features inferred to contain URLs.</p><p><b>Example:</b> Say that the feature <span>WebURL</span> contains the values <span>['http://google.com', 'http://yahoo.com']</span> from the reference set. This test raises a warning if we observe any unseen values in the evaluation set such as <span>'http://xyzabc.com'</span> that causes a significant change in model performance. If labels/predictions are provided in the run, then a severity issue would be raised if the Average Prediction changed by 0.03. If labels/predictions were not provided but <span>'xyzabc.com'</span> appeared in 3% of the evaluation dataset, an severity issue would be raised due to the significant increase in presence of an unseen feature.</p>

### Rare Categories
<p>This test measures the severity of passing to the model data points whose features contain rarely observed categories (relative to the reference set). The severity is a function of the impact of these values on the model, as well as the presence of these values in the data. The model impact is the difference in model performance between passing and failing rows with rarely observed categorical values. If labels are not provided, prediction change is used instead of model performance change. The number of failing rows refers to the number of times rarely observed categorical values are observed in the evaluation set.</p><p><b>Why it matters:</b> Rare categories are a common failure point in machine learning systems because less data often means worse performance. In addition, this may expose gaps or errors in data collection.</p><p><b>Configuration:</b> By default, this test runs over all categorical features. A category is considered rare if it occurs fewer than <span>min_num_occurrences</span> times, or if it occurs less than <span>min_pct_occurrences</span> of the time. If neither of these values are specified, the rate of appearance below which a category is considered rare is <span>min_ratio_rel_uniform</span> divided by the number of classes.</p><p><b>Example:</b> Say that the feature <span>AgeGroup</span> takes on the value <span>0-18</span> twice while taking on the value <span>35-55</span> a total of <span>98</span> times. If the <span>min_num_occurences</span> is <span>5</span> and the <span>min_pct_occurrences</span> is <span>0.03</span> then the test will flag the value <span>0-18</span> as a rare category.</p>

### Out of Range
<p>This test measures the number of failing rows in your data with values outside the inferred range of allowed values and their impact on the model. The model impact is the difference in model performance between passing and failing rows with values outside the inferred range of allowed values. If labels are not provided, prediction change is used instead of model performance change.</p><p><b>Why it matters:</b> In production, the model may encounter corrupted or manipulated out of range values. It is important that the model is robust to such extremities.</p><p><b>Configuration:</b> By default, this test runs over all numeric features.</p><p><b>Example:</b> In the reference set, the <span>Age</span> feature has a range of <span>[0, 121]</span>. This test raises a warning if we observe values outside of this range in the evaluation set (eg. <span>150, 200</span>) or if model performance decreases on observed datapoints outside of this range.</p>

### Required Characters
<p>This test measures the number of failing rows in your data with strings without any required characters and their impact on the model. The model impact is the difference in model performance between passing and failing rows with strings without any required characters. If labels are not provided, prediction change is used instead of model performance change.</p><p><b>Why it matters:</b> A feature may require specific characters. However, errors in the data pipeline may allow invalid data points that lack these required characters to pass. Failing to catch such errors may lead to noisier training data or noisier predictions during inference, which can degrade model metrics.</p><p><b>Configuration:</b> By default, this test runs over all string features that are inferred to have required characters.</p><p><b>Example:</b> Say that the feature <span>email</span> requires the character <span>@</span>. This test raises a warning if we observe any values in the evaluation set where the character is missing.</p>

### Inconsistencies
<p>This test measures the severity of passing to the model data points whose values are inconsistent (as inferred from the reference set). The severity is a function of the impact of these values on the model, as well as the presence of these values in the data. The model impact is the difference in model performance between passing and failing rows with data containing inconsistent feature values. If labels are not provided, prediction change is used instead of model performance change. The number of failing rows refers to the number of times data containing inconsistent feature values are observed in the evaluation set.</p><p><b>Why it matters:</b> Inconsistent values might be the result of malicious actors manipulating the data or errors in the data pipeline. Thus, it is important to be aware of inconsistent values to identify sources of manipulations or errors.</p><p><b>Configuration:</b> By default, this test runs on pairs of categorical features whose correlations exceed some minimum threshold. The default threshold for the frequency ratio below which values are considered to be inconsistent is <span>0.02</span>.</p><p><b>Example:</b> Suppose we have a feature <span>country</span> that takes on value <span>"US"</span> with frequency <span>0.5</span>, and a feature <span>time_zone</span> that takes on value <span>"Central European Time"</span> with frequency <span>0.2</span>. Then if these values appear together with frequency less than <span>0.5 * 0.2 * 0.02 = 0.002 </span>, in the reference set, rows in which these values do appear together are inconsistencies.</p>

### Capitalization
<p>This test measures the number of failing rows in your data with different types of capitalization and their impact on the model. The model impact is the difference in model performance between passing and failing rows with different types of capitalization. If labels are not provided, prediction change is used instead of model performance change.</p><p><b>Why it matters:</b> In production, models can come across the same value with different capitalizations, making it important to explicitly check that your model is invariant to such differences.</p><p><b>Configuration:</b> By default, this test runs over all categorical features.</p><p><b>Example:</b> Suppose we had a column that corresponded to country code. For a specific row, let's say the observed value in the reference set was <span>USA</span>. This test raises a warning if we observe a similar value in the evaluation set with case changes, e.g. <span>uSa</span> or if model performance decreases on observed datapoints with case changes.</p>

### Empty String
<p>This test measures the number of failing rows in your data with empty string values instead of null values and their impact on the model. The model impact is the difference in model performance between passing and failing rows with empty string values instead of null values. If labels are not provided, prediction change is used instead of model performance change.</p><p><b>Why it matters:</b> In production, the model may encounter corrupted or manipulated string values. Null values and empty strings are often expected to be treated the same, but the model might not treat them that way. It is important that the model is robust to such extremities.</p><p><b>Configuration:</b> By default, this test runs over all string features with null values.</p><p><b>Example:</b> In the reference set, the <span>Name</span> feature contains nulls. This test raises a warning if we observe any empty string in the <span>Name</span> feature or if these values decrease model performance.</p>

### Embedding Anomalies
<p>This test measures the number of failing rows in your data with anomalous embeddings and their impact on the model. The model impact is the difference in model performance between passing and failing rows with anomalous embeddings. If labels are not provided, prediction change is used instead of model performance change.</p><p><b>Why it matters:</b> In production, the presence of anomalous embeddings can indicate breaks in upstream data pipelines, poor model generalization, or other issues.</p><p><b>Configuration:</b> By default, this test runs over all configured embeddings.</p><p><b>Example:</b> Say that the 'user_id' embedding is two-dimensional and has a mean at the origin and a covariance matrix of [[1, 0], [0, 1]] in the reference set. This test will flag any embeddings in the test set that are distant from the reference distribution using the Mahalanobis distance.</p>

### Unseen Unigram
<p>This test measures the number of failing rows in your data with unseen unigrams and their impact on the model. The model impact is the difference in model performance between passing and failing rows with unseen unigrams. If labels are not provided, prediction change is used instead of model performance change.</p><p><b>Why it matters:</b> Unseen unigrams are a common failure point in machine learning systems; since these models are trained over a reference set, they may yield uninterpretable or undefined behavior when interacting with an unseen unigram. In addition, such errors may expose gaps or errors in data collection.</p><p><b>Configuration:</b> By default, this test is run over every data point.</p><p><b>Example:</b> Say that there is a text field with value <span>James went to his casa</span> and the unigram <span>casa</span> was not seen in the reference set. This test would raise a warning flagging that datapoint, with the severity depending on how bad the model performed on that datapoint.</p>

### Empty Text String
<p>This test measures the number of failing rows in your data with empty strings and their impact on the model. The model impact is the difference in model performance between passing and failing rows with empty strings. If labels are not provided, prediction change is used instead of model performance change.</p><p><b>Why it matters:</b> Empty strings are a common failure point in machine learning systems; as some models may yield uninterpretable or undefined behavior when interacting with an empty string. In addition, such errors may expose gaps or errors in data collection.</p><p><b>Configuration:</b> By default, this test is run over every data point.</p><p><b>Example:</b> Say that there is a text field that is just an empty string. This test would raise a warning flagging that datapoint, with the severity depending on how bad the model performed on that datapoint.</p>

## Drift

### Correlation Drift (Feature-to-Feature)
<p>This test measures the severity of feature-feature correlation drift from the reference to the evaluation set for a given pair of features. The severity is a function of the correlation drift in the data. The key detail is the difference in correlation scores between the reference and evaluation sets, along with an associated p-value. Correlation is a measure of the linear relationship between two numeric columns (feature-feature) so this test checks for significant changes in this relationship between each feature-feature in the reference and evaluation sets. To compute the p-value, we use Fisher's z-transformation to convert the distribution of sample correlations to a normal distribution, and then we run a standard two-sample test on two normal distributions.</p><p><b>Why it matters:</b> Correlation drift between training and inference can be caused by a variety of factors, including a change in the data generation process or a change in the underlying processing stage. A big shift in these dependencies could indicate shifting datasets and degradation in model performance, signaling the need for relabeling and retraining.</p><p><b>Configuration:</b> By default, this test runs over all pairs of features in the dataset.</p><p><b>Example:</b> Suppose that the correlation between <span>country</span> and <span>state</span> is 0.5 in the reference set but 0.7 in the evaluation set, and the p-value is 0.03. Then the large difference in scores indicates that the dependency between the two features has drifted. If our difference threshold was 0.2, and p-value threshold was 0.05, then the test would fail.</p>

### Correlation Drift (Feature-to-Label)
<p>This test measures the severity of feature-label correlation drift from the reference to the evaluation set for a given pair of a feature and label. The severity is a function of the correlation drift in the data. The key detail is the difference in correlation scores between the reference and evaluation sets, along with an associated p-value. Correlation is a measure of the linear relationship between two numeric columns (feature-label) so this test checks for significant changes in this relationship between each feature-label in the reference and evaluation sets. To compute the p-value, we use Fisher's z-transformation to convert the distribution of sample correlations to a normal distribution, and then we run a standard two-sample test on two normal distributions.</p><p><b>Why it matters:</b> Correlation drift between training and inference can be caused by a variety of factors, including a change in the data generation process or a change in the underlying processing stage. A big shift in these dependencies could indicate shifting datasets and degradation in model performance, signaling the need for relabeling and retraining.</p><p><b>Configuration:</b> By default, this test runs over all pairs of features and labels in the dataset.</p><p><b>Example:</b> Suppose that the correlation between <span>LotArea</span> and <span>SalePrice</span> is 0.4 in the reference set but 0.8 in the evaluation set, and the p-value is 0.15. Then the large difference in scores indicates that the impact of the feature on the label has drifted. If our difference threshold was 0.2, and p-value threshold was 0.05, then the test would fail.</p>

### Mutual Information Drift (Feature-to-Feature)
<p>This test measures the severity of feature mutual information drift from the reference to the evaluation set for a given pair of features. The severity is a function of the mutual information drift in the data. The key detail is the difference in mutual information scores between the reference and evaluation sets. Mutual information is a measure of how dependent two features are, so this checks for significant changes in dependence between pairs of features in the reference and evaluation sets.</p><p><b>Why it matters:</b> Mutual information drift between training and inference can be caused by a variety of factors, including a change in the data generation process or a change in the underlying processing stage. A big shift in these dependencies could indicate shifting datasets and degradation in model performance, signaling the need for relabeling and retraining.</p><p><b>Configuration:</b> By default, this test runs over all pairs of features in the dataset.</p><p><b>Example:</b> Suppose that the mutual information between <span>country</span> and <span>state</span> is 0.5 in the reference set but 0.7 in the evaluation set. Then the large difference in scores indicates that the dependency between the two features has drifted. If our difference threshold was 0.2 then the test would fail.</p>

### Mutual Information Drift (Feature-to-Label)
<p>This test measures the severity of feature mutual information drift from the reference to the evaluation set for a given pair of features. The severity is a function of the mutual information drift in the data. The key detail is the difference in mutual information scores between the reference and evaluation sets. Mutual information is a measure of how dependent two features are, so this checks for significant changes in dependence between pairs of features in the reference and evaluation sets.</p><p><b>Why it matters:</b> Mutual information drift between training and inference can be caused by a variety of factors, including a change in the data generation process or a change in the underlying processing stage. A big shift in these dependencies could indicate shifting datasets and degradation in model performance, signaling the need for relabeling and retraining.</p><p><b>Configuration:</b> By default, this test runs over all pairs of features in the dataset.</p><p><b>Example:</b> Suppose that the mutual information between <span>country</span> and <span>state</span> is 0.5 in the reference set but 0.7 in the evaluation set. Then the large difference in scores indicates that the dependency between the two features has drifted. If our difference threshold was 0.2 then the test would fail.</p>

### Label Drift (Categorical)
<p>This test checks that the difference in label distribution between the reference and evaluation sets is small, using PSI test. The key detail displayed is the PSI statistic which is a measure of how different the frequencies of the column in the reference and evaluation sets are.</p><p><b>Why it matters:</b> Label distribution shift between reference and test can indicate that the underlying data distribution has changed significantly enough to modify model decisions. This may mean that the model needs to be retrained to adjust to the new data environment. In addition, significant label distribution shift may indicate that upstream decision-making modules (e.g. thresholds) may need to be updated.</p><p><b>Configuration:</b> This test is run by default whenever both the reference and evaluation sets have associated labels.</p><p><b>Example:</b> Suppose that the observed frequencies of the label column is [100, 200] in the reference set but [25, 150] in the test set. Then the PSI would be 0.201. If our PSI threshold was 0.1 then the test would fail.</p>

### Predicted Label Drift
<p>This test checks that the difference in predicted label distribution between the reference and evaluation sets is small, using PSI test. The key detail displayed is the PSI statistic which is a measure of how different the frequencies of the column in the reference and evaluation sets are.</p><p><b>Why it matters:</b> Predicted Label distribution shift between reference and test can indicate that the underlying data distribution has changed significantly enough to modify model decisions. This may mean that the model needs to be retrained to adjust to the new data environment. In addition, significant predicted label distribution shift may indicate that upstream decision-making modules (e.g. thresholds) may need to be updated.</p><p><b>Configuration:</b> This test is run by default whenever the model or predictions is provided.</p><p><b>Example:</b> Suppose that the observed frequencies of the predicted label column is [100, 200] in the reference set but [25, 150] in the test set. Then the PSI would be 0.201. If our PSI threshold was 0.1 then the test would fail.</p>

### Label Drift (Regression)
<p>This test checks that the difference in label distribution between the reference and evaluation sets is small, using the PSI test. The key detail displayed is the KS statistic which is a measure of how different the labels in the reference and evaluation sets are. Concretely, the KS statistic is the maximum difference of the empirical CDF's of the two label columns.</p><p><b>Why it matters:</b> Label distribution shift between reference and test can indicate that the underlying data distribution has changed significantly enough to modify model decisions. This may mean that the model needs to be retrained to adjust to the new data environment. In addition, significant label distribution shift may indicate that upstream decision-making modules (e.g. thresholds) may need to be updated.</p><p><b>Configuration:</b> This test is run by default whenever both the reference and evaluation sets have associated labels.</p><p><b>Example:</b> Suppose that the distribution of labels changes between the reference and evaluation sets such that PSI these two samples is <span>0.2</span>. If the PSI threshold is <span>0.1</span>, then this test would raise a warning.</p>

### Categorical Feature Drift
<p>This test measures the severity of passing to the model data points that have categorical features which have drifted from the distribution observed in the reference set. The severity is a function of the impact on the model, as well as the presence of drift in the data. The model impact measures how much model performance changes due to drift in the given feature. The key detail displayed is the PSI test statistic, which is a measure of how statistically significant the difference between the frequencies of categorical values in the reference and evaluation sets is.</p><p><b>Why it matters:</b> Distribution drift in categorical features between training and inference can be caused by a variety of factors, including a change in the data generation process or a change in the preprocessing pipeline. A big shift in categorical features towards categorical subsets that your model performs poorly in could indicate a degradation in model performance and signal the need for relabeling and retraining. </p><p><b>Configuration:</b> By default, this test runs over all categorical columns with sufficiently many samples. </p><p><b>Example:</b> Suppose that the observed frequencies of the <span>isLoggedIn</span> feature is [100, 200] in the reference set but [25, 150] in the test set. Then the PSI would be 0.201. If our PSI threshold was 0.1 then the test would fail.</p>

### Numeric Feature Drift
<p>This test measures the severity of passing to the model data points that have numeric features that have drifted from the distribution observed in the reference set. The severity is a function of the impact on the model, as well as the presence of drift in the data. The model impact measures how much model performance changes due to drift in the given feature. The key detail is the Population Stability Index statistic. The Population Stability Index (PSI) is a measure of how different two distributions are. Given two distributions P and Q, it is computed as the sum of the KL Divergence between P and Q and the (reverse) KL Divergence between Q and P. Thus, PSI is symmetric.</p><p><b>Why it matters:</b> Distribution shift between training and inference can cause degradation in model performance. If the shift is sufficiently large, retraining the model on newer data may be necessary.</p><p><b>Configuration:</b> By default, this test runs over all numeric columns with sufficiently many samples and stored quantiles in each of the reference and evaluation sets.</p><p><b>Example:</b> Suppose that the distribution of a feature <span>Age</span> changes between the reference and evaluation sets such that the Population Stability Index between these two samples is <span>0.2</span>. If the distance threshold is set to <span>0.1</span>, this test would raise a warning.</p>

### Prediction Drift
<p>This test checks that the difference in the prediction distribution between the reference and evaluation sets is small, using Population Stability Index. The key detail displayed is the PSI which is a measure of how different the prediction distributions in the reference and evaluation sets are.</p><p><b>Why it matters:</b> Prediction distribution shift between reference and test can indicate that the underlying data distribution has changed significantly enough to modify model decisions. This may mean that the model needs to be retrained to adjust to the new data environment. In addition, significant prediction distribution drift may indicate that upstream decision-making modules (e.g. thresholds) may need to be updated.</p><p><b>Configuration:</b> This test is run by default whenever both the reference and evaluation sets have associated predictions. Different thresholds are associated with different severities.</p><p><b>Example:</b> Suppose that the PSI between the prediction distributions in the reference and evaluation sets is 0.201. Then if the PSI thresholds are (0.1, 0.2, 0.3), the test would fail with medium severity.</p>

### Embedding Drift
<p>This test measures the severity of passing to the model data points associated with embeddings that have drifted from the distribution observed in the reference set. The severity is a function of the impact on the model, as well as the presence of drift in the data. The model impact measures how much model performance changes due to drift in the given feature. The key detail is the Euclidean Distance statistic. The Euclidean Distance is defined as the square root of the sum of the squared differences between two vectors X and Y. The normalized version of this metric first divides each vector by its L2 norm. This test takes the normalized Euclidean distance between the centroids of the ref and eval data sets.</p><p><b>Why it matters:</b> Distribution shift between training and inference can cause degradation in model performance. If the shift is sufficiently large, retraining the model on newer data may be necessary.</p><p><b>Configuration:</b> By default, this test runs over all specified embeddings with sufficiently many samples in each of the reference and evaluation sets.</p><p><b>Example:</b> Suppose that the distribution of an embedding <span>User</span> changes between the reference and evaluation sets such that the Euclidean Distance between these two samples is <span>0.3</span>. If the distance threshold is set to <span>0.1</span>, this test would raise a warning.</p>

### Character Distribution
<p>This test measures the <span>character</span> distribution drift between the reference and evaluation sets. By default, it measures drift by using the Population Stability Index of the two distributions.The severity is determined by comparing the computed drift statistic to the configured severity thresholds.</p><p><b>Why it matters:</b> The reference set that you use to train your model may not be representative of the evaluation set you encounter in production. If there are statistically significant differences in the <span>character</span> distribution between these sets, it can lead to subpar real-world model performance.</p><p><b>Configuration:</b> To pass a given test case, the divergence metric must be below the configured threshold.</p><p><b>Example:</b> Suppose that the change in the <span>character</span> distribution in the reference set and evaluation set yielded a JS Divergence of 0.2. If the distance threshold is set to 0.1, this test would raise a warning.</p>

### Unigrams Distribution
<p>This test measures the <span>unigram</span> distribution drift between the reference and evaluation sets. By default, it measures drift by using the Population Stability Index of the two distributions.The severity is determined by comparing the computed drift statistic to the configured severity thresholds.</p><p><b>Why it matters:</b> The reference set that you use to train your model may not be representative of the evaluation set you encounter in production. If there are statistically significant differences in the <span>unigram</span> distribution between these sets, it can lead to subpar real-world model performance.</p><p><b>Configuration:</b> To pass a given test case, the divergence metric must be below the configured threshold.</p><p><b>Example:</b> Suppose that the change in the <span>unigram</span> distribution in the reference set and evaluation set yielded a JS Divergence of 0.2. If the distance threshold is set to 0.1, this test would raise a warning.</p>

### Bigrams Distribution
<p>This test measures the <span>bigram</span> distribution drift between the reference and evaluation sets. By default, it measures drift by using the Population Stability Index of the two distributions.The severity is determined by comparing the computed drift statistic to the configured severity thresholds.</p><p><b>Why it matters:</b> The reference set that you use to train your model may not be representative of the evaluation set you encounter in production. If there are statistically significant differences in the <span>bigram</span> distribution between these sets, it can lead to subpar real-world model performance.</p><p><b>Configuration:</b> To pass a given test case, the divergence metric must be below the configured threshold.</p><p><b>Example:</b> Suppose that the change in the <span>bigram</span> distribution in the reference set and evaluation set yielded a JS Divergence of 0.2. If the distance threshold is set to 0.1, this test would raise a warning.</p>

## Subset Performance

### Subset Macro F1
<p>F1 is a holistic measure of both precision and recall. When transitioning to the multiclass setting we can use macro F1 which computes the F1 of each class and averages them. This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the macro F1 of model predictions within a specific subset is significantly lower than the model prediction macro F1 over the entire population. </p><p><b>Why it matters:</b> Having different macro F1 between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. </p><p><b>Configuration:</b> By default, macro F1 is computed over all predictions/labels. Note that the predicted label is the label with the largest predicted probability.</p><p><b>Example:</b> Suppose we are differentiating between cats, bears, and dogs. Assume that across the data points where <span>height=2</span> the predictions are <span>[0.9, 0.1, 0], [0.1, 0.9, 0], [0.2, 0.1, 0.7]</span> and the labels are <span>[1, 0, 0], [1, 0, 0], [0, 0, 1]</span> (where the first index corresponds to cat, the second corresponds to bear, and the third corresponds to dog). Then the macro F1 across this subset is <span>0.78</span>. If the overall macro F1 across all subsets is <span>0.9</span> then this test raises a warning.</p>

### Subset Macro Precision
<p>The precision test is also popularly referred to as positive predictive parity in fairness literature. When transitioning to the multiclass setting, we can compute macro precision which computes the precisions of each class individually and then averages them.This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the Macro Precision of model predictions within a specific subset is significantly lower than the model prediction Macro Precision over the entire population. </p><p><b>Why it matters:</b> Having different macro precision (e.g. false discovery rates) between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. Unlike demographic parity, this test permits assuming different base label rates but flags differing mistake rates between different subgroups. Note that positive predictive parity does not necessarily indicate equal opportunity or predictive equality: as a hypothetical example, imagine that a loan qualification classifier flags 100 entries for group A and 100 entries for group B, each with a precision of 100%, but there are 100 actual qualified entries in group A and 9000 in group B. This would indicate disparities in opportunities given to each subgroup.</p><p><b>Configuration:</b> By default, Macro Precision is computed over all predictions/labels. Note that the predicted label is the label with the greatest predicted probability.</p><p><b>Example:</b> Suppose we are differentiating between cats, bears, and dogs. Assume that across the data points where <span>height=2</span> the predictions are <span>[0.9, 0.1, 0], [0.1, 0.9, 0], [0.2, 0.1, 0.7]</span> and the labels are <span>[1, 0, 0], [1, 0, 0], [0, 0, 1]</span> (where the first index corresponds to cat, the second corresponds to bear, and the third corresponds to dog). Then the Macro Precision across this subset is <span>0.67</span>. If the overall Macro Precision across all subsets is <span>0.9</span> then this test raises a warning.</p>

### Subset Macro Recall
<p>The recall test is more popularly referred to as equal opportunity or false negative error rate balance in fairness literature. When transitioning to the multiclass setting we can use macro recall which computes the recall of each individual class and then averages these numbers.This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the Macro Recall of model predictions within a specific subset is significantly lower than the model prediction Macro Recall over the entire population. </p><p><b>Why it matters:</b> Having different true positive rates (e.g. equal opportunity) between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. Unlike demographic parity, this test permits assuming different base label rates but flags differing mistake rates between different subgroups. An intuitive example is when the label indicates a positive attribute: if predicting whether to interview a given candidate, make sure that out of qualified candidates, the rate at which the model predicts an interview is similar to group A and B. </p><p><b>Configuration:</b> By default, Macro Recall is computed over all predictions/labels. Note that the predicted label is the label with the largest predicted class probability.</p><p><b>Example:</b> Suppose we are differentiating between cats, bears, and dogs. Assume that across the data points where <span>height=2</span> the predictions are <span>[0.9, 0.1, 0], [0.1, 0.9, 0], [0.2, 0.1, 0.7]</span> and the labels are <span>[1, 0, 0], [1, 0, 0], [0, 0, 1]</span> (where the first index corresponds to cat, the second corresponds to bear, and the third corresponds to dog). Then the Macro Recall across this subset is <span>0.67</span>. If the overall Macro Recall across all subsets is <span>0.9</span> then this test raises a warning.</p>

### Subset Multiclass Accuracy
<p>This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the accuracy of model predictions within a specific subset is significantly lower than the model prediction accuracy over the entire population. </p><p><b>Why it matters:</b> Having different accuracy between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. Accuracy can be thought of as a 'weaker' metric of model bias compared to measuring false positive rate (predictive equality) or false negative rate (equal opportunity). This is because we can have similar accuracy between group A and group B; yet group A actually has higher false positive rate, while group B has higher false negative rate (e.g. we reject qualified applicants in group A but accept non-qualified applicants in group B). Nevertheless, accuracy is a standard metric used during evaluation and should be considered as part of performance bias testing.</p><p><b>Configuration:</b> By default, accuracy is computed over all predictions/labels. Note we round predictions to 0/1 to compute accuracy.</p><p><b>Example:</b> Suppose we had data with 2 features: <span>[['cat', 0.2], ['dog', 0.3], ['cat', 0.5], ['dog', 0.7], ['cat', 0.7], ['dog', 0.2]]</span>, model predictions <span>[0.3, 0.51, 0.7, 0.49, 0.9, 0.58]</span>, and labels <span>[1, 0, 1, 0, 0, 1]</span>. Then, the accuracy over the feature subset value 'cat' would be 0.33, compared to the overall metric of 0.5.</p>

### Subset Multiclass AUC
<p>In the multiclass setting, we compute one vs. one area under the curve (AUC), which computes the AUC between every pairwise combination of classes. This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the Area Under Curve (AUC) of model predictions within a specific subset is significantly lower than the model prediction Area Under Curve (AUC) over the entire population. </p><p><b>Why it matters:</b> Having different AUC between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. </p><p><b>Configuration:</b> By default, AUC is computed over all predictions/labels. Note that we compute AUC of the Receiver Operating Characteristic (ROC) curve.</p><p><b>Example:</b> Suppose we are differentiating between cats, bears, and dogs. Assume that across the data points where <span>height=2</span> the predictions are <span>[0.9, 0.1, 0], [0.1, 0.9, 0], [0.2, 0.1, 0.7]</span> and the labels are <span>[1, 0, 0], [1, 0, 0], [0, 0, 1]</span> (where the first index corresponds to cat, the second corresponds to bear, and the third corresponds to dog). Then the AUC (one vs. one) across this subset is <span>0.75</span>. If the overall AUC (one vs. one) across all subsets is <span>0.9</span> then this test raises a warning.</p>

### Subset Positive Prediction Rate
Default long description for subset batch runner.

### Subset Average Confidence
Default long description for subset batch runner.

## Data Cleanliness

### Label Imbalance
<p>This test checks that no labels have exceedingly high frequency.</p><p><b>Why it matters:</b> Label imbalance in the training data can introduce bias into the model and possibly result in poor predictive performance on examples from the minority classes.</p><p><b>Configuration:</b> This test runs only on classification tasks.</p><p><b>Example:</b> Suppose we had a binary classification task. We can configure this test to check that neither label <span>0</span> nor <span>1</span> has frequency above a certain threshold.</p>

## Transformations

### Upper-Case Text
<p>This test measures the robustness of your model to Upper-Case Text transformations. It does this by taking a sample input, upper-casing all text, and measuring the behavior of the model on the transformed input.</p><p><b>Why it matters:</b> Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p><p><b>Configuration:</b> By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p><p><b>Example:</b> Given an input sequence "The boy saw Paris Hilton in Paris", this test measures the performance of the model when given the transformed input of "THE BOY SAW PARIS HILTON IN PARIS".</p>

### Lower-Case Text
<p>This test measures the robustness of your model to Lower-Case Text transformations. It does this by taking a sample input, lower-casing all text, and measuring the behavior of the model on the transformed input.</p><p><b>Why it matters:</b> Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p><p><b>Configuration:</b> By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p><p><b>Example:</b> Given an input sequence "The boy saw Paris Hilton in Paris", this test measures the performance of the model when given the transformed input of "the boy saw paris hilton in paris".</p>

### Remove Special Characters
<p>This test measures the robustness of your model to Remove Special Characters transformations. It does this by taking a sample input, removing all periods and apostrophes from the input string, and measuring the behavior of the model on the transformed input.</p><p><b>Why it matters:</b> Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p><p><b>Configuration:</b> By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p><p><b>Example:</b> Given an input sequence "The quick brown fox jumped over the lazy dog...", this test measures the performance of the model when given the transformed input of "The quick brown fox jumped over the lazy dog".</p>

### Replace Masculine with Feminine Pronouns
<p>This test measures the robustness of your model to Replace Masculine with Feminine Pronouns transformations. It does this by taking a sample input, swapping all masculine pronouns from the input string to feminine ones, and measuring the behavior of the model on the transformed input.</p><p><b>Why it matters:</b> Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p><p><b>Configuration:</b> By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p><p><b>Example:</b> Given an input sequence "He was elected because his opponent dropped out", this test measures the performance of the model when given the transformed input of "She was elected because her opponent dropped out".</p>

### Replace Feminine with Masculine Pronouns
<p>This test measures the robustness of your model to Replace Feminine with Masculine Pronouns transformations. It does this by taking a sample input, swapping all feminine pronouns from the input string to masculine ones, and measuring the behavior of the model on the transformed input.</p><p><b>Why it matters:</b> Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p><p><b>Configuration:</b> By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p><p><b>Example:</b> Given an input sequence "She was elected because her opponent dropped out", this test measures the performance of the model when given the transformed input of "He was elected because his opponent dropped out".</p>

### Replace Feminine with Masculine Names
<p>This test measures the invariance of your model to gendered name swap transformations. It does this by taking a sample input, swapping all instances of traditionally feminine names (in the provided list) with a traditionally masculine name, and measuring the behavior of the model on the transformed input.</p><p><b>Why it matters:</b> Production natural language input sequences must properly support people of all demographics. It is important that your NLP models are robust to spurious correlations and bias from the data.</p><p><b>Configuration:</b> By default, this test runs over a sample of up to strings from the evaluation set that contain one or more words from the source list.</p><p><b>Example:</b> Given an input sequence "Adrian is a good student.", this test measures the behavior of the model when given the transformed input of "Amy is a good student.".</p>

### Replace Masculine with Feminine Names
<p>This test measures the invariance of your model to gendered name swap transformations. It does this by taking a sample input, swapping all instances of traditionally masculine names (in the provided list) with a traditionally feminine name, and measuring the behavior of the model on the transformed input.</p><p><b>Why it matters:</b> Production natural language input sequences must properly support people of all demographics. It is important that your NLP models are robust to spurious correlations and bias from the data.</p><p><b>Configuration:</b> By default, this test runs over a sample of up to strings from the evaluation set that contain one or more words from the source list.</p><p><b>Example:</b> Given an input sequence "Amy is a good student.", this test measures the behavior of the model when given the transformed input of "Adrian is a good student.".</p>

### Unicode to ASCII
<p>This test measures the robustness of your model to Unicode to ASCII transformations. It does this by taking a sample input, converting all characters in the input string to their nearest ASCII representation, and measuring the behavior of the model on the transformed input.</p><p><b>Why it matters:</b> Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p><p><b>Configuration:</b> By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p><p><b>Example:</b> Given an input sequence "René François Lacôte did not like that movie", this test measures the performance of the model when given the transformed input of "Rene Francois Lacote did not like that movie".</p>

### Character Substitution
<p>This test measures the robustness of your model to character substitution attacks. It does this by randomly substituting characters in the input string and measuring your model's performance on the attacked string.</p><p><b>Why it matters:</b> Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p><p><b>Configuration:</b> By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p><p><b>Example:</b> Given an input sequence "The quick brown fox jumped over the lazy dog", this test measures the performance of the model when given the attacked input of "Tie quick brorn fox tumped over the lyzy dog".</p>

### Character Deletion
<p>This test measures the robustness of your model to character deletion attacks. It does this by randomly deleting characters in the input string and measuring your model's performance on the attacked string.</p><p><b>Why it matters:</b> Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p><p><b>Configuration:</b> By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p><p><b>Example:</b> Given an input sequence "The quick brown fox jumped over the lazy dog", this test measures the performance of the model when given the attacked input of "Th quick brwn fox jumpd over the lazy dog".</p>

### Character Insertion
<p>This test measures the robustness of your model to character insertion attacks. It does this by randomly adding characters to the input string and measuring your model's performance on the attacked string.</p><p><b>Why it matters:</b> Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p><p><b>Configuration:</b> By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p><p><b>Example:</b> Given an input sequence "The quick brown fox jumped over the lazy dog", this test measures the performance of the model when given the attacked input of "Thew quick broqwn fox jumqped over the lazy dog".</p>

### Character Swap
<p>This test measures the robustness of your model to character swap attacks. It does this by randomly swapping characters in the input string and measuring your model's performance on the attacked string.</p><p><b>Why it matters:</b> Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p><p><b>Configuration:</b> By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p><p><b>Example:</b> Given an input sequence "The quick brown fox jumped over the lazy dog", this test measures the performance of the model when given the attacked input of "Teh quick bornw fox ujmpde over the lazy dog".</p>

### Keyboard Augmentation
<p>This test measures the robustness of your model to keyboard augmentation attacks. It does this by adding common typos based on keyboard distance to the input string and measuring your model's performance on the attacked string.</p><p><b>Why it matters:</b> Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p><p><b>Configuration:</b> By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p><p><b>Example:</b> Given an input sequence "The quick brown fox jumped over the lazy dog", this test measures the performance of the model when given the attacked input of "Thr quick browb fox jumled over the lazy dog".</p>

### Common Misspellings
<p>This test measures the robustness of your model to common misspellings attacks. It does this by adding common misspellings to the input string and measuring your model's performance on the attacked string.</p><p><b>Why it matters:</b> Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p><p><b>Configuration:</b> By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p><p><b>Example:</b> Given an input sequence "The quick brown fox jumped over the lazy dog", this test measures the performance of the model when given the attacked input of "Thee quik brown focks jumped over the lasy dog".</p>

### OCR Error Simulation
<p>This test measures the robustness of your model to ocr error simulation attacks. It does this by adding common OCR errors to the input string and measuring your model's performance on the attacked string.</p><p><b>Why it matters:</b> Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p><p><b>Configuration:</b> By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p><p><b>Example:</b> Given an input sequence "The quick brown fox jumped over the lazy dog", this test measures the performance of the model when given the attacked input of "Th3 quick br0wn fox jumped over the 1azy d0g".</p>

## Model Performance

### Average Confidence
<p>This test checks the average confidence of the model predictions between the reference and evaluation sets to see if the metric has experienced significant degradation. The "confidence" of a prediction for classification tasks is defined as the distance between the probability of the predicted class (defined as the argmax over the prediction vector) and 1. We average this metric across all predictions.</p><p><b>Why it matters:</b> During production, factors like distribution shift or a change in <span>p(y|x)</span> may cause model performance to decrease significantly. Since oftentimes labels are not available in a production setting, this metric can serve as a useful proxy for model performance.</p><p><b>Configuration:</b> By default, this test runs if predictions are specified (no labels required).</p><p><b>Example:</b> Assume that on the reference set the model obtained <span>0.85</span> average confidence but on the evaluation set without labels we predict that the model obtained <span>0.5</span> average confidence. Then this test raises a warning.</p>

### Average Thresholded Confidence
<p>This test checks the average thresholded confidence (ATC) of the model predictions between the reference and evaluation sets to see if the metric has experienced significant degradation. ATC is a method for estimating accuracy of unlabeled examples taken from <a href="https://arxiv.org/abs/2201.04234">this paper</a>. The threshold is first computed on the reference set: we pick a confidence threshold such that the percentage of datapoints whose max predicted probability is less than the threshold is around equal to the error rate of the model (here, it is 1-accuracy) on the reference set. Then, we apply this threshold in the evaluation set: the predicted accuracy is then equal to the percentage of datapoints with max predicted probability greater than this threshold.</p><p><b>Why it matters:</b> During production, factors like distribution shift may cause model performance to decrease significantly. Since oftentimes labels are not available in a production setting, this metric can serve as a useful proxy for model performance.</p><p><b>Configuration:</b> By default, this test runs if predictions/labels are specified in the reference set and predictions are specified in the eval set (no labels required).</p><p><b>Example:</b> Assume that on the reference set the model obtained <span>0.85</span> accuracy but on the evaluation set, we find that only 55 percent of datapoints have max predicted probability greater than our threshold. Then our predicted accuracy is 0.55 and this test raises a warning.</p>

### Calibration Comparison
<p>This test checks that the reference and evaluation sets have sufficiently similar calibration curves as measured by the Mean Squared Error (MSE) between the two curves. The calibration curve is a line plot where the x-axis represents the average predicted probability and the y-axis is the proportion of positive predictions. The curve of the ideal calibrated model is thus a linear straight line from (0, 0) moving linearly.</p><p><b>Why it matters:</b> Knowing how well-calibrated your model is can help you better interpret and act upon model outputs, and can even be an indicator of generalization. A greater difference between reference and evaluation curves could indicate a lack of generalizability. In addition, a change in calibration could indicate that decision-making or thresholding conducted upstream needs to change as it is behaving differently on held-out data.</p><p><b>Configuration:</b> By default, this test runs over the predictions and labels.</p><p><b>Example:</b> Suppose the model’s task is binary classification and predicts whether or not a data point is fraudulent. If we have a reference set in which <span>1%</span> of the data points are fraudulent, but an evaluation set where <span>50%</span> are fraudulent, then our model may not be well calibrated, and the MSE difference in the curves will be large, resulting in a failing test.</p>

### Average Prediction
<p>This test checks the Average Prediction metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of Average Prediction has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p><p><b>Why it matters:</b> During production, factors like distribution shift or a change in <span>p(y|x)</span> may cause model performance to decrease significantly.</p><p><b>Configuration:</b> By default, this test runs over the Average Prediction metric with the below thresholds set for the absolute and degradation tests.</p><p><b>Example:</b> Assume that on the reference set the model obtained <span>0.85</span> AUC but on the evaluation set the model obtained <span>0.5</span> AUC. Then this test raises a warning.</p>

### Macro F1
<p>This test checks the Macro F1 metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of Macro F1 has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p><p><b>Why it matters:</b> During production, factors like distribution shift or a change in <span>p(y|x)</span> may cause model performance to decrease significantly.</p><p><b>Configuration:</b> By default, this test runs over the Macro F1 metric with the below thresholds set for the absolute and degradation tests.</p><p><b>Example:</b> Assume that on the reference set the model obtained <span>0.85</span> AUC but on the evaluation set the model obtained <span>0.5</span> AUC. Then this test raises a warning.</p>

### Macro Precision
<p>This test checks the Macro Precision metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of Macro Precision has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p><p><b>Why it matters:</b> During production, factors like distribution shift or a change in <span>p(y|x)</span> may cause model performance to decrease significantly.</p><p><b>Configuration:</b> By default, this test runs over the Macro Precision metric with the below thresholds set for the absolute and degradation tests.</p><p><b>Example:</b> Assume that on the reference set the model obtained <span>0.85</span> AUC but on the evaluation set the model obtained <span>0.5</span> AUC. Then this test raises a warning.</p>

### Macro Recall
<p>This test checks the Macro Recall metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of Macro Recall has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p><p><b>Why it matters:</b> During production, factors like distribution shift or a change in <span>p(y|x)</span> may cause model performance to decrease significantly.</p><p><b>Configuration:</b> By default, this test runs over the Macro Recall metric with the below thresholds set for the absolute and degradation tests.</p><p><b>Example:</b> Assume that on the reference set the model obtained <span>0.85</span> AUC but on the evaluation set the model obtained <span>0.5</span> AUC. Then this test raises a warning.</p>

### Multiclass Accuracy
<p>This test checks the Multiclass Accuracy metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of Multiclass Accuracy has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p><p><b>Why it matters:</b> During production, factors like distribution shift or a change in <span>p(y|x)</span> may cause model performance to decrease significantly.</p><p><b>Configuration:</b> By default, this test runs over the Multiclass Accuracy metric with the below thresholds set for the absolute and degradation tests.</p><p><b>Example:</b> Assume that on the reference set the model obtained <span>0.85</span> AUC but on the evaluation set the model obtained <span>0.5</span> AUC. Then this test raises a warning.</p>

### Multiclass AUC
<p>This test checks the Multiclass AUC metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of Multiclass AUC has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p><p><b>Why it matters:</b> During production, factors like distribution shift or a change in <span>p(y|x)</span> may cause model performance to decrease significantly.</p><p><b>Configuration:</b> By default, this test runs over the Multiclass AUC metric with the below thresholds set for the absolute and degradation tests.</p><p><b>Example:</b> Assume that on the reference set the model obtained <span>0.85</span> AUC but on the evaluation set the model obtained <span>0.5</span> AUC. Then this test raises a warning.</p>

### Positive Prediction Rate
<p>This test checks the Positive Prediction Rate metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of Positive Prediction Rate has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p><p><b>Why it matters:</b> During production, factors like distribution shift or a change in <span>p(y|x)</span> may cause model performance to decrease significantly.</p><p><b>Configuration:</b> By default, this test runs over the Positive Prediction Rate metric with the below thresholds set for the absolute and degradation tests.</p><p><b>Example:</b> Assume that on the reference set the model obtained <span>0.85</span> AUC but on the evaluation set the model obtained <span>0.5</span> AUC. Then this test raises a warning.</p>

## Subset Performance Degradation

### Subset Drift Average Prediction
Default long description for subset batch runner.

## Data Poisoning Detection

### Label Flipping Detection (Exact Match)
<p>This test detects corrupted data points in the evaluation dataset. It does this by checking for input text strings in the evaluation set that are also present in the reference set, but with a different label. This test assumes that the reference set is clean, trusted data and the evaluation set is potentially corrupted.</p><p><b>Why it matters:</b> Malicious actors can tamper with data pipelines by sending mislabeled data points to confuse your model. Detecting poisoning attacks before they affect your model is critical to ensuring model security.</p><p><b>Configuration:</b> By default, this test runs when the "Data Poisoning Detection" test category is selected.</p><p><b>Example:</b> Suppose there was an identical input text string in both datasets, with label <span>0</span> in the reference set and label <span>1</span> in the evaluation set. This test would flag the sample in the evaluation set as being corrupted.</p>

