<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>RIME overview &mdash; RIME  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="https://cdn.datatables.net/v/dt/dt-1.13.1/sb-1.4.0/datatables.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
        <script src="../_static/main.js"></script>
        <script src="https://cdn.datatables.net/v/dt/dt-1.13.1/sb-1.4.0/datatables.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Test Categories" href="explanation/tabular_test_categories.html" />
    <link rel="prev" title="RIME v11 Release Note" href="../changelogs/v11.html" />
<link href="../_static/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav">
<div class="ge_header no-print">
    <a id="header-logo" href="../index.html">
        <img src="../_static/header-logo.png" alt="logo" />
    </a>
</div>


  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> RIME
          </a>
              <div class="version">
                2.0.0-beta.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../changelogs.html">Release Notes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">RIME overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#the-ri-platform">The RI Platform</a></li>
<li class="toctree-l2"><a class="reference internal" href="#why-use-the-ri-platform">Why use the RI Platform?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#key-features">Key Features</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#ai-stress-testing">AI Stress Testing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ai-firewall">AI Firewall</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ai-continuous-testing">AI Continuous Testing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ai-compliance-management">AI Compliance Management</a></li>
<li class="toctree-l3"><a class="reference internal" href="#governance-dashboard">Governance Dashboard</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#key-machine-learning-tasks-covered">Key Machine Learning Tasks Covered</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#tabular">Tabular</a></li>
<li class="toctree-l3"><a class="reference internal" href="#natural-language-processing-nlp">Natural Language Processing (NLP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#computer-vision-cv">Computer Vision (CV)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#ri-platform-deployment-patterns">RI Platform Deployment Patterns</a></li>
<li class="toctree-l2"><a class="reference internal" href="#summary-tests">Summary Tests</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#test-result-metrics">Test result metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model-features">Model features</a></li>
<li class="toctree-l3"><a class="reference internal" href="#failing-rows">Failing rows</a></li>
<li class="toctree-l3"><a class="reference internal" href="#original-data-points">Original data points</a></li>
<li class="toctree-l3"><a class="reference internal" href="#software-monitoring-logs">Software monitoring logs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#ri-platform-consolidated-test-database">RI Platform consolidated test database</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tabular">Tabular</a><ul>
<li class="toctree-l3"><a class="reference internal" href="explanation/tabular_test_categories.html">Test Categories</a></li>
<li class="toctree-l3"><a class="reference internal" href="explanation/tests/tabular/tests.html">Tests</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#nlp">NLP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="explanation/unstructured_test_categories.html">Test Categories</a></li>
<li class="toctree-l3"><a class="reference internal" href="explanation/tests/nlp/text_classification_tests.html">Text Classification Tests</a></li>
<li class="toctree-l3"><a class="reference internal" href="explanation/tests/nlp/natural_language_inference_tests.html">Natural Language Inference Tests</a></li>
<li class="toctree-l3"><a class="reference internal" href="explanation/tests/nlp/named_entity_recognition_tests.html">Named Entity Recognition Tests</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#cv">CV</a><ul>
<li class="toctree-l3"><a class="reference internal" href="explanation/unstructured_test_categories.html">Test Categories</a></li>
<li class="toctree-l3"><a class="reference internal" href="explanation/tests/cv/image_classification_tests.html">Image Classification Tests</a></li>
<li class="toctree-l3"><a class="reference internal" href="explanation/tests/cv/object_detection_tests.html">Object Detection Tests</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#rime-local-trial">RIME local trial</a><ul>
<li class="toctree-l3"><a class="reference internal" href="local_trial/index.html">Local Trial</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Administering the RI Platform</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../for_admins/index.html">Administering the RI Platform</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Installing the RI Platform</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation/index.html">Installation Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data science with the RI Platform</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../for_data_scientists/get_started.html">Get Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../for_data_scientists/how_to_guides.html">How-To Guides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../for_data_scientists/reference.html">Reference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../reference/index.html">RIME in-depth reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../for_data_scientists/how_to_guides/common_use_cases/adversarial_nlp.html">NLP Adversarial Robustness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../for_data_scientists/reference/tabular/firewall_continuous_tests.html">Manual Continuous Tests Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../for_data_scientists/reference/tabular/scheduled_ct_configuration.html">Scheduling Continuous Tests</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">RIME</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a></li>
      <li class="breadcrumb-item active">RIME overview</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/getstarted/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="rime-overview">
<h1>RIME overview<a class="headerlink" href="#rime-overview" title="Permalink to this heading"></a></h1>
<p>The Robust Intelligence (RI) Platform secures your machine learning pipeline so you can focus on building better ML models for your business needs.</p>
<section id="the-ri-platform">
<h2>The RI Platform<a class="headerlink" href="#the-ri-platform" title="Permalink to this heading"></a></h2>
<p>The RI Platform operates at three stages of the ML lifecycle.</p>
<p>During model development, <strong>AI Stress Testing</strong> measures the robustness of your model by running dozens of pre-configured tests, each of which checks the model’s vulnerability to a specific form of potential failure in production. After production deployment and before inference, <strong>AI Firewall</strong> protects your model from such critical errors in real-time by flagging or blocking aberrant data from entering your ML system. After inference, <strong>AI Continuous Testing</strong> monitors your model and alerts on issues such as data drift and performance degradation. When things go wrong, it also offers automated root cause analysis of the underlying driver of performance change.</p>
<img src="../_static/rime.png">
</section>
<section id="why-use-the-ri-platform">
<h2>Why use the RI Platform?<a class="headerlink" href="#why-use-the-ri-platform" title="Permalink to this heading"></a></h2>
<p>In modern engineering organizations, data scientists and machine learning engineers typically spend the majority of their effort on the development stage of the model life cycle, which encompasses data ingestion and cleaning, feature extraction, and model training. During this stage, models are primarily evaluated based on their performance on some clean, held-out test set.</p>
<p>While such metrics might be sufficient for understanding model performance in controlled development environments, deploying models into production introduces a whole new set of challenges and failure modes that are often overlooked. Once a model is deployed, data scientists no longer have complete control over how a model is instantiated, how data is passed into the model, nor do they have any oversight over data pipelines in which the model is integrated. Even when the model is used correctly, the real world can change, and issues like distributional shifts in production data may silently degrade model performance.</p>
</section>
<section id="key-features">
<h2>Key Features<a class="headerlink" href="#key-features" title="Permalink to this heading"></a></h2>
<p>The RI Platform addresses these risks with four core products:</p>
<section id="ai-stress-testing">
<h3>AI Stress Testing<a class="headerlink" href="#ai-stress-testing" title="Permalink to this heading"></a></h3>
<p>AI Stress Testing is a set of tests that measure the robustness of your ML deployment by computing an aggregate severity score across all tests. The severity score is a measure of the magnitude of the identified failure mode specific to each test. It is a combination of the impact the failure has on model performance (<strong>Performance Change</strong> or <strong>Prediction Change</strong>) and the prevalence of the failure mode in the reference set (<strong>Abnormal Inputs</strong> or <strong>Drift Statistic</strong>). By running hundreds of these unit tests and simulations across both your model and associated reference and evaluation datasets, the RI Platform identifies implicit assumptions and failure modes of the ML deployment.</p>
<p>AI Stress Testing allows you to test your data and model before deployment. We recommend providing a model and labels when running AI Stress Testing to leverage the platform’s full potential; however, it is not required. You can run the RI Platform in various modes.</p>
<ul class="simple">
<li><p><strong>Model</strong>: Providing access to the model allows for testing the model behavior under different circumstances. In these tests, we perturb model inputs, provide them to the model, and examine the model behavior to uncover its vulnerabilities.</p></li>
<li><p><strong>Predictions</strong>: Providing predictions for the data can speed up the RI Platform and allows us to test your model even if you don’t provide a model interface. We use sophisticated statistical algorithms to run most of the same tests as when we have direct model access to uncover vulnerabilities within your model and approximate the impact of each vulnerability. If you provide neither a model nor predictions, the RI Platform will still run data quality and data distribution shift tests.</p></li>
<li><p><strong>Labels</strong>: Providing labels allows for testing model performance under different circumstances. If you do not provide labels, the RI Platform will still run data quality tests, data distribution tests, and prediction distribution tests (if possible).</p></li>
</ul>
</section>
<section id="ai-firewall">
<h3>AI Firewall<a class="headerlink" href="#ai-firewall" title="Permalink to this heading"></a></h3>
<p>AI Firewall protects your model from bad predictions before model inference. Firewall operates at the data point level. It can flag or block aberrant data points in realtime, and the AI Firewall is automatically configured from the results of stress testing. The end result is that the user gets a custom AI Firewall that protects against the unique forms of model failure to which a given model is susceptible. Firewall can be deployed with a single line of code directly in the inference pipeline of your ML model, wherein it logs, analyzes, and/or acts upon (flag, block, impute) aberrant data points in realtime.</p>
</section>
<section id="ai-continuous-testing">
<h3>AI Continuous Testing<a class="headerlink" href="#ai-continuous-testing" title="Permalink to this heading"></a></h3>
<p>AI Continuous Testing enables the user to monitor their ML model after inference. As suggested by the name, this view uses the same Stress Testing framework applied continually across time. Data drift will inevitably occur once a model is deployed in production. Continuous tests help answer both the what and the why of changing data. It not only detects issues as they happen but also alerts you regarding the issues and provides insight into their root causes - shortening the time to resolution. Continuous Testing can be set up by passively logging and analyzing predictions by uploading prediction logs after model inference. These can be automated to run at regular intervals.</p>
</section>
<section id="ai-compliance-management">
<h3>AI Compliance Management<a class="headerlink" href="#ai-compliance-management" title="Permalink to this heading"></a></h3>
<p>AI Compliance Management allows the user to download auto-generated model cards for internal and external documentation needs. This incorporates results from the AI Stress Testing suite (including a suite of bias and fairness tests) that measure a model’s production readiness. These reports help companies comply with AI regulatory standards.</p>
</section>
<section id="governance-dashboard">
<h3>Governance Dashboard<a class="headerlink" href="#governance-dashboard" title="Permalink to this heading"></a></h3>
<p>A single pane of glass provides visibility into all models in production, providing model health status and the ability to track models to any custom metric. The Governance dashboard is behind a feature flag. Request enabling of this feature directly from Robust Intelligence.</p>
</section>
</section>
<section id="key-machine-learning-tasks-covered">
<h2>Key Machine Learning Tasks Covered<a class="headerlink" href="#key-machine-learning-tasks-covered" title="Permalink to this heading"></a></h2>
<section id="tabular">
<h3>Tabular<a class="headerlink" href="#tabular" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p>Binary Classification</p></li>
<li><p>Multiclass Classification</p></li>
<li><p>Regression</p></li>
<li><p>Learning to Rank</p></li>
</ul>
</section>
<section id="natural-language-processing-nlp">
<h3>Natural Language Processing (NLP)<a class="headerlink" href="#natural-language-processing-nlp" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p>Text Classification</p></li>
<li><p>Named Entity Recognition</p></li>
</ul>
</section>
<section id="computer-vision-cv">
<h3>Computer Vision (CV)<a class="headerlink" href="#computer-vision-cv" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p>Image Classification</p></li>
<li><p>Object Detection</p></li>
</ul>
</section>
</section>
<section id="ri-platform-deployment-patterns">
<h2>RI Platform Deployment Patterns<a class="headerlink" href="#ri-platform-deployment-patterns" title="Permalink to this heading"></a></h2>
<p>We offer three variations of RI Platform tailored to different deployment patterns. <a class="reference internal" href="../installation/index.html"><span class="doc std std-doc">More information on deployment patterns can be found here</span></a>.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Self-Hosted</p></th>
<th class="head"><p>Managed Cloud</p></th>
<th class="head"><p>Cloud</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Installation</strong></p></td>
<td><p>K8s cluster in Customer VPC</p></td>
<td><p>K8s cluster in RI VPC</p></td>
<td><p>Control Plane K8s cluster in RI VPC <br/> Data Plane K8s cluster in Customer VPC</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Data Location</strong></p></td>
<td><p>Customer VPC</p></td>
<td><p>RI VPC</p></td>
<td><p>Customer VPC</p></td>
</tr>
<tr class="row-even"><td><p><strong>Compute Location</strong></p></td>
<td><p>Customer VPC</p></td>
<td><p>RI VPC</p></td>
<td><p>Customer VPC</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Test Result Location</strong></p></td>
<td><p>Customer VPC</p></td>
<td><p>RI VPC</p></td>
<td><p>RI VPC</p></td>
</tr>
</tbody>
</table>
<p>A lightweight <a class="reference internal" href="local_trial/index.html"><span class="doc std std-doc">local installation</span></a> is available for trial purposes.</p>
</section>
<section id="summary-tests">
<h2>Summary Tests<a class="headerlink" href="#summary-tests" title="Permalink to this heading"></a></h2>
<p>Individual tests measure a single, specific aspect. Drift, for example, is tracked by over 10
different tests. A summary test answers the higher-level question “is there problematic drift present in my data?”</p>
<p>By default, test run results presents these summary tests first. This sample test results page
includes five summary tests in the first view.</p>
<img src="../_static/summary_test_overview_page.png">
<p>Click a test to see the specific page for that summary test.</p>
<img src="../_static/summary_test_page.png">
<p>Summary tests provide the following information:</p>
<section id="test-result-metrics">
<h3>Test result metrics<a class="headerlink" href="#test-result-metrics" title="Permalink to this heading"></a></h3>
<p>These metrics are the outcome of data analysis and do not expose the underlying raw data, which cannot be recreated based on these results. Test results are encrypted.</p>
<p><strong>Performance impact</strong> describes the model’s baseline performance and changes to that performance.</p>
<p><strong>Drift impact</strong> estimates the effect on performance caused by statistically significant drift in a given feature.</p>
<p><strong>Severity scores</strong> are aggregate scores that describe the resilience of an ML deployment. Severity scores vary depending on the vulnerability of the model and dataset to the changes introduced by a given test.</p>
<p><strong>Thresholds</strong> are values that the tests consider to be significant indicators of test failure. When a metric exceeds a specified threshold, the test associated with that metric fails, either as a warning or as an alert.</p>
</section>
<section id="model-features">
<h3>Model features<a class="headerlink" href="#model-features" title="Permalink to this heading"></a></h3>
<p>Information in this section provides a summary of the data. The underlying data cannot be recreated from this information.</p>
<p><strong>Feature names</strong> in the dataset.</p>
<p><strong>Feature distributions</strong> in the dataset, including ranges and common categories.</p>
<p><strong>The values of individual features or groups of features</strong>. Categorical features list the different possible categories. Numerical features list the possible ranges.</p>
</section>
<section id="failing-rows">
<h3>Failing rows<a class="headerlink" href="#failing-rows" title="Permalink to this heading"></a></h3>
<p>Lists the rows that fail abnormal inputs tests. Abnormal inputs testing can be <a class="reference internal" href="../for_data_scientists/reference/nlp/test_suite.html"><span class="doc std std-doc">disabled</span></a>.</p>
</section>
<section id="original-data-points">
<h3>Original data points<a class="headerlink" href="#original-data-points" title="Permalink to this heading"></a></h3>
<p>NLP and Image tests include a sample of data points when the Transformation or Attacks tests are run. Transformation or Attacks testing can be <a class="reference internal" href="../for_data_scientists/reference/nlp/test_suite.html"><span class="doc std std-doc">disabled</span></a>.</p>
<p>Realtime Events tests include every individual data point in the underlying dataset. Realtime Events testing can be disabled by using feature or license flags.</p>
</section>
<section id="software-monitoring-logs">
<h3>Software monitoring logs<a class="headerlink" href="#software-monitoring-logs" title="Permalink to this heading"></a></h3>
<p>All confidential data in software monitoring logs is masked or filtered.</p>
</section>
</section>
<section id="ri-platform-consolidated-test-database">
<h2>RI Platform consolidated test database<a class="headerlink" href="#ri-platform-consolidated-test-database" title="Permalink to this heading"></a></h2>
<table class="colwidths-given datatable docutils align-default">
<colgroup>
<col style="width: 4%" />
<col style="width: 4%" />
<col style="width: 4%" />
<col style="width: 29%" />
<col style="width: 22%" />
<col style="width: 22%" />
<col style="width: 14%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Category</p></th>
<th class="head"><p>Modality</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Why it matters</p></th>
<th class="head"><p>Configuration</p></th>
<th class="head"><p>Model Types</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Average Confidence</p></td>
<td><p>Model Performance</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test checks the average confidence of the model predictions between the reference and evaluation sets to see if the metric has experienced significant degradation. The “confidence” of a prediction for classification tasks is defined as the distance between the probability of the predicted class (defined as the argmax over the prediction vector) and 1. We average this metric across all predictions.</p></td>
<td><p>During production, factors like distribution shift or a change in &lt;span&gt;p(y|x)&lt;/span&gt; may cause model performance to decrease significantly. Since oftentimes labels are not available in a production setting, this metric can serve as a useful proxy for model performance.</p></td>
<td><p>By default, this test runs if predictions are specified (no labels required).</p></td>
<td><p>[tabular] Binary Classification, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Average Thresholded Confidence</p></td>
<td><p>Model Performance</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test checks the average thresholded confidence (ATC) of the model predictions between the reference and evaluation sets to see if the metric has experienced significant degradation. ATC is a method for estimating accuracy of unlabeled examples taken from &lt;a href=”<a class="reference external" href="https://arxiv.org/abs/2201.04234">https://arxiv.org/abs/2201.04234</a>”&gt;this paper&lt;/a&gt;. The threshold is first computed on the reference set: we pick a confidence threshold such that the percentage of datapoints whose max predicted probability is less than the threshold is around equal to the error rate of the model (here, it is 1-accuracy) on the reference set. Then, we apply this threshold in the evaluation set: the predicted accuracy is then equal to the percentage of datapoints with max predicted probability greater than this threshold.</p></td>
<td><p>During production, factors like distribution shift may cause model performance to decrease significantly. Since oftentimes labels are not available in a production setting, this metric can serve as a useful proxy for model performance.</p></td>
<td><p>By default, this test runs if predictions/labels are specified in the reference set and predictions are specified in the eval set (no labels required).</p></td>
<td><p>[tabular] Binary Classification, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Calibration Comparison</p></td>
<td><p>Model Performance</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test checks that the reference and evaluation sets have sufficiently similar calibration curves as measured by the Mean Squared Error (MSE) between the two curves. The calibration curve is a line plot where the x-axis represents the average predicted probability and the y-axis is the proportion of positive predictions. The curve of the ideal calibrated model is thus a linear straight line from (0, 0) moving linearly.</p></td>
<td><p>Knowing how well-calibrated your model is can help you better interpret and act upon model outputs, and can even be an indicator of generalization. A greater difference between reference and evaluation curves could indicate a lack of generalizability. In addition, a change in calibration could indicate that decision-making or thresholding conducted upstream needs to change as it is behaving differently on held-out data.</p></td>
<td><p>By default, this test runs over the predictions and labels.</p></td>
<td><p>[tabular] Binary Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Precision</p></td>
<td><p>Model Performance</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test checks the Precision metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of Precision has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p></td>
<td><p>During production, factors like distribution shift or a change in &lt;span&gt;p(y|x)&lt;/span&gt; may cause model performance to decrease significantly.</p></td>
<td><p>By default, this test runs over the Precision metric with the below thresholds set for the absolute and degradation tests.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Prediction Variance (Positive Labels)</p></td>
<td><p>Model Performance</p></td>
<td><p>tabular</p></td>
<td><p>This test checks the Prediction Variance (Positive Labels) metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of Prediction Variance (Positive Labels) has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p></td>
<td><p>During production, factors like distribution shift or a change in &lt;span&gt;p(y|x)&lt;/span&gt; may cause model performance to decrease significantly.</p></td>
<td><p>By default, this test runs over the Prediction Variance (Positive Labels) metric with the below thresholds set for the absolute and degradation tests.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Positive Prediction Rate</p></td>
<td><p>Model Performance</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test checks the Positive Prediction Rate metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of Positive Prediction Rate has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p></td>
<td><p>During production, factors like distribution shift or a change in &lt;span&gt;p(y|x)&lt;/span&gt; may cause model performance to decrease significantly.</p></td>
<td><p>By default, this test runs over the Positive Prediction Rate metric with the below thresholds set for the absolute and degradation tests.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>False Negative Rate</p></td>
<td><p>Model Performance</p></td>
<td><p>tabular</p></td>
<td><p>This test checks the False Negative Rate metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of False Negative Rate has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p></td>
<td><p>During production, factors like distribution shift or a change in &lt;span&gt;p(y|x)&lt;/span&gt; may cause model performance to decrease significantly.</p></td>
<td><p>By default, this test runs over the False Negative Rate metric with the below thresholds set for the absolute and degradation tests.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Mean-Absolute Error (MAE)</p></td>
<td><p>Model Performance</p></td>
<td><p>tabular</p></td>
<td><p>This test checks the Mean-Absolute Error (MAE) metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of Mean-Absolute Error (MAE) has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p></td>
<td><p>During production, factors like distribution shift or a change in &lt;span&gt;p(y|x)&lt;/span&gt; may cause model performance to decrease significantly.</p></td>
<td><p>By default, this test runs over the Mean-Absolute Error (MAE) metric with the below thresholds set for the absolute and degradation tests.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Mean-Squared-Log Error (MSLE)</p></td>
<td><p>Model Performance</p></td>
<td><p>tabular</p></td>
<td><p>This test checks the Mean-Squared-Log Error (MSLE) metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of Mean-Squared-Log Error (MSLE) has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p></td>
<td><p>During production, factors like distribution shift or a change in &lt;span&gt;p(y|x)&lt;/span&gt; may cause model performance to decrease significantly.</p></td>
<td><p>By default, this test runs over the Mean-Squared-Log Error (MSLE) metric with the below thresholds set for the absolute and degradation tests.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Multiclass Accuracy</p></td>
<td><p>Model Performance</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test checks the Multiclass Accuracy metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of Multiclass Accuracy has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p></td>
<td><p>During production, factors like distribution shift or a change in &lt;span&gt;p(y|x)&lt;/span&gt; may cause model performance to decrease significantly.</p></td>
<td><p>By default, this test runs over the Multiclass Accuracy metric with the below thresholds set for the absolute and degradation tests.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Macro Precision</p></td>
<td><p>Model Performance</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test checks the Macro Precision metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of Macro Precision has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p></td>
<td><p>During production, factors like distribution shift or a change in &lt;span&gt;p(y|x)&lt;/span&gt; may cause model performance to decrease significantly.</p></td>
<td><p>By default, this test runs over the Macro Precision metric with the below thresholds set for the absolute and degradation tests.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Rank Correlation</p></td>
<td><p>Model Performance</p></td>
<td><p>tabular</p></td>
<td><p>This test checks the Rank Correlation metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of Rank Correlation has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p></td>
<td><p>During production, factors like distribution shift or a change in &lt;span&gt;p(y|x)&lt;/span&gt; may cause model performance to decrease significantly.</p></td>
<td><p>By default, this test runs over the Rank Correlation metric with the below thresholds set for the absolute and degradation tests.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>F1</p></td>
<td><p>Model Performance</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test checks the F1 metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of F1 has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p></td>
<td><p>During production, factors like distribution shift or a change in &lt;span&gt;p(y|x)&lt;/span&gt; may cause model performance to decrease significantly.</p></td>
<td><p>By default, this test runs over the F1 metric with the below thresholds set for the absolute and degradation tests.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Prediction Variance (Negative Labels)</p></td>
<td><p>Model Performance</p></td>
<td><p>tabular</p></td>
<td><p>This test checks the Prediction Variance (Negative Labels) metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of Prediction Variance (Negative Labels) has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p></td>
<td><p>During production, factors like distribution shift or a change in &lt;span&gt;p(y|x)&lt;/span&gt; may cause model performance to decrease significantly.</p></td>
<td><p>By default, this test runs over the Prediction Variance (Negative Labels) metric with the below thresholds set for the absolute and degradation tests.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>False Positive Rate</p></td>
<td><p>Model Performance</p></td>
<td><p>tabular</p></td>
<td><p>This test checks the False Positive Rate metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of False Positive Rate has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p></td>
<td><p>During production, factors like distribution shift or a change in &lt;span&gt;p(y|x)&lt;/span&gt; may cause model performance to decrease significantly.</p></td>
<td><p>By default, this test runs over the False Positive Rate metric with the below thresholds set for the absolute and degradation tests.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Root-Mean-Squared Error (RMSE)</p></td>
<td><p>Model Performance</p></td>
<td><p>tabular</p></td>
<td><p>This test checks the Root-Mean-Squared Error (RMSE) metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of Root-Mean-Squared Error (RMSE) has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p></td>
<td><p>During production, factors like distribution shift or a change in &lt;span&gt;p(y|x)&lt;/span&gt; may cause model performance to decrease significantly.</p></td>
<td><p>By default, this test runs over the Root-Mean-Squared Error (RMSE) metric with the below thresholds set for the absolute and degradation tests.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Average Prediction</p></td>
<td><p>Model Performance</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test checks the Average Prediction metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of Average Prediction has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p></td>
<td><p>During production, factors like distribution shift or a change in &lt;span&gt;p(y|x)&lt;/span&gt; may cause model performance to decrease significantly.</p></td>
<td><p>By default, this test runs over the Average Prediction metric with the below thresholds set for the absolute and degradation tests.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Mean-Absolute Percentage Error (MAPE)</p></td>
<td><p>Model Performance</p></td>
<td><p>tabular</p></td>
<td><p>This test checks the Mean-Absolute Percentage Error (MAPE) metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of Mean-Absolute Percentage Error (MAPE) has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p></td>
<td><p>During production, factors like distribution shift or a change in &lt;span&gt;p(y|x)&lt;/span&gt; may cause model performance to decrease significantly.</p></td>
<td><p>By default, this test runs over the Mean-Absolute Percentage Error (MAPE) metric with the below thresholds set for the absolute and degradation tests.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Average Rank</p></td>
<td><p>Model Performance</p></td>
<td><p>tabular</p></td>
<td><p>This test checks the Average Rank metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of Average Rank has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p></td>
<td><p>During production, factors like distribution shift or a change in &lt;span&gt;p(y|x)&lt;/span&gt; may cause model performance to decrease significantly.</p></td>
<td><p>By default, this test runs over the Average Rank metric with the below thresholds set for the absolute and degradation tests.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Prediction Variance</p></td>
<td><p>Model Performance</p></td>
<td><p>tabular</p></td>
<td><p>This test checks the Prediction Variance metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of Prediction Variance has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p></td>
<td><p>During production, factors like distribution shift or a change in &lt;span&gt;p(y|x)&lt;/span&gt; may cause model performance to decrease significantly.</p></td>
<td><p>By default, this test runs over the Prediction Variance metric with the below thresholds set for the absolute and degradation tests.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Macro F1</p></td>
<td><p>Model Performance</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test checks the Macro F1 metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of Macro F1 has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p></td>
<td><p>During production, factors like distribution shift or a change in &lt;span&gt;p(y|x)&lt;/span&gt; may cause model performance to decrease significantly.</p></td>
<td><p>By default, this test runs over the Macro F1 metric with the below thresholds set for the absolute and degradation tests.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Normalized Discounted Cumulative Gain (NDCG)</p></td>
<td><p>Model Performance</p></td>
<td><p>tabular</p></td>
<td><p>This test checks the Normalized Discounted Cumulative Gain (NDCG) metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of Normalized Discounted Cumulative Gain (NDCG) has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p></td>
<td><p>During production, factors like distribution shift or a change in &lt;span&gt;p(y|x)&lt;/span&gt; may cause model performance to decrease significantly.</p></td>
<td><p>By default, this test runs over the Normalized Discounted Cumulative Gain (NDCG) metric with the below thresholds set for the absolute and degradation tests.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>AUC</p></td>
<td><p>Model Performance</p></td>
<td><p>tabular</p></td>
<td><p>This test checks the AUC metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of AUC has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p></td>
<td><p>During production, factors like distribution shift or a change in &lt;span&gt;p(y|x)&lt;/span&gt; may cause model performance to decrease significantly.</p></td>
<td><p>By default, this test runs over the AUC metric with the below thresholds set for the absolute and degradation tests.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Recall</p></td>
<td><p>Model Performance</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test checks the Recall metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of Recall has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p></td>
<td><p>During production, factors like distribution shift or a change in &lt;span&gt;p(y|x)&lt;/span&gt; may cause model performance to decrease significantly.</p></td>
<td><p>By default, this test runs over the Recall metric with the below thresholds set for the absolute and degradation tests.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Accuracy</p></td>
<td><p>Model Performance</p></td>
<td><p>tabular</p></td>
<td><p>This test checks the Accuracy metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of Accuracy has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p></td>
<td><p>During production, factors like distribution shift or a change in &lt;span&gt;p(y|x)&lt;/span&gt; may cause model performance to decrease significantly.</p></td>
<td><p>By default, this test runs over the Accuracy metric with the below thresholds set for the absolute and degradation tests.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Mean-Squared Error (MSE)</p></td>
<td><p>Model Performance</p></td>
<td><p>tabular</p></td>
<td><p>This test checks the Mean-Squared Error (MSE) metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of Mean-Squared Error (MSE) has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p></td>
<td><p>During production, factors like distribution shift or a change in &lt;span&gt;p(y|x)&lt;/span&gt; may cause model performance to decrease significantly.</p></td>
<td><p>By default, this test runs over the Mean-Squared Error (MSE) metric with the below thresholds set for the absolute and degradation tests.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Macro Recall</p></td>
<td><p>Model Performance</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test checks the Macro Recall metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of Macro Recall has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p></td>
<td><p>During production, factors like distribution shift or a change in &lt;span&gt;p(y|x)&lt;/span&gt; may cause model performance to decrease significantly.</p></td>
<td><p>By default, this test runs over the Macro Recall metric with the below thresholds set for the absolute and degradation tests.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Mean Reciprocal Rank (MRR)</p></td>
<td><p>Model Performance</p></td>
<td><p>tabular</p></td>
<td><p>This test checks the Mean Reciprocal Rank (MRR) metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of Mean Reciprocal Rank (MRR) has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p></td>
<td><p>During production, factors like distribution shift or a change in &lt;span&gt;p(y|x)&lt;/span&gt; may cause model performance to decrease significantly.</p></td>
<td><p>By default, this test runs over the Mean Reciprocal Rank (MRR) metric with the below thresholds set for the absolute and degradation tests.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Multiclass AUC</p></td>
<td><p>Model Performance</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test checks the Multiclass AUC metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of Multiclass AUC has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p></td>
<td><p>During production, factors like distribution shift or a change in &lt;span&gt;p(y|x)&lt;/span&gt; may cause model performance to decrease significantly.</p></td>
<td><p>By default, this test runs over the Multiclass AUC metric with the below thresholds set for the absolute and degradation tests.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Protected Feature Drift</p></td>
<td><p>Bias and Fairness</p></td>
<td><p>tabular</p></td>
<td><p>This test measures the severity of passing to the model data points that have categorical features which have drifted from the distribution observed in the reference set. The severity is a function of the impact on the model, as well as the presence of drift in the data. The model impact measures how much model performance changes due to drift in the given feature. The key detail displayed is the PSI test statistic, which is a measure of how statistically significant the difference between the frequencies of categorical values in the reference and evaluation sets is.</p></td>
<td><p>Distribution drift in categorical features between training and inference can be caused by a variety of factors, including a change in the data generation process or a change in the preprocessing pipeline. A big shift in categorical features towards categorical subsets that your model performs poorly in could indicate a degradation in model performance and signal the need for relabeling and retraining.</p></td>
<td><p>By default, this test runs over all categorical columns with sufficiently many samples.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Selection Rate</p></td>
<td><p>Bias and Fairness</p></td>
<td><p>tabular</p></td>
<td><p>This test checks whether the Selection Rate for any subset of a feature performs as well as the best Selection Rate across all subsets of that feature. The Selection Rate is calculated as the Positive Prediction Rate. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the Selection Rate of model predictions within a specific subset is significantly lower than that of other subsets by taking a ratio of the rates.</p></td>
<td><p>Assessing differences in Selection Rate is an important measures of fairness. It is meant to be used in a setting where we assert that the base Selection Rates between subgroups should be the same (even if empirically they are different). This contrasts with equality of opportunity or predictive parity tests, which permit classification rates to depend on a sensitive attribute. It can be useful in legal/compliance settings where we want a Selection Rate for any sensitive group to fundamentally be the same as other groups.</p></td>
<td><p>By default, the Selection Rate is computed for all protected features.</p></td>
<td><p>[tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Selection Rate (Avg Pred)</p></td>
<td><p>Bias and Fairness</p></td>
<td><p>tabular</p></td>
<td><p>This test checks whether the Average Prediction for any subset of a feature performs as well as the best Average Prediction across all subsets of that feature.  The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the Average Prediction of model predictions within a specific subset is significantly lower than that of other subsets by taking a ratio of the rates.</p></td>
<td><p>Assessing differences in Average Prediction is an important measures of fairness. It is meant to be used in a setting where we assert that the base Average Predictions between subgroups should be the same (even if empirically they are different). This contrasts with equality of opportunity or predictive parity tests, which permit classification rates to depend on a sensitive attribute. It can be useful in legal/compliance settings where we want a Average Prediction for any sensitive group to fundamentally be the same as other groups.</p></td>
<td><p>By default, the Average Prediction is computed for all protected features.</p></td>
<td><p>[tabular] Regression</p></td>
</tr>
<tr class="row-even"><td><p>Class Imbalance</p></td>
<td><p>Bias and Fairness</p></td>
<td><p>tabular</p></td>
<td><p>This test checks whether the training sample size for any subset of a feature is significantly smaller than other subsets of that feature. The test first splits the dataset into various subset classes within the feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the class imbalance measure of that subset compared to the largest subset exceeds a set threshold.</p></td>
<td><p>Assessing class imbalance is an important measure of fairness. Features with low subset sizes can result in the model overfitting those subsets, and hence cause a larger error when those subsets appear in test data. This test can be useful in legal/compliance settings where sufficient data for all subsets of a protected feature is important.</p></td>
<td><p>By default, class imbalance is tested for all protected features.</p></td>
<td><p>[tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Feature Independence</p></td>
<td><p>Bias and Fairness</p></td>
<td><p>tabular</p></td>
<td><p>This test checks the independence of each protected feature with the predicted label class. It runs over categorical protected features and uses the chi square test of independence to determine the feature independence. The test compares the observed data to a model that distributes the data according to the expectation that the variables are independent. Wherever the observed data does not fit the model, the likelihood that the variables are dependent becomes stronger.</p></td>
<td><p>A test of independence assesses whether observations consisting of measures on two variables, expressed in a contingency table, are independent of each other. This can be useful when assessing how protected features impact the predicted class and helping with the feature selection process.</p></td>
<td><p>By default, this test is run over all protected categorical features.</p></td>
<td><p>[tabular] Binary Classification, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Predict Protected Features</p></td>
<td><p>Bias and Fairness</p></td>
<td><p>tabular</p></td>
<td><p>The Predict Protected Features test works by training a multi-class logistic regression model to infer categorical protected features from unprotected categorical and numerical features. The model is fit to the reference data and scored based on its accuracy over the evaluation data. The unprotected categorical features are one-hot encoded.</p></td>
<td><p>In a compliance setting, it may be prohibited to include certain protected features in your training data. However, unprotected features might still provide your model with information about the protected features. If a simple logistic regression model can be trained to accurately predict protected features, your model might have a hidden reliance on protected features, resulting in biased decisions.</p></td>
<td><p>By default, the selection rate is computed for all protected features.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Demographic Parity (Pos Pred)</p></td>
<td><p>Bias and Fairness</p></td>
<td><p>tabular</p></td>
<td><p>This test is commonly known as the demographic parity or statistical parity test in fairness literature. This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the Positive Prediction Rate of model predictions within a specific subset is significantly different than the model prediction Positive Prediction Rate over the entire population.</p></td>
<td><p>Demographic parity is one of the most well-known and strict measures of fairness. It is meant to be used in a setting where we assert that the base label rates between subgroups should be the same (even if empirically they are different). This contrasts with equality of opportunity or predictive parity tests, which permit classification rates to depend on a protected attribute. It can be useful in legal/compliance settings where we want a Selection Rate for any protected group to fundamentally be the same as other groups.</p></td>
<td><p>By default, the Positive Prediction Rate is computed for all protected features.</p></td>
<td><p>[tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Demographic Parity (Avg Pred)</p></td>
<td><p>Bias and Fairness</p></td>
<td><p>tabular</p></td>
<td><p>This test is commonly known as the demographic parity or statistical parity test in fairness literature. This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the Average Prediction of model predictions within a specific subset is significantly different than the model prediction Average Prediction over the entire population.</p></td>
<td><p>Demographic parity is one of the most well-known and strict measures of fairness. It is meant to be used in a setting where we assert that the base label rates between subgroups should be the same (even if empirically they are different). This contrasts with equality of opportunity or predictive parity tests, which permit classification rates to depend on a protected attribute. It can be useful in legal/compliance settings where we want a Selection Rate for any protected group to fundamentally be the same as other groups.</p></td>
<td><p>By default, the Average Prediction is computed for all protected features.</p></td>
<td><p>[tabular] Regression, [tabular] Ranking</p></td>
</tr>
<tr class="row-odd"><td><p>Demographic Parity (Avg Rank)</p></td>
<td><p>Bias and Fairness</p></td>
<td><p>tabular</p></td>
<td><p>This test is commonly known as the demographic parity or statistical parity test in fairness literature. This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the Average Rank of model predictions within a specific subset is significantly different than the model prediction Average Rank over the entire population.</p></td>
<td><p>Demographic parity is one of the most well-known and strict measures of fairness. It is meant to be used in a setting where we assert that the base label rates between subgroups should be the same (even if empirically they are different). This contrasts with equality of opportunity or predictive parity tests, which permit classification rates to depend on a protected attribute. It can be useful in legal/compliance settings where we want a Selection Rate for any protected group to fundamentally be the same as other groups.</p></td>
<td><p>By default, the Average Rank is computed for all protected features.</p></td>
<td><p>[tabular] Ranking</p></td>
</tr>
<tr class="row-even"><td><p>Equal Opportunity (Recall)</p></td>
<td><p>Bias and Fairness</p></td>
<td><p>tabular</p></td>
<td><p>The recall test is more popularly referred to as equal opportunity or false negative error rate balance in fairness literature. This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the Recall of model predictions within a specific subset is significantly lower than the model prediction Recall over the entire population.</p></td>
<td><p>Having different true positive rates (e.g. equal opportunity) between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. Unlike demographic parity, this test permits assuming different base label rates but flags differing mistake rates between different subgroups. An intuitive example is when the label indicates a positive attribute: if predicting whether to interview a given candidate, make sure that out of qualified candidates, the rate at which the model predicts a rejection is similar to group A and B.</p></td>
<td><p>By default, Recall is computed over all predictions/labels. Note that we round predictions to 0/1 to compute recall.</p></td>
<td><p>[tabular] Binary Classification, [tabular] Ranking</p></td>
</tr>
<tr class="row-odd"><td><p>Equal Opportunity (Macro Recall)</p></td>
<td><p>Bias and Fairness</p></td>
<td><p>tabular</p></td>
<td><p>The recall test is more popularly referred to as equal opportunity or false negative error rate balance in fairness literature. When transitioning to the multiclass setting we can use macro recall which computes the recall of each individual class and then averages these numbers.This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the Macro Recall of model predictions within a specific subset is significantly lower than the model prediction Macro Recall over the entire population.</p></td>
<td><p>Having different true positive rates (e.g. equal opportunity) between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. Unlike demographic parity, this test permits assuming different base label rates but flags differing mistake rates between different subgroups. An intuitive example is when the label indicates a positive attribute: if predicting whether to interview a given candidate, make sure that out of qualified candidates, the rate at which the model predicts an interview is similar to group A and B.</p></td>
<td><p>By default, Macro Recall is computed over all predictions/labels. Note that the predicted label is the label with the largest predicted class probability.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Intersectional Group Fairness (Pos Pred)</p></td>
<td><p>Bias and Fairness</p></td>
<td><p>tabular</p></td>
<td><p>This test checks whether the model performs equally well across subgroups created from the intersection of protected groups. The test first creates unique pairs of categorical protected features. We then test whether the positive prediction rate of model predictions within a specific subset is significantly lower than the model positive prediction rate over the entire population. This will expose hidden biases against groups at the intersection of these protected features</p></td>
<td><p>Most existing work in the fairness literature deals with a binary view of fairness - either a particular group is performing worse or not. This binary categorization misses the important nuance of the fairness field - that biases can often be amplified in subgroups that combine membership from different protected groups, especially if such a subgroup is particularly underrepresented in opportunities historically. The intersectional group fairness test is run over subsets representing this intersection between two protected groups.</p></td>
<td><p>This test runs over unique pairs of categorical protected features.</p></td>
<td><p>[tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Intersectional Group Fairness (Avg Pred)</p></td>
<td><p>Bias and Fairness</p></td>
<td><p>tabular</p></td>
<td><p>This test checks whether the model performs equally well across subgroups created from the intersection of protected groups. The test first creates unique pairs of categorical protected features. We then test whether the average prediction of model predictions within a specific subset is significantly lower than the model average prediction over the entire population. This will expose hidden biases against groups at the intersection of these protected features</p></td>
<td><p>Most existing work in the fairness literature deals with a binary view of fairness - either a particular group is performing worse or not. This binary categorization misses the important nuance of the fairness field - that biases can often be amplified in subgroups that combine membership from different protected groups, especially if such a subgroup is particularly underrepresented in opportunities historically. The intersectional group fairness test is run over subsets representing this intersection between two protected groups.</p></td>
<td><p>This test runs over unique pairs of categorical protected features.</p></td>
<td><p>[tabular] Regression, [tabular] Ranking</p></td>
</tr>
<tr class="row-even"><td><p>Intersectional Group Fairness (Avg Rank)</p></td>
<td><p>Bias and Fairness</p></td>
<td><p>tabular</p></td>
<td><p>This test checks whether the model performs equally well across subgroups created from the intersection of protected groups. The test first creates unique pairs of categorical protected features. We then test whether the average rank of model predictions within a specific subset is significantly lower than the model average rank over the entire population. This will expose hidden biases against groups at the intersection of these protected features</p></td>
<td><p>Most existing work in the fairness literature deals with a binary view of fairness - either a particular group is performing worse or not. This binary categorization misses the important nuance of the fairness field - that biases can often be amplified in subgroups that combine membership from different protected groups, especially if such a subgroup is particularly underrepresented in opportunities historically. The intersectional group fairness test is run over subsets representing this intersection between two protected groups.</p></td>
<td><p>This test runs over unique pairs of categorical protected features.</p></td>
<td><p>[tabular] Ranking</p></td>
</tr>
<tr class="row-odd"><td><p>Predictive Equality (FPR)</p></td>
<td><p>Bias and Fairness</p></td>
<td><p>tabular</p></td>
<td><p>The false positive error rate test is also popularly referred to as as predictive equality, or equal mis-opportunity in fairness literature. This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the false positive rate of model predictions within a specific subset is significantly higher than the model prediction false positive rate over the entire population.</p></td>
<td><p>Having different false positive rates (e.g. predictive equality) between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. Unlike demographic parity, this test permits assuming different base label rates but flags differing mistake rates between different subgroups. As an intuitive example, consider the case when the label indicates an undesirable attribute: if predicting whether a person will default on their loan, make sure that for people who didn’t default, the rate at which the model incorrectly predicts positive is similar for group A and B.</p></td>
<td><p>By default, false positive rate is computed over all predictions/labels. Note that we round predictions to 0/1 to compute false positive rate.</p></td>
<td><p>[tabular] Binary Classification, [tabular] Ranking</p></td>
</tr>
<tr class="row-even"><td><p>Discrimination By Proxy</p></td>
<td><p>Bias and Fairness</p></td>
<td><p>tabular</p></td>
<td><p>This test checks whether any feature is a proxy for a protected feature. It runs over categorical features, using mutual information as a measure of similarity with a protected feature. Mutual information measures any dependencies between two variables.</p></td>
<td><p>A common strategy to try to ensure a model is not biased is to remove protected features from the training data entirely so the model cannot learn over them. However, if other features are highly dependent on those features, that could lead to the model effectively still training over those features by proxy.</p></td>
<td><p>By default, this test is run over all categorical protected columns.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Subset Sensitivity (Pos Pred)</p></td>
<td><p>Bias and Fairness</p></td>
<td><p>tabular</p></td>
<td><p>This test measures how sensitive the model is to substituting the lowest performing subset of a feature into a sample of data. The test splits the dataset into various subsets based on the feature values and finds the lowest performing subset, based on the lowest Positive Prediction Rate. The test then substitutes this subset into a sample from the original data and calculates the change in Positive Prediction Rate. This test fails if a model demonstrates significantly lower Positive Prediction Rate on the lowest performing subset.</p></td>
<td><p>Assessing differences in model output is an important measure of fairness. If the model performs worse because of the value of a protected feature such as race or gender, then this could indicate bias. It can be useful in legal/compliance settings where we fundamentally want the prediction for any protected group to be the same as for other groups.</p></td>
<td><p>By default, the subset sensitivity is computed for all protected features that are strings.</p></td>
<td><p>[tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Subset Sensitivity (Avg Pred)</p></td>
<td><p>Bias and Fairness</p></td>
<td><p>tabular</p></td>
<td><p>This test measures how sensitive the model is to substituting the lowest performing subset of a feature into a sample of data. The test splits the dataset into various subsets based on the feature values and finds the lowest performing subset, based on the lowest Average Prediction. The test then substitutes this subset into a sample from the original data and calculates the change in Average Prediction. This test fails if a model demonstrates significantly lower Average Prediction on the lowest performing subset.</p></td>
<td><p>Assessing differences in model output is an important measure of fairness. If the model performs worse because of the value of a protected feature such as race or gender, then this could indicate bias. It can be useful in legal/compliance settings where we fundamentally want the prediction for any protected group to be the same as for other groups.</p></td>
<td><p>By default, the subset sensitivity is computed for all protected features that are strings.</p></td>
<td><p>[tabular] Regression, [tabular] Ranking</p></td>
</tr>
<tr class="row-odd"><td><p>Subset Sensitivity (Avg Rank)</p></td>
<td><p>Bias and Fairness</p></td>
<td><p>tabular</p></td>
<td><p>This test measures how sensitive the model is to substituting the lowest performing subset of a feature into a sample of data. The test splits the dataset into various subsets based on the feature values and finds the lowest performing subset, based on the lowest Average Rank. The test then substitutes this subset into a sample from the original data and calculates the change in Average Rank. This test fails if a model demonstrates significantly lower Average Rank on the lowest performing subset.</p></td>
<td><p>Assessing differences in model output is an important measure of fairness. If the model performs worse because of the value of a protected feature such as race or gender, then this could indicate bias. It can be useful in legal/compliance settings where we fundamentally want the prediction for any protected group to be the same as for other groups.</p></td>
<td><p>By default, the subset sensitivity is computed for all protected features that are strings.</p></td>
<td><p>[tabular] Ranking</p></td>
</tr>
<tr class="row-even"><td><p>Out of Range Substitution</p></td>
<td><p>Transformations</p></td>
<td><p>tabular</p></td>
<td><p>This test measures the impact on the model when we substitute values outside the inferred range of allowed values into clean datapoints.</p></td>
<td><p>In production, the model may encounter corrupted or manipulated out of range values. It is important that the model is robust to such extremities.</p></td>
<td><p>By default, this test runs over all numeric features.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Numeric Outliers Substitution</p></td>
<td><p>Transformations</p></td>
<td><p>tabular</p></td>
<td><p>This test measures the impact on the model when we substitute outliers into clean datapoints. Outliers are values which may not necessarily be outside of an allowed range for a feature, but are extreme values that are unusual and may be indicative of abnormality.</p></td>
<td><p>Outliers can be a sign of corrupted or otherwise erroneous data, and can degrade model performance if used in the training data, or lead to unexpected behaviour if input at inference time.</p></td>
<td><p>By default this test is run over each numeric feature that is neither unique nor ascending.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Int Feature Type Change</p></td>
<td><p>Transformations</p></td>
<td><p>tabular</p></td>
<td><p>This test measures the impact on the model when we substitute values not of type Integer into features that are inferred to be Integer type from the reference set. In this specific test, we add a decimal value to the integer to convert it to a float.</p></td>
<td><p>A feature may require a specific type. However, errors in the data pipeline may produce values that are outside the expected type. Failing to catch such errors may lead to errors or undefined behavior from the model.</p></td>
<td><p>By default, this test runs over all features that are inferred to be type Integer.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Float Feature Type Change</p></td>
<td><p>Transformations</p></td>
<td><p>tabular</p></td>
<td><p>This test measures the impact on the model when we substitute values not of type Float into features that are inferred to be Float type from the reference set. In this specific test, we cast the float as a string (2.3 becomes ‘2.3’)</p></td>
<td><p>A feature may require a specific type. However, errors in the data pipeline may produce values that are outside the expected type. Failing to catch such errors may lead to errors or undefined behavior from the model.</p></td>
<td><p>By default, this test runs over all features that are inferred to be type Float.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>String Feature Type Change</p></td>
<td><p>Transformations</p></td>
<td><p>tabular</p></td>
<td><p>This test measures the impact on the model when we substitute values not of type String Categorical into features that are inferred to be String Categorical type from the reference set. In this specific test, we fix a random integer as the input.</p></td>
<td><p>A feature may require a specific type. However, errors in the data pipeline may produce values that are outside the expected type. Failing to catch such errors may lead to errors or undefined behavior from the model.</p></td>
<td><p>By default, this test runs over all features that are inferred to be type String Categorical.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Boolean Feature Type Change</p></td>
<td><p>Transformations</p></td>
<td><p>tabular</p></td>
<td><p>This test measures the impact on the model when we substitute values not of type Boolean Categorical into features that are inferred to be Boolean Categorical type from the reference set. In this specific test, we randomly fix an integer as the input.</p></td>
<td><p>A feature may require a specific type. However, errors in the data pipeline may produce values that are outside the expected type. Failing to catch such errors may lead to errors or undefined behavior from the model.</p></td>
<td><p>By default, this test runs over all features that are inferred to be type Boolean Categorical.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>URL Feature Type Change</p></td>
<td><p>Transformations</p></td>
<td><p>tabular</p></td>
<td><p>This test measures the impact on the model when we substitute values not of type URL Categorical into features that are inferred to be URL Categorical type from the reference set. In this specific test, we create a random string and fix that as the input.</p></td>
<td><p>A feature may require a specific type. However, errors in the data pipeline may produce values that are outside the expected type. Failing to catch such errors may lead to errors or undefined behavior from the model.</p></td>
<td><p>By default, this test runs over all features that are inferred to be type URL Categorical.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Domain Feature Type Change</p></td>
<td><p>Transformations</p></td>
<td><p>tabular</p></td>
<td><p>This test measures the impact on the model when we substitute values not of type Domain Categorical into features that are inferred to be Domain Categorical type from the reference set. In this specific test, we create a random string and fix that as the input.</p></td>
<td><p>A feature may require a specific type. However, errors in the data pipeline may produce values that are outside the expected type. Failing to catch such errors may lead to errors or undefined behavior from the model.</p></td>
<td><p>By default, this test runs over all features that are inferred to be type Domain Categorical.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Email Feature Type Change</p></td>
<td><p>Transformations</p></td>
<td><p>tabular</p></td>
<td><p>This test measures the impact on the model when we substitute values not of type Email Categorical into features that are inferred to be Email Categorical type from the reference set. In this specific test, we create a random string and fix that as the input.</p></td>
<td><p>A feature may require a specific type. However, errors in the data pipeline may produce values that are outside the expected type. Failing to catch such errors may lead to errors or undefined behavior from the model.</p></td>
<td><p>By default, this test runs over all features that are inferred to be type Email Categorical.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Empty String Substitution</p></td>
<td><p>Transformations</p></td>
<td><p>tabular</p></td>
<td><p>This test measures the impact on the model when we substitute empty string values instead of null values into clean datapoints.</p></td>
<td><p>In production, the model may encounter corrupted or manipulated string values. Null values and empty strings are often expected to be treated the same, but the model might not treat them that way. It is important that the model is robust to such extremities.</p></td>
<td><p>By default, this test runs over all string features with null values.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Required Characters Deletion</p></td>
<td><p>Transformations</p></td>
<td><p>tabular</p></td>
<td><p>This test measures the impact on the model when we delete required characters, inferred from the reference set, from the strings of clean datapoints.</p></td>
<td><p>A feature may require specific characters. However, errors in the data pipeline may allow invalid data points that lack these required characters to pass. Failing to catch such errors may lead to noisier training data or noisier predictions during inference, which can degrade model metrics.</p></td>
<td><p>By default, this test runs over all string features that are inferred to have required characters.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Unseen Categorical Substitution</p></td>
<td><p>Transformations</p></td>
<td><p>tabular</p></td>
<td><p>This test measures the impact on the model when we substitute unseen categorical values into clean datapoints.</p></td>
<td><p>Unseen categorical values are a common failure point in machine learning systems; since these models are trained over a reference set, they may yield uninterpretable or undefined behavior when interacting with an unseen categorical value. In addition, such errors may expose gaps or errors in data collection.</p></td>
<td><p>By default, this test runs over all categorical features.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Unseen Domain Substitution</p></td>
<td><p>Transformations</p></td>
<td><p>tabular</p></td>
<td><p>This test measures the impact on the model when we substitute unseen domain values into clean datapoints.</p></td>
<td><p>Unseen categorical values are a common failure point in machine learning systems; since these models are trained over a reference set, they may yield uninterpretable or undefined behavior when interacting with an unseen categorical value. In addition, such errors may expose gaps or errors in data collection.</p></td>
<td><p>By default, this test runs over all features inferred to contain domains.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Unseen Email Substitution</p></td>
<td><p>Transformations</p></td>
<td><p>tabular</p></td>
<td><p>This test measures the impact on the model when we substitute unseen email values into clean datapoints.</p></td>
<td><p>Unseen categorical values are a common failure point in machine learning systems; since these models are trained over a reference set, they may yield uninterpretable or undefined behavior when interacting with an unseen categorical value. In addition, such errors may expose gaps or errors in data collection.</p></td>
<td><p>By default, this test runs over all features inferred to contain emails.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Unseen URL Substitution</p></td>
<td><p>Transformations</p></td>
<td><p>tabular</p></td>
<td><p>This test measures the impact on the model when we substitute unseen URL values into clean datapoints.</p></td>
<td><p>Unseen categorical values are a common failure point in machine learning systems; since these models are trained over a reference set, they may yield uninterpretable or undefined behavior when interacting with an unseen categorical value. In addition, such errors may expose gaps or errors in data collection.</p></td>
<td><p>By default, this test runs over all features inferred to contain URLs.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Null Substitution</p></td>
<td><p>Transformations</p></td>
<td><p>tabular</p></td>
<td><p>This test measures the impact on the model when we substitute nulls in features that should not have nulls into clean datapoints.</p></td>
<td><p>The model may make certain assumptions about a column depending on whether or not it had nulls in the training data. If these assumptions break during production, this may damage the model’s performance. For example, if a column was never null during training then a model may not have learned to be robust against noise in that column.</p></td>
<td><p>By default, this test runs over all columns that had zero nulls in the reference set.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Capitalization Change</p></td>
<td><p>Transformations</p></td>
<td><p>tabular</p></td>
<td><p>This test measures the impact on the model when we substitute different types of capitalization into clean datapoints.</p></td>
<td><p>In production, models can come across the same value with different capitalizations, making it important to explicitly check that your model is invariant to such differences.</p></td>
<td><p>By default, this test runs over all categorical features.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Correlation Drift (Feature-to-Feature)</p></td>
<td><p>Drift</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test measures the severity of feature-feature correlation drift from the reference to the evaluation set for a given pair of features. The severity is a function of the correlation drift in the data. The key detail is the difference in correlation scores between the reference and evaluation sets, along with an associated p-value. Correlation is a measure of the linear relationship between two numeric columns (feature-feature) so this test checks for significant changes in this relationship between each feature-feature in the reference and evaluation sets. To compute the p-value, we use Fisher’s z-transformation to convert the distribution of sample correlations to a normal distribution, and then we run a standard two-sample test on two normal distributions.</p></td>
<td><p>Correlation drift between training and inference can be caused by a variety of factors, including a change in the data generation process or a change in the underlying processing stage. A big shift in these dependencies could indicate shifting datasets and degradation in model performance, signaling the need for relabeling and retraining.</p></td>
<td><p>By default, this test runs over all pairs of features in the dataset.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Correlation Drift (Feature-to-Label)</p></td>
<td><p>Drift</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test measures the severity of feature-label correlation drift from the reference to the evaluation set for a given pair of a feature and label. The severity is a function of the correlation drift in the data. The key detail is the difference in correlation scores between the reference and evaluation sets, along with an associated p-value. Correlation is a measure of the linear relationship between two numeric columns (feature-label) so this test checks for significant changes in this relationship between each feature-label in the reference and evaluation sets. To compute the p-value, we use Fisher’s z-transformation to convert the distribution of sample correlations to a normal distribution, and then we run a standard two-sample test on two normal distributions.</p></td>
<td><p>Correlation drift between training and inference can be caused by a variety of factors, including a change in the data generation process or a change in the underlying processing stage. A big shift in these dependencies could indicate shifting datasets and degradation in model performance, signaling the need for relabeling and retraining.</p></td>
<td><p>By default, this test runs over all pairs of features and labels in the dataset.</p></td>
<td><p>[tabular] Regression</p></td>
</tr>
<tr class="row-odd"><td><p>Mutual Information Drift (Feature-to-Feature)</p></td>
<td><p>Drift</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test measures the severity of feature mutual information drift from the reference to the evaluation set for a given pair of features. The severity is a function of the mutual information drift in the data. The key detail is the difference in mutual information scores between the reference and evaluation sets. Mutual information is a measure of how dependent two features are, so this checks for significant changes in dependence between pairs of features in the reference and evaluation sets.</p></td>
<td><p>Mutual information drift between training and inference can be caused by a variety of factors, including a change in the data generation process or a change in the underlying processing stage. A big shift in these dependencies could indicate shifting datasets and degradation in model performance, signaling the need for relabeling and retraining.</p></td>
<td><p>By default, this test runs over all pairs of features in the dataset.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Mutual Information Drift (Feature-to-Label)</p></td>
<td><p>Drift</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test measures the severity of feature mutual information drift from the reference to the evaluation set for a given pair of features. The severity is a function of the mutual information drift in the data. The key detail is the difference in mutual information scores between the reference and evaluation sets. Mutual information is a measure of how dependent two features are, so this checks for significant changes in dependence between pairs of features in the reference and evaluation sets.</p></td>
<td><p>Mutual information drift between training and inference can be caused by a variety of factors, including a change in the data generation process or a change in the underlying processing stage. A big shift in these dependencies could indicate shifting datasets and degradation in model performance, signaling the need for relabeling and retraining.</p></td>
<td><p>By default, this test runs over all pairs of features in the dataset.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Label Drift (Categorical)</p></td>
<td><p>Drift</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test checks that the difference in label distribution between the reference and evaluation sets is small, using PSI test. The key detail displayed is the PSI statistic which is a measure of how different the frequencies of the column in the reference and evaluation sets are.</p></td>
<td><p>Label distribution shift between reference and test can indicate that the underlying data distribution has changed significantly enough to modify model decisions. This may mean that the model needs to be retrained to adjust to the new data environment. In addition, significant label distribution shift may indicate that upstream decision-making modules (e.g. thresholds) may need to be updated.</p></td>
<td><p>This test is run by default whenever both the reference and evaluation sets have associated labels.</p></td>
<td><p>[tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Predicted Label Drift</p></td>
<td><p>Drift</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test checks that the difference in predicted label distribution between the reference and evaluation sets is small, using PSI test. The key detail displayed is the PSI statistic which is a measure of how different the frequencies of the column in the reference and evaluation sets are.</p></td>
<td><p>Predicted Label distribution shift between reference and test can indicate that the underlying data distribution has changed significantly enough to modify model decisions. This may mean that the model needs to be retrained to adjust to the new data environment. In addition, significant predicted label distribution shift may indicate that upstream decision-making modules (e.g. thresholds) may need to be updated.</p></td>
<td><p>This test is run by default whenever the model or predictions is provided.</p></td>
<td><p>tabular Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Label Drift (Regression)</p></td>
<td><p>Drift</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test checks that the difference in label distribution between the reference and evaluation sets is small, using the PSI test. The key detail displayed is the KS statistic which is a measure of how different the labels in the reference and evaluation sets are. Concretely, the KS statistic is the maximum difference of the empirical CDF’s of the two label columns.</p></td>
<td><p>Label distribution shift between reference and test can indicate that the underlying data distribution has changed significantly enough to modify model decisions. This may mean that the model needs to be retrained to adjust to the new data environment. In addition, significant label distribution shift may indicate that upstream decision-making modules (e.g. thresholds) may need to be updated.</p></td>
<td><p>This test is run by default whenever both the reference and evaluation sets have associated labels.</p></td>
<td><p>[tabular] Regression, [tabular] Ranking</p></td>
</tr>
<tr class="row-even"><td><p>Categorical Feature Drift</p></td>
<td><p>Drift</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test measures the severity of passing to the model data points that have categorical features which have drifted from the distribution observed in the reference set. The severity is a function of the impact on the model, as well as the presence of drift in the data. The model impact measures how much model performance changes due to drift in the given feature. The key detail displayed is the PSI test statistic, which is a measure of how statistically significant the difference between the frequencies of categorical values in the reference and evaluation sets is.</p></td>
<td><p>Distribution drift in categorical features between training and inference can be caused by a variety of factors, including a change in the data generation process or a change in the preprocessing pipeline. A big shift in categorical features towards categorical subsets that your model performs poorly in could indicate a degradation in model performance and signal the need for relabeling and retraining.</p></td>
<td><p>By default, this test runs over all categorical columns with sufficiently many samples.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Numeric Feature Drift</p></td>
<td><p>Drift</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test measures the severity of passing to the model data points that have numeric features that have drifted from the distribution observed in the reference set. The severity is a function of the impact on the model, as well as the presence of drift in the data. The model impact measures how much model performance changes due to drift in the given feature. The key detail is the Population Stability Index statistic. The Population Stability Index (PSI) is a measure of how different two distributions are. Given two distributions P and Q, it is computed as the sum of the KL Divergence between P and Q and the (reverse) KL Divergence between Q and P. Thus, PSI is symmetric.</p></td>
<td><p>Distribution shift between training and inference can cause degradation in model performance. If the shift is sufficiently large, retraining the model on newer data may be necessary.</p></td>
<td><p>By default, this test runs over all numeric columns with sufficiently many samples and stored quantiles in each of the reference and evaluation sets.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Prediction Drift</p></td>
<td><p>Drift</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test checks that the difference in the prediction distribution between the reference and evaluation sets is small, using Population Stability Index. The key detail displayed is the PSI which is a measure of how different the prediction distributions in the reference and evaluation sets are.</p></td>
<td><p>Prediction distribution shift between reference and test can indicate that the underlying data distribution has changed significantly enough to modify model decisions. This may mean that the model needs to be retrained to adjust to the new data environment. In addition, significant prediction distribution drift may indicate that upstream decision-making modules (e.g. thresholds) may need to be updated.</p></td>
<td><p>This test is run by default whenever both the reference and evaluation sets have associated predictions. Different thresholds are associated with different severities.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Embedding Drift</p></td>
<td><p>Drift</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test measures the severity of passing to the model data points associated with embeddings that have drifted from the distribution observed in the reference set. The severity is a function of the impact on the model, as well as the presence of drift in the data. The model impact measures how much model performance changes due to drift in the given feature. The key detail is the Euclidean Distance statistic. The Euclidean Distance is defined as the square root of the sum of the squared differences between two vectors X and Y. The normalized version of this metric first divides each vector by its L2 norm. This test takes the normalized Euclidean distance between the centroids of the ref and eval data sets.</p></td>
<td><p>Distribution shift between training and inference can cause degradation in model performance. If the shift is sufficiently large, retraining the model on newer data may be necessary.</p></td>
<td><p>By default, this test runs over all specified embeddings with sufficiently many samples in each of the reference and evaluation sets.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Nulls Per Feature Drift</p></td>
<td><p>Drift</p></td>
<td><p>tabular</p></td>
<td><p>This test measures the severity of passing to the model data points that have features with a null proportion that has drifted from the distribution observed in the reference set. The severity is a function of the impact on the model, as well as the presence of drift in the data. The model impact measures how much model performance changes due to drift in the given feature. The key detail is the p-value from a two-sample proportion test that checks if there is a statistically significant difference in the frequencies of null values between the reference and evaluation sets.</p></td>
<td><p>Distribution drift in null values between training and inference can be caused by a variety of factors, including a change in the data generation process or a change in the preprocessing pipeline. A big shift in null value proportion could indicate a degradation in model performance and signal the need for relabeling and retraining.</p></td>
<td><p>By default, this test runs over all columns with sufficiently many samples.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Nulls Per Row Drift</p></td>
<td><p>Drift</p></td>
<td><p>tabular</p></td>
<td><p>This test measures the severity of passing to the model data points that have proportions of null values that have drifted from the distribution observed in the reference set. The severity is a function of the impact on the model, as well as the presence of drift in the data. The model impact measures how much predictions change when the observed drift is applied to a given row. The key detail displayed is the PSI statistic that is a measure of how statistically significant the difference in the proportion of null values in a row between the reference and evaluation sets is.</p></td>
<td><p>Distribution drift in null values between training and inference can be caused by a variety of factors, including a change in the data generation process or a change in the preprocessing pipeline. A big shift in null value proportion could indicate a degradation in model performance and signal the need for relabeling and retraining.</p></td>
<td><p>By default, this test runs over all rows.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Single-Feature Changes</p></td>
<td><p>Attacks</p></td>
<td><p>tabular</p></td>
<td><p>This test measures the severity of passing to the model data points that have been manipulated across a single feature in an unbounded manner. The severity is a function of the impact of these manipulations on the model.</p></td>
<td><p>In production, your model will likely come across inputs that are out-of-distribution with respect to the training data, and it is often difficult to know ahead of time how your model will behave on such inputs. ‘Attacking’ a model in the manner of this test is a technique for finding the out-of-distribution regions of the input space where your model most severely misbehaves, &lt;i&gt;before&lt;/i&gt; putting it into production. Rstricting ourselves to changing a single feature at a time is one proxy for what ‘realistic’ out-of-distribution data can look like.</p></td>
<td><p>By default, for a given input we aim to change your model’s prediction in the opposite direction of the true label. This test raises a warning if the average prediction change that can be achieved exceeds an acceptable threshold.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Bounded Single-Feature Changes</p></td>
<td><p>Attacks</p></td>
<td><p>tabular</p></td>
<td><p>This test measures the severity of passing to the model data points that have been manipulated across a single feature in a bounded manner. The severity is a function of the impact of these manipulations on the model.We bound the manipulations to be less than some fraction of the range of the given feature.</p></td>
<td><p>In production, your model will likely come across inputs that are out-of-distribution with respect to the training data, and it is often difficult to know ahead of time how your model will behave on such inputs. ‘Attacking’ a model in the manner of this test is a technique for finding the out-of-distribution regions of the input space where your model most severely misbehaves, &lt;i&gt;before&lt;/i&gt; putting it into production. Restricting ourselves to changing a single feature by a small amount is one proxy for what ‘realistic’ out-of-distribution data can look like.</p></td>
<td><p>By default, for a given input we aim to change your model’s prediction in the opposite direction of the true label. This test raises a warning if the average prediction change that can be achieved exceeds an acceptable threshold. This test runs only over numeric features.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Multi-Feature Changes</p></td>
<td><p>Attacks</p></td>
<td><p>tabular</p></td>
<td><p>This test measures the severity of passing to the model data points that have been manipulated across multiple features in an unbounded manner. The severity is a function of the impact of these manipulations on the model.</p></td>
<td><p>In production, your model will likely come across inputs that are out-of-distribution with respect to the training data, and it is often difficult to know ahead of time how your model will behave on such inputs. ‘Attacking’ a model in the manner of this test is a technique for finding the out-of-distribution regions of the input space where your model most severely misbehaves, &lt;i&gt;before&lt;/i&gt; putting it into production. Restricting the number of features that can be changed is one proxy for what ‘realistic’ out-of-distribution data can look like.</p></td>
<td><p>By default, for a given input we aim to change your model’s prediction in the opposite direction of the true label. This test raises a warning if the average prediction change that can be achieved exceeds an acceptable threshold.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Bounded Multi-Feature Changes</p></td>
<td><p>Attacks</p></td>
<td><p>tabular</p></td>
<td><p>This test measures the severity of passing to the model data points that have been manipulated across multiple features in an bounded manner. The severity is a function of the impact of these manipulations on the model.We bound the manipulations to be less than some fraction of the range of the given feature.</p></td>
<td><p>In production, your model will likely come across inputs that are out-of-distribution with respect to the training data, and it is often difficult to know ahead of time how your model will behave on such inputs. ‘Attacking’ a model in the manner of this test is a technique for finding the out-of-distribution regions of the input space where your model most severely misbehaves, &lt;i&gt;before&lt;/i&gt; putting it into production. Restricting the number of features that can be changed and the magnitude of the change that can be made to each feature is one proxy for what ‘realistic’ out-of-distribution data can look like.</p></td>
<td><p>By default, for a given input we aim to change your model’s prediction in the opposite direction of the true label. This test raises a warning if the average prediction change that can be achieved exceeds an acceptable threshold. This test runs only over numeric features.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Label Imbalance</p></td>
<td><p>Data Cleanliness</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test checks that no labels have exceedingly high frequency.</p></td>
<td><p>Label imbalance in the training data can introduce bias into the model and possibly result in poor predictive performance on examples from the minority classes.</p></td>
<td><p>This test runs only on classification tasks.</p></td>
<td><p>[tabular] Binary Classification, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Required Features</p></td>
<td><p>Data Cleanliness</p></td>
<td><p>tabular</p></td>
<td><p>This test checks that the features of a dataset are as expected.</p></td>
<td><p>Errors in data collection and processing can lead to invalid missing (or extra) features. In the case of missing features, this can cause failures in models. In the case of extra features, this can lead to unnecessary storage and computation.</p></td>
<td><p>This test runs only when required features are specified.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Duplicate Row</p></td>
<td><p>Data Cleanliness</p></td>
<td><p>tabular</p></td>
<td><p>This test checks if there are any duplicate rows in your dataset. The key detail displays the number of duplicate rows in your dataset.</p></td>
<td><p>Duplicate rows are potentially a sign of a broken data pipeline or an otherwise corrupted input.</p></td>
<td><p>By default this test is run over all features, meaning two rows are considered duplicates only if they match across all features.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Mutual Information Decrease (Feature to Label)</p></td>
<td><p>Data Cleanliness</p></td>
<td><p>tabular</p></td>
<td><p>This test flags a likely data leakage issue in the model.Data leakage occurs when a model is trained on features containing information about the label that is not normally present during production.This test flags an issue if both of the following occur:&lt;ul&gt;&lt;li&gt;the normalized mutual information between the feature and the label is too high in the reference set&lt;/li&gt;&lt;li&gt;the normalized mutual information for the reference set is much higher than for the evaluation set&lt;/li&gt;&lt;/ul&gt; The first criterion is an indicator that the feature has unreasonably high predictive power for the label during training, and the second criterion checks that the feature is no longer a good predictor in the evaluation set. One requirement for this test to flag data leakage is that the evaluation set labels and features are collected properly. This test should be utilized if one trusts their eval data is collected correctly, else the High MI test should be used.</p></td>
<td><p>Errors in data collection and processing can lead to some features containing information about the label in the reference set that do not appear in the evaluation set. This causes the model to under-perform during production.</p></td>
<td><p>By default, this test always runs on all categorical features.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>High Mutual Information (Feature to Label)</p></td>
<td><p>Data Cleanliness</p></td>
<td><p>tabular</p></td>
<td><p>This test flags a likely data leakage issue if the normalized mutual information between the feature and the label is too high in the reference set. Data leakage occurs when a model is trained on features containing information about the label that is not normally present during production. This criterion is an indicator that this feature has unreasonably high predictive power for the label during training. One requirement for this test to flag data leakage is that the reference set labels and features are collected properly. This test should be utilized when one doesn’t trust their eval data is collected correctly, else the MI Decrease test should be used.</p></td>
<td><p>Errors in data collection and processing can lead to some features containing information about the label in the reference set. This causes the model to under-perform during production.</p></td>
<td><p>By default, this test always runs on all categorical features.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>High Feature Correlation</p></td>
<td><p>Data Cleanliness</p></td>
<td><p>tabular</p></td>
<td><p>This test checks that the correlation between two features in the reference set is not too high. Correlation is a measure of the linear relationship between two numeric features.</p></td>
<td><p>Correlation in training features can be caused by a variety of factors, including interdependencies between the collected features, data collection processes, or change in data labeling. Training on too similar features can lead to underperforming or non-robust models.</p></td>
<td><p>By default, this test runs over all pairs of numeric features in the dataset.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Subset Precision</p></td>
<td><p>Subset Performance</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>The precision test is also popularly referred to as positive predictive parity in fairness literature. This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the Precision of model predictions within a specific subset is significantly lower than the model prediction Precision over the entire population.</p></td>
<td><p>Having different precision (e.g. false discovery rates) between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. Unlike demographic parity, this test permits assuming different base label rates but flags differing mistake rates between different subgroups. Note that positive predictive parity does not necessarily indicate equal opportunity or predictive equality: as a hypothetical example, imagine that a loan qualification classifier flags 100 entries for group A and 100 entries for group B, each with a precision of 100%, but there are 100 actual qualified entries in group A and 9000 in group B. This would indicate disparities in opportunities given to each subgroup.</p></td>
<td><p>By default, Precision is computed over all predictions/labels. Note that we round predictions to 0/1 to compute precision.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Subset Prediction Variance (Positive Labels)</p></td>
<td><p>Subset Performance</p></td>
<td><p>tabular</p></td>
<td><p>The subset variance test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the variance of model predictions within a specific subset is significantly higher than model prediction variance of the entire population. In this test, the population refers to all data with positive ground-truth labels.</p></td>
<td><p>High variance within a feature subset compared to the overall population could mean a few different things, and should be analyzed with other subset performance tests (accuracy, AUC) for a more clear view. In the variance metric over positive/negative labels, this could mean the model is much more uncertain about the given subset. When paired with a decrease in AUC, this implies the model underperforms on this subset.</p></td>
<td><p>By default, the variance is computed over all predictions with a positive ground-truth label.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Subset Mean-Absolute Error (MAE)</p></td>
<td><p>Subset Performance</p></td>
<td><p>tabular</p></td>
<td><p>This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the MAE of model predictions within a specific subset is significantly higher than the model prediction MAE over the entire population.</p></td>
<td><p>Having different mean-absolute error between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation.</p></td>
<td><p>By default, mean-absolute error is computed over all predictions/labels.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Subset Multiclass Accuracy</p></td>
<td><p>Subset Performance</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the accuracy of model predictions within a specific subset is significantly lower than the model prediction accuracy over the entire population.</p></td>
<td><p>Having different accuracy between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. Accuracy can be thought of as a ‘weaker’ metric of model bias compared to measuring false positive rate (predictive equality) or false negative rate (equal opportunity). This is because we can have similar accuracy between group A and group B; yet group A actually has higher false positive rate, while group B has higher false negative rate (e.g. we reject qualified applicants in group A but accept non-qualified applicants in group B). Nevertheless, accuracy is a standard metric used during evaluation and should be considered as part of performance bias testing.</p></td>
<td><p>By default, accuracy is computed over all predictions/labels. Note we round predictions to 0/1 to compute accuracy.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Subset Macro Precision</p></td>
<td><p>Subset Performance</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>The precision test is also popularly referred to as positive predictive parity in fairness literature. When transitioning to the multiclass setting, we can compute macro precision which computes the precisions of each class individually and then averages them.This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the Macro Precision of model predictions within a specific subset is significantly lower than the model prediction Macro Precision over the entire population.</p></td>
<td><p>Having different macro precision (e.g. false discovery rates) between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. Unlike demographic parity, this test permits assuming different base label rates but flags differing mistake rates between different subgroups. Note that positive predictive parity does not necessarily indicate equal opportunity or predictive equality: as a hypothetical example, imagine that a loan qualification classifier flags 100 entries for group A and 100 entries for group B, each with a precision of 100%, but there are 100 actual qualified entries in group A and 9000 in group B. This would indicate disparities in opportunities given to each subgroup.</p></td>
<td><p>By default, Macro Precision is computed over all predictions/labels. Note that the predicted label is the label with the greatest predicted probability.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Subset Rank Correlation</p></td>
<td><p>Subset Performance</p></td>
<td><p>tabular</p></td>
<td><p>This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the rank correlation of model predictions within a specific subset is significantly lower than the model prediction rank correlation over the entire population.</p></td>
<td><p>Having different rank correlation between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation.</p></td>
<td><p>By default, rank correlation is computed over all predictions/labels.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Subset F1</p></td>
<td><p>Subset Performance</p></td>
<td><p>tabular</p></td>
<td><p>This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the F1 of model predictions within a specific subset is significantly lower than the model prediction F1 over the entire population.</p></td>
<td><p>Having different F1 between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation.</p></td>
<td><p>By default, F1 is computed over all predictions/labels. Note that we round predictions to 0/1 to compute F1 score.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Subset Prediction Variance (Negative Labels)</p></td>
<td><p>Subset Performance</p></td>
<td><p>tabular</p></td>
<td><p>The subset variance test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the variance of model predictions within a specific subset is significantly higher than model prediction variance of the entire population. In this test, the population refers to all data with negative ground-truth labels.</p></td>
<td><p>High variance within a feature subset compared to the overall population could mean a few different things, and should be analyzed with other subset performance tests (accuracy, AUC) for a more clear view. In the variance metric over positive/negative labels, this could mean the model is much more uncertain about the given subset. When paired with a decrease in AUC, this implies the model underperforms on this subset.</p></td>
<td><p>By default, the variance is computed over all predictions with a negative ground-truth label.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Subset False Positive Rate</p></td>
<td><p>Subset Performance</p></td>
<td><p>tabular</p></td>
<td><p>The false positive error rate test is also popularly referred to as as predictive equality, or equal mis-opportunity in fairness literature. This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the false positive rate of model predictions within a specific subset is significantly higher than the model prediction false positive rate over the entire population.</p></td>
<td><p>Having different false positive rates (e.g. predictive equality) between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. Unlike demographic parity, this test permits assuming different base label rates but flags differing mistake rates between different subgroups. As an intuitive example, consider the case when the label indicates an undesirable attribute: if predicting whether a person will default on their loan, make sure that for people who didn’t default, the rate at which the model incorrectly predicts positive is similar for group A and B.</p></td>
<td><p>By default, false positive rate is computed over all predictions/labels. Note that we round predictions to 0/1 to compute false positive rate.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Subset Root-Mean-Squared Error (RMSE)</p></td>
<td><p>Subset Performance</p></td>
<td><p>tabular</p></td>
<td><p>This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the RMSE of model predictions within a specific subset is significantly higher than the model prediction RMSE over the entire population.</p></td>
<td><p>Having different RMSE between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation.</p></td>
<td><p>By default, RMSE is computed over all predictions/labels.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Subset Mean-Absolute Percentage Error (MAPE)</p></td>
<td><p>Subset Performance</p></td>
<td><p>tabular</p></td>
<td><p>This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the MAPE of model predictions within a specific subset is significantly higher than the model prediction MAPE over the entire population.</p></td>
<td><p>Having different mean-absolute percentage error between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation.</p></td>
<td><p>By default, mean-absolute percentage error is computed over all predictions/labels.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Subset Macro F1</p></td>
<td><p>Subset Performance</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>F1 is a holistic measure of both precision and recall. When transitioning to the multiclass setting we can use macro F1 which computes the F1 of each class and averages them. This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the macro F1 of model predictions within a specific subset is significantly lower than the model prediction macro F1 over the entire population.</p></td>
<td><p>Having different macro F1 between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation.</p></td>
<td><p>By default, macro F1 is computed over all predictions/labels. Note that the predicted label is the label with the largest predicted probability.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Subset Normalized Discounted Cumulative Gain (NDCG)</p></td>
<td><p>Subset Performance</p></td>
<td><p>tabular</p></td>
<td><p>This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the NDCG of model predictions within a specific subset is significantly lower than the model prediction NDCG over the entire population.</p></td>
<td><p>Having different NDCG between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation.</p></td>
<td><p>By default, NDCG is computed over all predictions/labels.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Subset AUC</p></td>
<td><p>Subset Performance</p></td>
<td><p>tabular</p></td>
<td><p>This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the Area Under Curve (AUC) of model predictions within a specific subset is significantly lower than the model prediction Area Under Curve (AUC) over the entire population.</p></td>
<td><p>Having different AUC between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation.</p></td>
<td><p>By default, AUC is computed over all predictions/labels. Note that we compute AUC of the Receiver Operating Characteristic (ROC) curve.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Subset Recall</p></td>
<td><p>Subset Performance</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>The recall test is more popularly referred to as equal opportunity or false negative error rate balance in fairness literature. This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the Recall of model predictions within a specific subset is significantly lower than the model prediction Recall over the entire population.</p></td>
<td><p>Having different true positive rates (e.g. equal opportunity) between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. Unlike demographic parity, this test permits assuming different base label rates but flags differing mistake rates between different subgroups. An intuitive example is when the label indicates a positive attribute: if predicting whether to interview a given candidate, make sure that out of qualified candidates, the rate at which the model predicts a rejection is similar to group A and B.</p></td>
<td><p>By default, Recall is computed over all predictions/labels. Note that we round predictions to 0/1 to compute recall.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Subset Accuracy</p></td>
<td><p>Subset Performance</p></td>
<td><p>tabular</p></td>
<td><p>This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the accuracy of model predictions within a specific subset is significantly lower than the model prediction accuracy over the entire population.</p></td>
<td><p>Having different accuracy between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. Accuracy can be thought of as a ‘weaker’ metric of model bias compared to measuring false positive rate (predictive equality) or false negative rate (equal opportunity). This is because we can have similar accuracy between group A and group B; yet group A actually has higher false positive rate, while group B has higher false negative rate (e.g. we reject qualified applicants in group A but accept non-qualified applicants in group B). Nevertheless, accuracy is a standard metric used during evaluation and should be considered as part of performance bias testing.</p></td>
<td><p>By default, accuracy is computed over all predictions/labels. Note we round predictions to 0/1 to compute accuracy.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Subset Macro Recall</p></td>
<td><p>Subset Performance</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>The recall test is more popularly referred to as equal opportunity or false negative error rate balance in fairness literature. When transitioning to the multiclass setting we can use macro recall which computes the recall of each individual class and then averages these numbers.This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the Macro Recall of model predictions within a specific subset is significantly lower than the model prediction Macro Recall over the entire population.</p></td>
<td><p>Having different true positive rates (e.g. equal opportunity) between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. Unlike demographic parity, this test permits assuming different base label rates but flags differing mistake rates between different subgroups. An intuitive example is when the label indicates a positive attribute: if predicting whether to interview a given candidate, make sure that out of qualified candidates, the rate at which the model predicts an interview is similar to group A and B.</p></td>
<td><p>By default, Macro Recall is computed over all predictions/labels. Note that the predicted label is the label with the largest predicted class probability.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Subset Mean Reciprocal Rank (MRR)</p></td>
<td><p>Subset Performance</p></td>
<td><p>tabular</p></td>
<td><p>This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the MRR of model predictions within a specific subset is significantly lower than the model prediction MRR over the entire population.</p></td>
<td><p>Having different MRR between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation.</p></td>
<td><p>By default, MRR is computed over all predictions/labels.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Subset Multiclass AUC</p></td>
<td><p>Subset Performance</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>In the multiclass setting, we compute one vs. one area under the curve (AUC), which computes the AUC between every pairwise combination of classes. This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the Area Under Curve (AUC) of model predictions within a specific subset is significantly lower than the model prediction Area Under Curve (AUC) over the entire population.</p></td>
<td><p>Having different AUC between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation.</p></td>
<td><p>By default, AUC is computed over all predictions/labels. Note that we compute AUC of the Receiver Operating Characteristic (ROC) curve.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Numeric Outliers</p></td>
<td><p>Abnormal Inputs</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test measures the number of failing rows in your data with outliers and their impact on the model. Outliers are values which may not necessarily be outside of an allowed range for a feature, but are extreme values that are unusual and may be indicative of abnormality. The model impact is the difference in model performance between passing and failing rows with outliers. If labels are not provided, prediction change is used instead of model performance change.</p></td>
<td><p>Outliers can be a sign of corrupted or otherwise erroneous data, and can degrade model performance if used in the training data, or lead to unexpected behaviour if input at inference time.</p></td>
<td><p>By default this test is run over each numeric feature that is neither unique nor ascending.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Unseen Categorical</p></td>
<td><p>Abnormal Inputs</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test measures the number of failing rows in your data with unseen categorical values and their impact on the model. The model impact is the difference in model performance between passing and failing rows with unseen categorical values. If labels are not provided, prediction change is used instead of model performance change.</p></td>
<td><p>Unseen categorical values are a common failure point in machine learning systems; since these models are trained over a reference set, they may yield uninterpretable or undefined behavior when interacting with an unseen categorical value. In addition, such errors may expose gaps or errors in data collection.</p></td>
<td><p>By default, this test runs over all categorical features.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Unseen Domain</p></td>
<td><p>Abnormal Inputs</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test measures the number of failing rows in your data with unseen domain values and their impact on the model. The model impact is the difference in model performance between passing and failing rows with unseen domain values. If labels are not provided, prediction change is used instead of model performance change.</p></td>
<td><p>Unseen categorical values are a common failure point in machine learning systems; since these models are trained over a reference set, they may yield uninterpretable or undefined behavior when interacting with an unseen categorical value. In addition, such errors may expose gaps or errors in data collection.</p></td>
<td><p>By default, this test runs over all features inferred to contain domains.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Unseen Email</p></td>
<td><p>Abnormal Inputs</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test measures the number of failing rows in your data with unseen email values and their impact on the model. The model impact is the difference in model performance between passing and failing rows with unseen email values. If labels are not provided, prediction change is used instead of model performance change.</p></td>
<td><p>Unseen categorical values are a common failure point in machine learning systems; since these models are trained over a reference set, they may yield uninterpretable or undefined behavior when interacting with an unseen categorical value. In addition, such errors may expose gaps or errors in data collection.</p></td>
<td><p>By default, this test runs over all features inferred to contain emails.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Unseen URL</p></td>
<td><p>Abnormal Inputs</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test measures the number of failing rows in your data with unseen URL values and their impact on the model. The model impact is the difference in model performance between passing and failing rows with unseen URL values. If labels are not provided, prediction change is used instead of model performance change.</p></td>
<td><p>Unseen categorical values are a common failure point in machine learning systems; since these models are trained over a reference set, they may yield uninterpretable or undefined behavior when interacting with an unseen categorical value. In addition, such errors may expose gaps or errors in data collection.</p></td>
<td><p>By default, this test runs over all features inferred to contain URLs.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Rare Categories</p></td>
<td><p>Abnormal Inputs</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test measures the severity of passing to the model data points whose features contain rarely observed categories (relative to the reference set). The severity is a function of the impact of these values on the model, as well as the presence of these values in the data. The model impact is the difference in model performance between passing and failing rows with rarely observed categorical values. If labels are not provided, prediction change is used instead of model performance change. The number of failing rows refers to the number of times rarely observed categorical values are observed in the evaluation set.</p></td>
<td><p>Rare categories are a common failure point in machine learning systems because less data often means worse performance. In addition, this may expose gaps or errors in data collection.</p></td>
<td><p>By default, this test runs over all categorical features. A category is considered rare if it occurs fewer than &lt;span&gt;min_num_occurrences&lt;/span&gt; times, or if it occurs less than &lt;span&gt;min_pct_occurrences&lt;/span&gt; of the time. If neither of these values are specified, the rate of appearance below which a category is considered rare is &lt;span&gt;min_ratio_rel_uniform&lt;/span&gt; divided by the number of classes.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Out of Range</p></td>
<td><p>Abnormal Inputs</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test measures the number of failing rows in your data with values outside the inferred range of allowed values and their impact on the model. The model impact is the difference in model performance between passing and failing rows with values outside the inferred range of allowed values. If labels are not provided, prediction change is used instead of model performance change.</p></td>
<td><p>In production, the model may encounter corrupted or manipulated out of range values. It is important that the model is robust to such extremities.</p></td>
<td><p>By default, this test runs over all numeric features.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Required Characters</p></td>
<td><p>Abnormal Inputs</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test measures the number of failing rows in your data with strings without any required characters and their impact on the model. The model impact is the difference in model performance between passing and failing rows with strings without any required characters. If labels are not provided, prediction change is used instead of model performance change.</p></td>
<td><p>A feature may require specific characters. However, errors in the data pipeline may allow invalid data points that lack these required characters to pass. Failing to catch such errors may lead to noisier training data or noisier predictions during inference, which can degrade model metrics.</p></td>
<td><p>By default, this test runs over all string features that are inferred to have required characters.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Inconsistencies</p></td>
<td><p>Abnormal Inputs</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test measures the severity of passing to the model data points whose values are inconsistent (as inferred from the reference set). The severity is a function of the impact of these values on the model, as well as the presence of these values in the data. The model impact is the difference in model performance between passing and failing rows with data containing inconsistent feature values. If labels are not provided, prediction change is used instead of model performance change. The number of failing rows refers to the number of times data containing inconsistent feature values are observed in the evaluation set.</p></td>
<td><p>Inconsistent values might be the result of malicious actors manipulating the data or errors in the data pipeline. Thus, it is important to be aware of inconsistent values to identify sources of manipulations or errors.</p></td>
<td><p>By default, this test runs on pairs of categorical features whose correlations exceed some minimum threshold. The default threshold for the frequency ratio below which values are considered to be inconsistent is &lt;span&gt;0.02&lt;/span&gt;.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Capitalization</p></td>
<td><p>Abnormal Inputs</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test measures the number of failing rows in your data with different types of capitalization and their impact on the model. The model impact is the difference in model performance between passing and failing rows with different types of capitalization. If labels are not provided, prediction change is used instead of model performance change.</p></td>
<td><p>In production, models can come across the same value with different capitalizations, making it important to explicitly check that your model is invariant to such differences.</p></td>
<td><p>By default, this test runs over all categorical features.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Empty String</p></td>
<td><p>Abnormal Inputs</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test measures the number of failing rows in your data with empty string values instead of null values and their impact on the model. The model impact is the difference in model performance between passing and failing rows with empty string values instead of null values. If labels are not provided, prediction change is used instead of model performance change.</p></td>
<td><p>In production, the model may encounter corrupted or manipulated string values. Null values and empty strings are often expected to be treated the same, but the model might not treat them that way. It is important that the model is robust to such extremities.</p></td>
<td><p>By default, this test runs over all string features with null values.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Embedding Anomalies</p></td>
<td><p>Abnormal Inputs</p></td>
<td><p>tabular, nlp, cv</p></td>
<td><p>This test measures the number of failing rows in your data with anomalous embeddings and their impact on the model. The model impact is the difference in model performance between passing and failing rows with anomalous embeddings. If labels are not provided, prediction change is used instead of model performance change.</p></td>
<td><p>In production, the presence of anomalous embeddings can indicate breaks in upstream data pipelines, poor model generalization, or other issues.</p></td>
<td><p>By default, this test runs over all configured embeddings.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Null Check</p></td>
<td><p>Abnormal Inputs</p></td>
<td><p>tabular</p></td>
<td><p>This test measures the number of failing rows in your data with nulls in features that should not have nulls and their impact on the model. The model impact is the difference in model performance between passing and failing rows with nulls in features that should not have nulls. If labels are not provided, prediction change is used instead of model performance change.</p></td>
<td><p>The model may make certain assumptions about a column depending on whether or not it had nulls in the training data. If these assumptions break during production, this may damage the model’s performance. For example, if a column was never null during training then a model may not have learned to be robust against noise in that column.</p></td>
<td><p>By default, this test runs over all columns that had zero nulls in the reference set.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Must be Int</p></td>
<td><p>Abnormal Inputs</p></td>
<td><p>tabular</p></td>
<td><p>This test measures the number of failing rows in your data with values not of type Integer and their impact on the model. The model impact is the difference in model performance between passing and failing rows with values not of type Integer. If labels are not provided, prediction change is used instead of model performance change.</p></td>
<td><p>A feature may require a specific type. However, errors in the data pipeline may produce values that are outside the expected type. Failing to catch such errors may lead to errors or undefined behavior from the model.</p></td>
<td><p>By default, this test runs over all features that are inferred to be type Integer.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Must be Float</p></td>
<td><p>Abnormal Inputs</p></td>
<td><p>tabular</p></td>
<td><p>This test measures the number of failing rows in your data with values not of type Float and their impact on the model. The model impact is the difference in model performance between passing and failing rows with values not of type Float. If labels are not provided, prediction change is used instead of model performance change.</p></td>
<td><p>A feature may require a specific type. However, errors in the data pipeline may produce values that are outside the expected type. Failing to catch such errors may lead to errors or undefined behavior from the model.</p></td>
<td><p>By default, this test runs over all features that are inferred to be type Float.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Must be String</p></td>
<td><p>Abnormal Inputs</p></td>
<td><p>tabular</p></td>
<td><p>This test measures the number of failing rows in your data with values not of type String Categorical and their impact on the model. The model impact is the difference in model performance between passing and failing rows with values not of type String Categorical. If labels are not provided, prediction change is used instead of model performance change.</p></td>
<td><p>A feature may require a specific type. However, errors in the data pipeline may produce values that are outside the expected type. Failing to catch such errors may lead to errors or undefined behavior from the model.</p></td>
<td><p>By default, this test runs over all features that are inferred to be type String Categorical.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Must be Boolean</p></td>
<td><p>Abnormal Inputs</p></td>
<td><p>tabular</p></td>
<td><p>This test measures the number of failing rows in your data with values not of type Boolean Categorical and their impact on the model. The model impact is the difference in model performance between passing and failing rows with values not of type Boolean Categorical. If labels are not provided, prediction change is used instead of model performance change.</p></td>
<td><p>A feature may require a specific type. However, errors in the data pipeline may produce values that are outside the expected type. Failing to catch such errors may lead to errors or undefined behavior from the model.</p></td>
<td><p>By default, this test runs over all features that are inferred to be type Boolean Categorical.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Must be URL</p></td>
<td><p>Abnormal Inputs</p></td>
<td><p>tabular</p></td>
<td><p>This test measures the number of failing rows in your data with values not of type URL Categorical and their impact on the model. The model impact is the difference in model performance between passing and failing rows with values not of type URL Categorical. If labels are not provided, prediction change is used instead of model performance change.</p></td>
<td><p>A feature may require a specific type. However, errors in the data pipeline may produce values that are outside the expected type. Failing to catch such errors may lead to errors or undefined behavior from the model.</p></td>
<td><p>By default, this test runs over all features that are inferred to be type URL Categorical.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Must be Domain</p></td>
<td><p>Abnormal Inputs</p></td>
<td><p>tabular</p></td>
<td><p>This test measures the number of failing rows in your data with values not of type Domain Categorical and their impact on the model. The model impact is the difference in model performance between passing and failing rows with values not of type Domain Categorical. If labels are not provided, prediction change is used instead of model performance change.</p></td>
<td><p>A feature may require a specific type. However, errors in the data pipeline may produce values that are outside the expected type. Failing to catch such errors may lead to errors or undefined behavior from the model.</p></td>
<td><p>By default, this test runs over all features that are inferred to be type Domain Categorical.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Must be Email</p></td>
<td><p>Abnormal Inputs</p></td>
<td><p>tabular</p></td>
<td><p>This test measures the number of failing rows in your data with values not of type Email Categorical and their impact on the model. The model impact is the difference in model performance between passing and failing rows with values not of type Email Categorical. If labels are not provided, prediction change is used instead of model performance change.</p></td>
<td><p>A feature may require a specific type. However, errors in the data pipeline may produce values that are outside the expected type. Failing to catch such errors may lead to errors or undefined behavior from the model.</p></td>
<td><p>By default, this test runs over all features that are inferred to be type Email Categorical.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Subset Drift Precision</p></td>
<td><p>Subset Performance Degradation</p></td>
<td><p>tabular, ‘nlp</p></td>
<td><p>The precision test is also popularly referred to as positive predictive parity in fairness literature. This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the Precision of model predictions within a specific subset is significantly lower than the model prediction Precision over the entire population.</p></td>
<td><p>Having different precision (e.g. false discovery rates) between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. Unlike demographic parity, this test permits assuming different base label rates but flags differing mistake rates between different subgroups. Note that positive predictive parity does not necessarily indicate equal opportunity or predictive equality: as a hypothetical example, imagine that a loan qualification classifier flags 100 entries for group A and 100 entries for group B, each with a precision of 100%, but there are 100 actual qualified entries in group A and 9000 in group B. This would indicate disparities in opportunities given to each subgroup.</p></td>
<td><p>By default, Precision is computed over all predictions/labels. Note that we round predictions to 0/1 to compute precision.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Subset Drift Prediction Variance (Positive Labels)</p></td>
<td><p>Subset Performance Degradation</p></td>
<td><p>tabular</p></td>
<td><p>The subset variance test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the variance of model predictions within a specific subset is significantly higher than model prediction variance of the entire population. In this test, the population refers to all data with positive ground-truth labels.</p></td>
<td><p>High variance within a feature subset compared to the overall population could mean a few different things, and should be analyzed with other subset performance tests (accuracy, AUC) for a more clear view. In the variance metric over positive/negative labels, this could mean the model is much more uncertain about the given subset. When paired with a decrease in AUC, this implies the model underperforms on this subset.</p></td>
<td><p>By default, the variance is computed over all predictions with a positive ground-truth label.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Subset Drift Mean-Absolute Error (MAE)</p></td>
<td><p>Subset Performance Degradation</p></td>
<td><p>tabular</p></td>
<td><p>This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the MAE of model predictions within a specific subset is significantly higher than the model prediction MAE over the entire population.</p></td>
<td><p>Having different mean-absolute error between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation.</p></td>
<td><p>By default, mean-absolute error is computed over all predictions/labels.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Subset Drift Rank Correlation</p></td>
<td><p>Subset Performance Degradation</p></td>
<td><p>tabular</p></td>
<td><p>This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the rank correlation of model predictions within a specific subset is significantly lower than the model prediction rank correlation over the entire population.</p></td>
<td><p>Having different rank correlation between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation.</p></td>
<td><p>By default, rank correlation is computed over all predictions/labels.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Subset Drift F1</p></td>
<td><p>Subset Performance Degradation</p></td>
<td><p>tabular</p></td>
<td><p>This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the F1 of model predictions within a specific subset is significantly lower than the model prediction F1 over the entire population.</p></td>
<td><p>Having different F1 between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation.</p></td>
<td><p>By default, F1 is computed over all predictions/labels. Note that we round predictions to 0/1 to compute F1 score.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Subset Drift Prediction Variance (Negative Labels)</p></td>
<td><p>Subset Performance Degradation</p></td>
<td><p>tabular</p></td>
<td><p>The subset variance test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the variance of model predictions within a specific subset is significantly higher than model prediction variance of the entire population. In this test, the population refers to all data with negative ground-truth labels.</p></td>
<td><p>High variance within a feature subset compared to the overall population could mean a few different things, and should be analyzed with other subset performance tests (accuracy, AUC) for a more clear view. In the variance metric over positive/negative labels, this could mean the model is much more uncertain about the given subset. When paired with a decrease in AUC, this implies the model underperforms on this subset.</p></td>
<td><p>By default, the variance is computed over all predictions with a negative ground-truth label.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Subset Drift False Positive Rate</p></td>
<td><p>Subset Performance Degradation</p></td>
<td><p>tabular</p></td>
<td><p>The false positive error rate test is also popularly referred to as as predictive equality, or equal mis-opportunity in fairness literature. This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the false positive rate of model predictions within a specific subset is significantly higher than the model prediction false positive rate over the entire population.</p></td>
<td><p>Having different false positive rates (e.g. predictive equality) between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. Unlike demographic parity, this test permits assuming different base label rates but flags differing mistake rates between different subgroups. As an intuitive example, consider the case when the label indicates an undesirable attribute: if predicting whether a person will default on their loan, make sure that for people who didn’t default, the rate at which the model incorrectly predicts positive is similar for group A and B.</p></td>
<td><p>By default, false positive rate is computed over all predictions/labels. Note that we round predictions to 0/1 to compute false positive rate.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Subset Drift Root-Mean-Squared Error (RMSE)</p></td>
<td><p>Subset Performance Degradation</p></td>
<td><p>tabular</p></td>
<td><p>This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the RMSE of model predictions within a specific subset is significantly higher than the model prediction RMSE over the entire population.</p></td>
<td><p>Having different RMSE between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation.</p></td>
<td><p>By default, RMSE is computed over all predictions/labels.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Subset Drift Mean-Absolute Percentage Error (MAPE)</p></td>
<td><p>Subset Performance Degradation</p></td>
<td><p>tabular</p></td>
<td><p>This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the MAPE of model predictions within a specific subset is significantly higher than the model prediction MAPE over the entire population.</p></td>
<td><p>Having different mean-absolute percentage error between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation.</p></td>
<td><p>By default, mean-absolute percentage error is computed over all predictions/labels.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Subset Drift NDCG</p></td>
<td><p>Subset Performance Degradation</p></td>
<td><p>tabular</p></td>
<td><p>This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the NDCG of model predictions within a specific subset is significantly lower than the model prediction NDCG over the entire population.</p></td>
<td><p>Having different NDCG between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation.</p></td>
<td><p>By default, NDCG is computed over all predictions/labels.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Subset Drift AUC</p></td>
<td><p>Subset Performance Degradation</p></td>
<td><p>tabular</p></td>
<td><p>This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the Area Under Curve (AUC) of model predictions within a specific subset is significantly lower than the model prediction Area Under Curve (AUC) over the entire population.</p></td>
<td><p>Having different AUC between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation.</p></td>
<td><p>By default, AUC is computed over all predictions/labels. Note that we compute AUC of the Receiver Operating Characteristic (ROC) curve.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Subset Drift Recall</p></td>
<td><p>Subset Performance Degradation</p></td>
<td><p>tabular, ‘nlp</p></td>
<td><p>The recall test is more popularly referred to as equal opportunity or false negative error rate balance in fairness literature. This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the Recall of model predictions within a specific subset is significantly lower than the model prediction Recall over the entire population.</p></td>
<td><p>Having different true positive rates (e.g. equal opportunity) between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. Unlike demographic parity, this test permits assuming different base label rates but flags differing mistake rates between different subgroups. An intuitive example is when the label indicates a positive attribute: if predicting whether to interview a given candidate, make sure that out of qualified candidates, the rate at which the model predicts a rejection is similar to group A and B.</p></td>
<td><p>By default, Recall is computed over all predictions/labels. Note that we round predictions to 0/1 to compute recall.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Subset Drift Accuracy</p></td>
<td><p>Subset Performance Degradation</p></td>
<td><p>tabular</p></td>
<td><p>This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the accuracy of model predictions within a specific subset is significantly lower than the model prediction accuracy over the entire population.</p></td>
<td><p>Having different accuracy between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. Accuracy can be thought of as a ‘weaker’ metric of model bias compared to measuring false positive rate (predictive equality) or false negative rate (equal opportunity). This is because we can have similar accuracy between group A and group B; yet group A actually has higher false positive rate, while group B has higher false negative rate (e.g. we reject qualified applicants in group A but accept non-qualified applicants in group B). Nevertheless, accuracy is a standard metric used during evaluation and should be considered as part of performance bias testing.</p></td>
<td><p>By default, accuracy is computed over all predictions/labels. Note we round predictions to 0/1 to compute accuracy.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-even"><td><p>Subset Drift Mean Reciprocal Rank (MRR)</p></td>
<td><p>Subset Performance Degradation</p></td>
<td><p>tabular</p></td>
<td><p>This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the MRR of model predictions within a specific subset is significantly lower than the model prediction MRR over the entire population.</p></td>
<td><p>Having different MRR between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation.</p></td>
<td><p>By default, MRR is computed over all predictions/labels.</p></td>
<td><p>[tabular] Regression, [tabular] Binary Classification, [tabular] Ranking, [tabular] Multi-class Classification</p></td>
</tr>
<tr class="row-odd"><td><p>Invisible Character Attack</p></td>
<td><p>Adversarial</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the robustness of your model to invisible character attacks. It does this by taking a sample input, inserting zero-width unicode characters, and measuring the performance of the model on the perturbed input. See the paper  “Fall of Giants: How Popular Text-Based MLaaS Fall against a Simple Evasion Attack” by Pajola and Conti (<a class="reference external" href="https://arxiv.org/abs/2104.05996">https://arxiv.org/abs/2104.05996</a>) for more details.</p></td>
<td><p>Malicious actors can perturb natural language input sequences to  alter model behavior in unexpected ways. It is important that your NLP models are robust to such attacks.</p></td>
<td><p>By default, this test runs in adversarial mode.</p></td>
<td><p>[nlp] text_classification</p></td>
</tr>
<tr class="row-even"><td><p>Deletion Control Character Attack</p></td>
<td><p>Adversarial</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the robustness of your model to deletion control character attacks. It does this by taking a sample input, inserting deletion control characters, and measuring the performance of the model on the perturbed input. See the paper  “Bad Characters: Imperceptible NLP Attacks” by Boucher, Shumailov, et al. (<a class="reference external" href="https://arxiv.org/abs/2106.09898">https://arxiv.org/abs/2106.09898</a>) for more details.</p></td>
<td><p>Malicious actors can perturb natural language input sequences to  alter model behavior in unexpected ways. It is important that your NLP models are robust to such attacks.</p></td>
<td><p>By default, this test runs in adversarial mode.</p></td>
<td><p>[nlp] text_classification</p></td>
</tr>
<tr class="row-odd"><td><p>Intentional Homoglyph Attack</p></td>
<td><p>Adversarial</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the robustness of your model to intentional homoglyph attacks. It does this by taking a sample input, substituting homoglyphs designed to look like other characters, and measuring the performance of the model on the perturbed input. See the paper  “Bad Characters: Imperceptible NLP Attacks” by Boucher, Shumailov, et al. (<a class="reference external" href="https://arxiv.org/abs/2106.09898">https://arxiv.org/abs/2106.09898</a>) for more details.</p></td>
<td><p>Malicious actors can perturb natural language input sequences to  alter model behavior in unexpected ways. It is important that your NLP models are robust to such attacks.</p></td>
<td><p>By default, this test runs in adversarial mode.</p></td>
<td><p>[nlp] text_classification</p></td>
</tr>
<tr class="row-even"><td><p>Confusable Homoglyph Attack</p></td>
<td><p>Adversarial</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the robustness of your model to confusable homoglyph attacks. It does this by taking a sample input, substituting homoglyphs that are easily confused with other characters, and measuring the performance of the model on the perturbed input. See the paper  “Bad Characters: Imperceptible NLP Attacks” by Boucher, Shumailov, et al. (<a class="reference external" href="https://arxiv.org/abs/2106.09898">https://arxiv.org/abs/2106.09898</a>) for more details.</p></td>
<td><p>Malicious actors can perturb natural language input sequences to  alter model behavior in unexpected ways. It is important that your NLP models are robust to such attacks.</p></td>
<td><p>By default, this test runs in adversarial mode.</p></td>
<td><p>[nlp] text_classification</p></td>
</tr>
<tr class="row-odd"><td><p>Character Substitution</p></td>
<td><p>Adversarial</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the robustness of your model to character substitution attacks. It does this by randomly substituting characters in the input string and measuring your model’s performance on the attacked string.</p></td>
<td><p>Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p></td>
<td><p>By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p></td>
<td><p>[nlp] text_classification</p></td>
</tr>
<tr class="row-even"><td><p>Character Deletion</p></td>
<td><p>Adversarial</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the robustness of your model to character deletion attacks. It does this by randomly deleting characters in the input string and measuring your model’s performance on the attacked string.</p></td>
<td><p>Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p></td>
<td><p>By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p></td>
<td><p>[nlp] text_classification</p></td>
</tr>
<tr class="row-odd"><td><p>Character Insertion</p></td>
<td><p>Adversarial</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the robustness of your model to character insertion attacks. It does this by randomly adding characters to the input string and measuring your model’s performance on the attacked string.</p></td>
<td><p>Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p></td>
<td><p>By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p></td>
<td><p>[nlp] text_classification</p></td>
</tr>
<tr class="row-even"><td><p>Character Swap</p></td>
<td><p>Adversarial</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the robustness of your model to character swap attacks. It does this by randomly swapping characters in the input string and measuring your model’s performance on the attacked string.</p></td>
<td><p>Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p></td>
<td><p>By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p></td>
<td><p>[nlp] text_classification</p></td>
</tr>
<tr class="row-odd"><td><p>Keyboard Augmentation</p></td>
<td><p>Adversarial</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the robustness of your model to keyboard augmentation attacks. It does this by adding common typos based on keyboard distance to the input string and measuring your model’s performance on the attacked string.</p></td>
<td><p>Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p></td>
<td><p>By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p></td>
<td><p>[nlp] text_classification</p></td>
</tr>
<tr class="row-even"><td><p>Common Misspellings</p></td>
<td><p>Adversarial</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the robustness of your model to common misspellings attacks. It does this by adding common misspellings to the input string and measuring your model’s performance on the attacked string.</p></td>
<td><p>Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p></td>
<td><p>By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p></td>
<td><p>[nlp] text_classification</p></td>
</tr>
<tr class="row-odd"><td><p>OCR Error Simulation</p></td>
<td><p>Adversarial</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the robustness of your model to ocr error simulation attacks. It does this by adding common OCR errors to the input string and measuring your model’s performance on the attacked string.</p></td>
<td><p>Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p></td>
<td><p>By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p></td>
<td><p>[nlp] text_classification</p></td>
</tr>
<tr class="row-even"><td><p>Synonym Swap</p></td>
<td><p>Adversarial</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the robustness of your model to synonym swap attacks. It does this by randomly swapping synonyms in the input string and measuring your model’s performance on the attacked string.</p></td>
<td><p>Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p></td>
<td><p>By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p></td>
<td><p>[nlp] text_classification</p></td>
</tr>
<tr class="row-odd"><td><p>Contextual Word Swap</p></td>
<td><p>Adversarial</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the robustness of your model to contextual word swap attacks. It does this by replacing words with those close in embedding space and measuring your model’s performance on the attacked string.</p></td>
<td><p>Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p></td>
<td><p>By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p></td>
<td><p>[nlp] text_classification</p></td>
</tr>
<tr class="row-even"><td><p>Contextual Word Insertion</p></td>
<td><p>Adversarial</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the robustness of your model to contextual word insertion attacks. It does this by inserting words generated from a language model and measuring your model’s performance on the attacked string.</p></td>
<td><p>Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p></td>
<td><p>By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p></td>
<td><p>[nlp] text_classification</p></td>
</tr>
<tr class="row-odd"><td><p>Universal Prefix Attack</p></td>
<td><p>Adversarial</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the robustness of your model to ‘universal’ adversarial prefix injections. It does this by sampling a batch of inputs, and searching over the model vocabulary to find a prefix that is nonsensical to a reader but that, when prepended to the batch of inputs, will cause the model to output a different prediction. See the paper  “Universal Adversarial Triggers for Attacking and Analyzing NLP” by Wallace, Feng, Kandpal, et al. (<a class="reference external" href="https://arxiv.org/abs/1908.07125">https://arxiv.org/abs/1908.07125</a>) for more details.</p></td>
<td><p>Malicious actors can perturb natural language input sequences to  alter model behavior in unexpected ways. ‘Universal triggers’  pose a particularly large threat since they easily transfer between models and data points to permit an adversary to make large-scale, cost-efficient attacks. It is important that your NLP models are robust to such threat vectors.</p></td>
<td><p>By default, this test runs when the ‘Adversarial’ category is specified.</p></td>
<td><p>[nlp] text_classification</p></td>
</tr>
<tr class="row-even"><td><p>Unseen Unigram</p></td>
<td><p>Abnormal Inputs</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the number of failing rows in your data with unseen unigrams and their impact on the model. The model impact is the difference in model performance between passing and failing rows with unseen unigrams. If labels are not provided, prediction change is used instead of model performance change.</p></td>
<td><p>Unseen unigrams are a common failure point in machine learning systems; since these models are trained over a reference set, they may yield uninterpretable or undefined behavior when interacting with an unseen unigram. In addition, such errors may expose gaps or errors in data collection.</p></td>
<td><p>By default, this test is run over every data point.</p></td>
<td><p>[nlp] text_classification</p></td>
</tr>
<tr class="row-odd"><td><p>Empty Text String</p></td>
<td><p>Abnormal Inputs</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the number of failing rows in your data with empty strings and their impact on the model. The model impact is the difference in model performance between passing and failing rows with empty strings. If labels are not provided, prediction change is used instead of model performance change.</p></td>
<td><p>Empty strings are a common failure point in machine learning systems; as some models may yield uninterpretable or undefined behavior when interacting with an empty string. In addition, such errors may expose gaps or errors in data collection.</p></td>
<td><p>By default, this test is run over every data point.</p></td>
<td><p>[nlp] text_classification</p></td>
</tr>
<tr class="row-even"><td><p>Character Distribution</p></td>
<td><p>Drift</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the &lt;span&gt;character&lt;/span&gt; distribution drift between the reference and evaluation sets. By default, it measures drift by using the Population Stability Index of the two distributions.The severity is determined by comparing the computed drift statistic to the configured severity thresholds.</p></td>
<td><p>The reference set that you use to train your model may not be representative of the evaluation set you encounter in production. If there are statistically significant differences in the &lt;span&gt;character&lt;/span&gt; distribution between these sets, it can lead to subpar real-world model performance.</p></td>
<td><p>To pass a given test case, the divergence metric must be below the configured threshold.</p></td>
<td><p>[nlp] text_classification</p></td>
</tr>
<tr class="row-odd"><td><p>Unigrams Distribution</p></td>
<td><p>Drift</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the &lt;span&gt;unigram&lt;/span&gt; distribution drift between the reference and evaluation sets. By default, it measures drift by using the Population Stability Index of the two distributions.The severity is determined by comparing the computed drift statistic to the configured severity thresholds.</p></td>
<td><p>The reference set that you use to train your model may not be representative of the evaluation set you encounter in production. If there are statistically significant differences in the &lt;span&gt;unigram&lt;/span&gt; distribution between these sets, it can lead to subpar real-world model performance.</p></td>
<td><p>To pass a given test case, the divergence metric must be below the configured threshold.</p></td>
<td><p>[nlp] text_classification</p></td>
</tr>
<tr class="row-even"><td><p>Bigrams Distribution</p></td>
<td><p>Drift</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the &lt;span&gt;bigram&lt;/span&gt; distribution drift between the reference and evaluation sets. By default, it measures drift by using the Population Stability Index of the two distributions.The severity is determined by comparing the computed drift statistic to the configured severity thresholds.</p></td>
<td><p>The reference set that you use to train your model may not be representative of the evaluation set you encounter in production. If there are statistically significant differences in the &lt;span&gt;bigram&lt;/span&gt; distribution between these sets, it can lead to subpar real-world model performance.</p></td>
<td><p>To pass a given test case, the divergence metric must be below the configured threshold.</p></td>
<td><p>[nlp] text_classification</p></td>
</tr>
<tr class="row-odd"><td><p>Upper-Case Text</p></td>
<td><p>Transformations</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the robustness of your model to Upper-Case Text transformations. It does this by taking a sample input, upper-casing all text, and measuring the behavior of the model on the transformed input.</p></td>
<td><p>Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p></td>
<td><p>By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p></td>
<td><p>[nlp] text_classification</p></td>
</tr>
<tr class="row-even"><td><p>Lower-Case Text</p></td>
<td><p>Transformations</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the robustness of your model to Lower-Case Text transformations. It does this by taking a sample input, lower-casing all text, and measuring the behavior of the model on the transformed input.</p></td>
<td><p>Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p></td>
<td><p>By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p></td>
<td><p>[nlp] text_classification</p></td>
</tr>
<tr class="row-odd"><td><p>Remove Special Characters</p></td>
<td><p>Transformations</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the robustness of your model to Remove Special Characters transformations. It does this by taking a sample input, removing all periods and apostrophes from the input string, and measuring the behavior of the model on the transformed input.</p></td>
<td><p>Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p></td>
<td><p>By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p></td>
<td><p>[nlp] text_classification</p></td>
</tr>
<tr class="row-even"><td><p>Replace Masculine with Feminine Pronouns</p></td>
<td><p>Transformations</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the robustness of your model to Replace Masculine with Feminine Pronouns transformations. It does this by taking a sample input, swapping all masculine pronouns from the input string to feminine ones, and measuring the behavior of the model on the transformed input.</p></td>
<td><p>Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p></td>
<td><p>By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p></td>
<td><p>[nlp] text_classification</p></td>
</tr>
<tr class="row-odd"><td><p>Replace Feminine with Masculine Pronouns</p></td>
<td><p>Transformations</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the robustness of your model to Replace Feminine with Masculine Pronouns transformations. It does this by taking a sample input, swapping all feminine pronouns from the input string to masculine ones, and measuring the behavior of the model on the transformed input.</p></td>
<td><p>Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p></td>
<td><p>By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p></td>
<td><p>[nlp] text_classification</p></td>
</tr>
<tr class="row-even"><td><p>Replace Feminine with Masculine Names</p></td>
<td><p>Transformations</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the invariance of your model to gendered name swap transformations. It does this by taking a sample input, swapping all instances of traditionally feminine names (in the provided list) with a traditionally masculine name, and measuring the behavior of the model on the transformed input.</p></td>
<td><p>Production natural language input sequences must properly support people of all demographics. It is important that your NLP models are robust to spurious correlations and bias from the data.</p></td>
<td><p>By default, this test runs over a sample of up to strings from the evaluation set that contain one or more words from the source list.</p></td>
<td><p>[nlp] text_classification</p></td>
</tr>
<tr class="row-odd"><td><p>Replace Masculine with Feminine Names</p></td>
<td><p>Transformations</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the invariance of your model to gendered name swap transformations. It does this by taking a sample input, swapping all instances of traditionally masculine names (in the provided list) with a traditionally feminine name, and measuring the behavior of the model on the transformed input.</p></td>
<td><p>Production natural language input sequences must properly support people of all demographics. It is important that your NLP models are robust to spurious correlations and bias from the data.</p></td>
<td><p>By default, this test runs over a sample of up to strings from the evaluation set that contain one or more words from the source list.</p></td>
<td><p>[nlp] text_classification</p></td>
</tr>
<tr class="row-even"><td><p>Unicode to ASCII</p></td>
<td><p>Transformations</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the robustness of your model to Unicode to ASCII transformations. It does this by taking a sample input, converting all characters in the input string to their nearest ASCII representation, and measuring the behavior of the model on the transformed input.</p></td>
<td><p>Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p></td>
<td><p>By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 5% of the words in each input.</p></td>
<td><p>[nlp] text_classification</p></td>
</tr>
<tr class="row-odd"><td><p>Entity Type Distribution</p></td>
<td><p>Drift</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the &lt;span&gt;label entity type&lt;/span&gt; distribution drift between the reference and evaluation sets. By default, it measures drift by using the Population Stability Index of the two distributions.The severity is a function of the magnitude of data drift, and the impact of that drift on model performance. Performance change is attributed using the performance on subsets (quantiles or categories) of a given feature and the change in subset prevalence across datasets.</p></td>
<td><p>The reference set that you use to train your model may not be representative of the evaluation set you encounter in production. If there are statistically significant differences in the &lt;span&gt;label entity type&lt;/span&gt; distribution between these sets, it can lead to subpar real-world model performance.</p></td>
<td><p>To pass a given test case, the divergence metric must be below the configured threshold.</p></td>
<td><p>[nlp] named_entity_recognition</p></td>
</tr>
<tr class="row-even"><td><p>Predicted Entity Type Distribution</p></td>
<td><p>Drift</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the &lt;span&gt;predicted entity type&lt;/span&gt; distribution drift between the reference and evaluation sets. By default, it measures drift by using the Population Stability Index of the two distributions.The severity is a function of the magnitude of data drift, and the impact of that drift on model performance. Performance change is attributed using the performance on subsets (quantiles or categories) of a given feature and the change in subset prevalence across datasets.</p></td>
<td><p>The reference set that you use to train your model may not be representative of the evaluation set you encounter in production. If there are statistically significant differences in the &lt;span&gt;predicted entity type&lt;/span&gt; distribution between these sets, it can lead to subpar real-world model performance.</p></td>
<td><p>To pass a given test case, the divergence metric must be below the configured threshold.</p></td>
<td><p>[nlp] named_entity_recognition</p></td>
</tr>
<tr class="row-odd"><td><p>Entity Lengths Distribution</p></td>
<td><p>Drift</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the &lt;span&gt;entity length&lt;/span&gt; distribution drift between the reference and evaluation sets. By default, it measures drift by using the Population Stability Index of the two distributions.The severity is a function of the magnitude of data drift, and the impact of that drift on model performance. Performance change is attributed using the performance on subsets (quantiles or categories) of a given feature and the change in subset prevalence across datasets.</p></td>
<td><p>The reference set that you use to train your model may not be representative of the evaluation set you encounter in production. If there are statistically significant differences in the &lt;span&gt;entity length&lt;/span&gt; distribution between these sets, it can lead to subpar real-world model performance.</p></td>
<td><p>To pass a given test case, the divergence metric must be below the configured threshold.</p></td>
<td><p>[nlp] named_entity_recognition</p></td>
</tr>
<tr class="row-even"><td><p>Label Entity Type Subsets</p></td>
<td><p>Subset Performance</p></td>
<td><p>nlp</p></td>
<td><p>This test measures whether the model performs equally well across subsets of the data when grouped by &lt;span&gt;label entity type&lt;/span&gt;. These subsets are defined by grouping input sequences into approximately equal-width bins of the aforementioned metric. The test then measures whether model performance, as defined by the recall, for any given subset is significantly worse than the average performance across all subsets of the data.</p></td>
<td><p>Having similar performance across various subsets of the data is an important measure of performance bias.</p></td>
<td><p>By default, this test measures whether the recall of each subgroup is within 0.05 of the overall performance.</p></td>
<td><p>[nlp] named_entity_recognition</p></td>
</tr>
<tr class="row-odd"><td><p>Predicted Entity Type Subsets</p></td>
<td><p>Subset Performance</p></td>
<td><p>nlp</p></td>
<td><p>This test measures whether the model performs equally well across subsets of the data when grouped by &lt;span&gt;predicted entity type&lt;/span&gt;. These subsets are defined by grouping input sequences into approximately equal-width bins of the aforementioned metric. The test then measures whether model performance, as defined by the precision, for any given subset is significantly worse than the average performance across all subsets of the data.</p></td>
<td><p>Having similar performance across various subsets of the data is an important measure of performance bias.</p></td>
<td><p>By default, this test measures whether the precision of each subgroup is within 0.05 of the overall performance.</p></td>
<td><p>[nlp] named_entity_recognition</p></td>
</tr>
<tr class="row-even"><td><p>Lower-Case Entity</p></td>
<td><p>Transformations</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the robustness of your model to Lower-Case Entity transformations. It does this by taking a sample input, lower-casing all entities, and measuring the behavior of the model on the transformed input.</p></td>
<td><p>Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p></td>
<td><p>By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 30% of the words in each input.</p></td>
<td><p>[nlp] named_entity_recognition</p></td>
</tr>
<tr class="row-odd"><td><p>Upper-Case Entity</p></td>
<td><p>Transformations</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the robustness of your model to Upper-Case Entity transformations. It does this by taking a sample input, upper-casing all entities, and measuring the behavior of the model on the transformed input.</p></td>
<td><p>Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p></td>
<td><p>By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 30% of the words in each input.</p></td>
<td><p>[nlp] named_entity_recognition</p></td>
</tr>
<tr class="row-even"><td><p>Ampersand</p></td>
<td><p>Transformations</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the robustness of your model to Ampersand transformations. It does this by taking a sample input, changing &lt;span&gt;&amp;&lt;/span&gt; to &lt;span&gt;and&lt;/span&gt;, and measuring the behavior of the model on the transformed input.</p></td>
<td><p>Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p></td>
<td><p>By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 30% of the words in each input.</p></td>
<td><p>[nlp] named_entity_recognition</p></td>
</tr>
<tr class="row-odd"><td><p>Abbreviation Expander</p></td>
<td><p>Transformations</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the robustness of your model to Abbreviation Expander transformations. It does this by taking a sample input, expanding abbreviations in entities, and measuring the behavior of the model on the transformed input.</p></td>
<td><p>Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p></td>
<td><p>By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 30% of the words in each input.</p></td>
<td><p>[nlp] named_entity_recognition</p></td>
</tr>
<tr class="row-even"><td><p>Whitespace Around Special Character</p></td>
<td><p>Transformations</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the robustness of your model to Whitespace Around Special Character transformations. It does this by taking a sample input, adding whitespace around special characters, and measuring the behavior of the model on the transformed input.</p></td>
<td><p>Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p></td>
<td><p>By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 30% of the words in each input.</p></td>
<td><p>[nlp] named_entity_recognition</p></td>
</tr>
<tr class="row-odd"><td><p>Swap Seen Entities</p></td>
<td><p>Transformations</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the robustness of your model to Swap Seen Entities transformations. It does this by taking a sample input, swapping all the entities in a text with random entities of the same type seen in the rest of the data, and measuring the behavior of the model on the transformed input.</p></td>
<td><p>Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p></td>
<td><p>By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 30% of the words in each input.</p></td>
<td><p>[nlp] named_entity_recognition</p></td>
</tr>
<tr class="row-even"><td><p>Swap Unseen Entities</p></td>
<td><p>Transformations</p></td>
<td><p>nlp</p></td>
<td><p>This test measures the robustness of your model to Swap Unseen Entities transformations. It does this by taking a sample input, swapping all the entities in a text with random entities of the same category, unseen in the data, and measuring the behavior of the model on the transformed input. This test supports swapping entities from commonly-appearing categories in NER tasks: Person, Geopolitical Entity, Location, Nationality, Product, Corporation, and Organization.</p></td>
<td><p>Production natural language input sequences can have errors from data preprocessing or human input (mistaken or adversarial). It is important that your NLP models are robust to the introduction of such errors.</p></td>
<td><p>By default, this test runs over a sample of strings from the evaluation set, and it performs this attack on 30% of the words in each input.</p></td>
<td><p>[nlp] named_entity_recognition</p></td>
</tr>
<tr class="row-odd"><td><p>Average Number of Predicted Entities</p></td>
<td><p>Model Performance</p></td>
<td><p>nlp</p></td>
<td><p>This test checks the Average Number of Predicted Entities metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of Average Number of Predicted Entities has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p></td>
<td><p>During production, factors like distribution shift or a change in &lt;span&gt;p(y|x)&lt;/span&gt; may cause model performance to decrease significantly.</p></td>
<td><p>By default, this test runs over the Average Number of Predicted Entities metric with the below thresholds set for the absolute and degradation tests.</p></td>
<td><p>[nlp] named_entity_recognition</p></td>
</tr>
<tr class="row-even"><td><p>Gaussian Blur</p></td>
<td><p>Transformations</p></td>
<td><p>cv</p></td>
<td><p>This test measures the robustness of your model to Gaussian Blur transformations. It does this by taking a sample input, blurring the image, and measuring the behavior of the model on the transformed input.</p></td>
<td><p>Production inputs can have unusual variations amongst many different dimensions, ranging from lighting changes to sensor errors to compression artifacts. It is important that your models are robust to the introduction of such variations.</p></td>
<td></td>
<td><p>[cv] image_classification</p></td>
</tr>
<tr class="row-odd"><td><p>Color Jitter</p></td>
<td><p>Transformations</p></td>
<td><p>cv</p></td>
<td><p>This test measures the robustness of your model to Color Jitter transformations. It does this by taking a sample input, jittering the image colors, and measuring the behavior of the model on the transformed input.</p></td>
<td><p>Production inputs can have unusual variations amongst many different dimensions, ranging from lighting changes to sensor errors to compression artifacts. It is important that your models are robust to the introduction of such variations.</p></td>
<td></td>
<td><p>[cv] image_classification</p></td>
</tr>
<tr class="row-even"><td><p>Gaussian Noise</p></td>
<td><p>Transformations</p></td>
<td><p>cv</p></td>
<td><p>This test measures the robustness of your model to Gaussian Noise transformations. It does this by taking a sample input, adding gaussian noise to the image, and measuring the behavior of the model on the transformed input.</p></td>
<td><p>Production inputs can have unusual variations amongst many different dimensions, ranging from lighting changes to sensor errors to compression artifacts. It is important that your models are robust to the introduction of such variations.</p></td>
<td></td>
<td><p>[cv] image_classification</p></td>
</tr>
<tr class="row-odd"><td><p>Vertical Flip</p></td>
<td><p>Transformations</p></td>
<td><p>cv</p></td>
<td><p>This test measures the robustness of your model to Vertical Flip transformations. It does this by taking a sample input, flipping the image vertically, and measuring the behavior of the model on the transformed input.</p></td>
<td><p>Production inputs can have unusual variations amongst many different dimensions, ranging from lighting changes to sensor errors to compression artifacts. It is important that your models are robust to the introduction of such variations.</p></td>
<td></td>
<td><p>[cv] image_classification</p></td>
</tr>
<tr class="row-even"><td><p>Horizontal Flip</p></td>
<td><p>Transformations</p></td>
<td><p>cv</p></td>
<td><p>This test measures the robustness of your model to Horizontal Flip transformations. It does this by taking a sample input, flipping the image horizontally, and measuring the behavior of the model on the transformed input.</p></td>
<td><p>Production inputs can have unusual variations amongst many different dimensions, ranging from lighting changes to sensor errors to compression artifacts. It is important that your models are robust to the introduction of such variations.</p></td>
<td></td>
<td><p>[cv] image_classification</p></td>
</tr>
<tr class="row-odd"><td><p>Randomize Pixels With Mask</p></td>
<td><p>Transformations</p></td>
<td><p>cv</p></td>
<td><p>This test measures the robustness of your model to Randomize Pixels With Mask transformations. It does this by taking a sample input, randomizing pixels with fixed probability, and measuring the behavior of the model on the transformed input.</p></td>
<td><p>Production inputs can have unusual variations amongst many different dimensions, ranging from lighting changes to sensor errors to compression artifacts. It is important that your models are robust to the introduction of such variations.</p></td>
<td></td>
<td><p>[cv] image_classification</p></td>
</tr>
<tr class="row-even"><td><p>Contrast Increase</p></td>
<td><p>Transformations</p></td>
<td><p>cv</p></td>
<td><p>This test measures the robustness of your model to Contrast Increase transformations. It does this by taking a sample input, increase image contrast, and measuring the behavior of the model on the transformed input.</p></td>
<td><p>Production inputs can have unusual variations amongst many different dimensions, ranging from lighting changes to sensor errors to compression artifacts. It is important that your models are robust to the introduction of such variations.</p></td>
<td></td>
<td><p>[cv] image_classification</p></td>
</tr>
<tr class="row-odd"><td><p>Contrast Decrease</p></td>
<td><p>Transformations</p></td>
<td><p>cv</p></td>
<td><p>This test measures the robustness of your model to Contrast Decrease transformations. It does this by taking a sample input, decrease image contrast, and measuring the behavior of the model on the transformed input.</p></td>
<td><p>Production inputs can have unusual variations amongst many different dimensions, ranging from lighting changes to sensor errors to compression artifacts. It is important that your models are robust to the introduction of such variations.</p></td>
<td></td>
<td><p>[cv] image_classification</p></td>
</tr>
<tr class="row-even"><td><p>Add Rain</p></td>
<td><p>Transformations</p></td>
<td><p>cv</p></td>
<td><p>This test measures the robustness of your model to Add Rain transformations. It does this by taking a sample input, adding rain texture to the image, and measuring the behavior of the model on the transformed input.</p></td>
<td><p>Production inputs can have unusual variations amongst many different dimensions, ranging from lighting changes to sensor errors to compression artifacts. It is important that your models are robust to the introduction of such variations.</p></td>
<td></td>
<td><p>[cv] image_classification</p></td>
</tr>
<tr class="row-odd"><td><p>Add Snow</p></td>
<td><p>Transformations</p></td>
<td><p>cv</p></td>
<td><p>This test measures the robustness of your model to Add Snow transformations. It does this by taking a sample input, adding snow texture to the image, and measuring the behavior of the model on the transformed input.</p></td>
<td><p>Production inputs can have unusual variations amongst many different dimensions, ranging from lighting changes to sensor errors to compression artifacts. It is important that your models are robust to the introduction of such variations.</p></td>
<td></td>
<td><p>[cv] image_classification</p></td>
</tr>
<tr class="row-even"><td><p>Area of Predicted Boxes Distribution</p></td>
<td><p>Drift</p></td>
<td><p>cv</p></td>
<td><p>This test measures the &lt;span&gt;predicted box area&lt;/span&gt; distribution drift between the reference and evaluation sets. By default, it measures drift by using the Population Stability Index of the two distributions.The severity is a function of the magnitude of data drift, and the impact of that drift on model performance. Performance change is attributed using the performance on subsets (quantiles or categories) of a given feature and the change in subset prevalence across datasets.</p></td>
<td><p>The reference set that you use to train your model may not be representative of the evaluation set you encounter in production. If there are statistically significant differences in the &lt;span&gt;predicted box area&lt;/span&gt; distribution between these sets, it can lead to subpar real-world model performance.</p></td>
<td><p>To pass a given test case, the divergence metric must be below the configured threshold.</p></td>
<td><p>[cv] object_detection</p></td>
</tr>
<tr class="row-odd"><td><p>Average Number of Predicted Boxes</p></td>
<td><p>Model Performance</p></td>
<td><p>cv</p></td>
<td><p>This test checks the Average Number of Predicted Boxes metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of Average Number of Predicted Boxes has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p></td>
<td><p>During production, factors like distribution shift or a change in &lt;span&gt;p(y|x)&lt;/span&gt; may cause model performance to decrease significantly.</p></td>
<td><p>By default, this test runs over the Average Number of Predicted Boxes metric with the below thresholds set for the absolute and degradation tests.</p></td>
<td><p>[cv] object_detection</p></td>
</tr>
</tbody>
</table>
<p>All of the RIME tests are listed in the following sections, along with detailed descriptions.</p>
</section>
<section id="tabular">
<h2>Tabular<a class="headerlink" href="#tabular" title="Permalink to this heading"></a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="explanation/tabular_test_categories.html">Test Categories</a></li>
<li class="toctree-l1"><a class="reference internal" href="explanation/tests/tabular/tests.html">Tests</a></li>
</ul>
</div>
</section>
<section id="nlp">
<h2>NLP<a class="headerlink" href="#nlp" title="Permalink to this heading"></a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="explanation/unstructured_test_categories.html">Test Categories</a></li>
<li class="toctree-l1"><a class="reference internal" href="explanation/tests/nlp/text_classification_tests.html">Text Classification Tests</a></li>
<li class="toctree-l1"><a class="reference internal" href="explanation/tests/nlp/natural_language_inference_tests.html">Natural Language Inference Tests</a></li>
<li class="toctree-l1"><a class="reference internal" href="explanation/tests/nlp/named_entity_recognition_tests.html">Named Entity Recognition Tests</a></li>
</ul>
</div>
</section>
<section id="cv">
<h2>CV<a class="headerlink" href="#cv" title="Permalink to this heading"></a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="explanation/unstructured_test_categories.html">Test Categories</a></li>
<li class="toctree-l1"><a class="reference internal" href="explanation/tests/cv/image_classification_tests.html">Image Classification Tests</a></li>
<li class="toctree-l1"><a class="reference internal" href="explanation/tests/cv/object_detection_tests.html">Object Detection Tests</a></li>
</ul>
</div>
</section>
<section id="rime-local-trial">
<h2>RIME local trial<a class="headerlink" href="#rime-local-trial" title="Permalink to this heading"></a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="local_trial/index.html">Local Trial</a></li>
</ul>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../changelogs/v11.html" class="btn btn-neutral float-left" title="RIME v11 Release Note" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="explanation/tabular_test_categories.html" class="btn btn-neutral float-right" title="Test Categories" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Robust Intelligence.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>