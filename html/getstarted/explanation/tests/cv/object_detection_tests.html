<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Object Detection Tests &mdash; RIME  documentation</title>
      <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="https://cdn.datatables.net/1.10.23/css/jquery.dataTables.min.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/sphinx_highlight.js"></script>
        <script src="../../../../_static/clipboard.min.js"></script>
        <script src="../../../../_static/copybutton.js"></script>
        <script src="https://cdn.datatables.net/1.10.23/js/jquery.dataTables.min.js"></script>
        <script src="../../../../_static/main.js"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="Local Trial" href="../../../local_trial/index.html" />
    <link rel="prev" title="Image Classification Tests" href="image_classification_tests.html" />
<link href="../../../../_static/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav">
<div class="ge_header no-print">
    <a id="header-logo" href="../../../../index.html">
        <img src="../../../../_static/header-logo.png" alt="logo" />
    </a>
</div>


  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../index.html" class="icon icon-home">
            RIME
          </a>
              <div class="version">
                0.20.19
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../changelogs.html">Release Notes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../../index.html">RIME overview</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#the-ri-platform">The RI Platform</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#why-use-the-ri-platform">Why use the RI Platform?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#key-features">Key Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#key-machine-learning-tasks-covered">Key Machine Learning Tasks Covered</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#ri-platform-deployment-patterns">RI Platform Deployment Patterns</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../summary_tests.html">Summary Tests</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#ri-platform-consolidated-test-database">RI Platform consolidated test database</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#tabular">Tabular</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#nlp">NLP</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../../../index.html#cv">CV</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../../unstructured_test_categories.html">Summary Tests</a></li>
<li class="toctree-l3"><a class="reference internal" href="image_classification_tests.html">Image Classification Tests</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Object Detection Tests</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#subset-performance">Subset Performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="#transformations">Transformations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#model-performance">Model Performance</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../index.html#rime-local-trial">RIME local trial</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Administering the RI Platform</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../for_admins/index.html">Administering the RI Platform</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Installing the RI Platform</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../installation/index.html">Installation Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data science with the RI Platform</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../for_data_scientists/get_started.html">Get Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../for_data_scientists/how_to_guides.html">How-To Guides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../for_data_scientists/reference.html">Reference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../reference/index.html">RIME in-depth reference</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">RIME</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../../index.html">RIME overview</a></li>
      <li class="breadcrumb-item active">Object Detection Tests</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../_sources/getstarted/explanation/tests/cv/object_detection_tests.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="object-detection-tests">
<h1>Object Detection Tests<a class="headerlink" href="#object-detection-tests" title="Permalink to this heading"></a></h1>
<section id="subset-performance">
<h2>Subset Performance<a class="headerlink" href="#subset-performance" title="Permalink to this heading"></a></h2>
<section id="subset-f1">
<h3>Subset F1<a class="headerlink" href="#subset-f1" title="Permalink to this heading"></a></h3>
</section>
<section id="subset-precision">
<h3>Subset Precision<a class="headerlink" href="#subset-precision" title="Permalink to this heading"></a></h3>
<p>This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the Precision of model predictions within a specific subset is significantly lower than the model prediction Precision over the entire population. </p><p><b>Why it matters:</b> Having different Precision between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. </p><p><b>Configuration:</b> By default, Precision is computed over all predictions/labels. </p><p><b>Example:</b> Suppose in our subset the ground truth has two cats and one dog in the image. Suppose your actual detection has two true positives (the cats), one false positive (it predicts a bird) and one false negative (does not predict the dog). This leads to a Precision of 0.66 on this subset of data. We then compare that to the overall Precision on the full dataset.</p>
</section>
<section id="subset-recall">
<h3>Subset Recall<a class="headerlink" href="#subset-recall" title="Permalink to this heading"></a></h3>
<p>This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the Recall of model predictions within a specific subset is significantly lower than the model prediction Recall over the entire population. </p><p><b>Why it matters:</b> Having different Recall between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. </p><p><b>Configuration:</b> By default, Recall is computed over all predictions/labels. </p><p><b>Example:</b> Suppose in our subset the ground truth has two cats and one dog in the image. Suppose your actual detection has two true positives (the cats), one false positive (it predicts a bird) and one false negative (does not predict the dog). This leads to a Recall of 0.66 on this subset of data. We then compare that to the overall Recall on the full dataset.</p>
</section>
<section id="subset-average-number-of-predicted-boxes">
<h3>Subset Average Number of Predicted Boxes<a class="headerlink" href="#subset-average-number-of-predicted-boxes" title="Permalink to this heading"></a></h3>
</section>
</section>
<section id="transformations">
<h2>Transformations<a class="headerlink" href="#transformations" title="Permalink to this heading"></a></h2>
<section id="gaussian-blur">
<h3>Gaussian Blur<a class="headerlink" href="#gaussian-blur" title="Permalink to this heading"></a></h3>
<p>This test measures the robustness of your model to Gaussian Blur transformations. It does this by taking a sample input, blurring the image, and measuring the behavior of the model on the transformed input.</p><p><b>Why it matters:</b> Production inputs can have unusual variations amongst many different dimensions, ranging from lighting changes to sensor errors to compression artifacts. It is important that your models are robust to the introduction of such variations.</p>
</section>
<section id="color-jitter">
<h3>Color Jitter<a class="headerlink" href="#color-jitter" title="Permalink to this heading"></a></h3>
<p>This test measures the robustness of your model to Color Jitter transformations. It does this by taking a sample input, jittering the image colors, and measuring the behavior of the model on the transformed input.</p><p><b>Why it matters:</b> Production inputs can have unusual variations amongst many different dimensions, ranging from lighting changes to sensor errors to compression artifacts. It is important that your models are robust to the introduction of such variations.</p>
</section>
<section id="gaussian-noise">
<h3>Gaussian Noise<a class="headerlink" href="#gaussian-noise" title="Permalink to this heading"></a></h3>
<p>This test measures the robustness of your model to Gaussian Noise transformations. It does this by taking a sample input, adding gaussian noise to the image, and measuring the behavior of the model on the transformed input.</p><p><b>Why it matters:</b> Production inputs can have unusual variations amongst many different dimensions, ranging from lighting changes to sensor errors to compression artifacts. It is important that your models are robust to the introduction of such variations.</p>
</section>
<section id="randomize-pixels-with-mask">
<h3>Randomize Pixels With Mask<a class="headerlink" href="#randomize-pixels-with-mask" title="Permalink to this heading"></a></h3>
<p>This test measures the robustness of your model to Randomize Pixels With Mask transformations. It does this by taking a sample input, randomizing pixels with fixed probability, and measuring the behavior of the model on the transformed input.</p><p><b>Why it matters:</b> Production inputs can have unusual variations amongst many different dimensions, ranging from lighting changes to sensor errors to compression artifacts. It is important that your models are robust to the introduction of such variations.</p>
</section>
<section id="vertical-flip">
<h3>Vertical Flip<a class="headerlink" href="#vertical-flip" title="Permalink to this heading"></a></h3>
<p>This test measures the robustness of your model to Vertical Flip transformations. It does this by taking a sample input, flipping the image vertically, and measuring the behavior of the model on the transformed input.</p><p><b>Why it matters:</b> Production inputs can have unusual variations amongst many different dimensions, ranging from lighting changes to sensor errors to compression artifacts. It is important that your models are robust to the introduction of such variations.</p>
</section>
<section id="horizontal-flip">
<h3>Horizontal Flip<a class="headerlink" href="#horizontal-flip" title="Permalink to this heading"></a></h3>
<p>This test measures the robustness of your model to Horizontal Flip transformations. It does this by taking a sample input, flipping the image horizontally, and measuring the behavior of the model on the transformed input.</p><p><b>Why it matters:</b> Production inputs can have unusual variations amongst many different dimensions, ranging from lighting changes to sensor errors to compression artifacts. It is important that your models are robust to the introduction of such variations.</p>
</section>
<section id="contrast-increase">
<h3>Contrast Increase<a class="headerlink" href="#contrast-increase" title="Permalink to this heading"></a></h3>
<p>This test measures the robustness of your model to Contrast Increase transformations. It does this by taking a sample input, increase image contrast, and measuring the behavior of the model on the transformed input.</p><p><b>Why it matters:</b> Production inputs can have unusual variations amongst many different dimensions, ranging from lighting changes to sensor errors to compression artifacts. It is important that your models are robust to the introduction of such variations.</p>
</section>
<section id="contrast-decrease">
<h3>Contrast Decrease<a class="headerlink" href="#contrast-decrease" title="Permalink to this heading"></a></h3>
<p>This test measures the robustness of your model to Contrast Decrease transformations. It does this by taking a sample input, decrease image contrast, and measuring the behavior of the model on the transformed input.</p><p><b>Why it matters:</b> Production inputs can have unusual variations amongst many different dimensions, ranging from lighting changes to sensor errors to compression artifacts. It is important that your models are robust to the introduction of such variations.</p>
</section>
<section id="add-rain">
<h3>Add Rain<a class="headerlink" href="#add-rain" title="Permalink to this heading"></a></h3>
<p>This test measures the robustness of your model to Add Rain transformations. It does this by taking a sample input, adding rain texture to the image, and measuring the behavior of the model on the transformed input.</p><p><b>Why it matters:</b> Production inputs can have unusual variations amongst many different dimensions, ranging from lighting changes to sensor errors to compression artifacts. It is important that your models are robust to the introduction of such variations.</p>
</section>
<section id="add-snow">
<h3>Add Snow<a class="headerlink" href="#add-snow" title="Permalink to this heading"></a></h3>
<p>This test measures the robustness of your model to Add Snow transformations. It does this by taking a sample input, adding snow texture to the image, and measuring the behavior of the model on the transformed input.</p><p><b>Why it matters:</b> Production inputs can have unusual variations amongst many different dimensions, ranging from lighting changes to sensor errors to compression artifacts. It is important that your models are robust to the introduction of such variations.</p>
</section>
</section>
<section id="model-performance">
<h2>Model Performance<a class="headerlink" href="#model-performance" title="Permalink to this heading"></a></h2>
<section id="average-confidence">
<h3>Average Confidence<a class="headerlink" href="#average-confidence" title="Permalink to this heading"></a></h3>
<p>This test checks the average confidence of the model predictions between the reference and evaluation sets to see if the metric has experienced significant degradation. The "confidence" of a prediction for classification tasks is defined as the distance between the probability of the predicted class (defined as the argmax over the prediction vector) and 1. We average this metric across all predictions.</p><p><b>Why it matters:</b> During production, factors like distribution shift or a change in <span>p(y|x)</span> may cause model performance to decrease significantly. Since oftentimes labels are not available in a production setting, this metric can serve as a useful proxy for model performance.</p><p><b>Configuration:</b> By default, this test runs if predictions are specified (no labels required).</p><p><b>Example:</b> Assume that on the reference set the model obtained <span>0.85</span> average confidence but on the evaluation set without labels we predict that the model obtained <span>0.5</span> average confidence. Then this test raises a warning.</p>
</section>
<section id="average-thresholded-confidence">
<h3>Average Thresholded Confidence<a class="headerlink" href="#average-thresholded-confidence" title="Permalink to this heading"></a></h3>
<p>This test checks the average thresholded confidence (ATC) of the model predictions between the reference and evaluation sets to see if the metric has experienced significant degradation. ATC is a method for estimating accuracy of unlabeled examples taken from <a href="https://arxiv.org/abs/2201.04234">this paper</a>. The threshold is first computed on the reference set: we pick a confidence threshold such that the percentage of datapoints whose max predicted probability is less than the threshold is around equal to the error rate of the model (here, it is 1-accuracy) on the reference set. Then, we apply this threshold in the evaluation set: the predicted accuracy is then equal to the percentage of datapoints with max predicted probability greater than this threshold.</p><p><b>Why it matters:</b> During production, factors like distribution shift may cause model performance to decrease significantly. Since oftentimes labels are not available in a production setting, this metric can serve as a useful proxy for model performance.</p><p><b>Configuration:</b> By default, this test runs if predictions/labels are specified in the reference set and predictions are specified in the eval set (no labels required).</p><p><b>Example:</b> Assume that on the reference set the model obtained <span>0.85</span> accuracy but on the evaluation set, we find that only 55 percent of datapoints have max predicted probability greater than our threshold. Then our predicted accuracy is <span>0.55</span> and this test raises a warning.</p>
</section>
<section id="calibration-comparison">
<h3>Calibration Comparison<a class="headerlink" href="#calibration-comparison" title="Permalink to this heading"></a></h3>
<p>This test checks that the reference and evaluation sets have sufficiently similar calibration curves as measured by the Mean Squared Error (MSE) between the two curves. The calibration curve is a line plot where the x-axis represents the average predicted probability and the y-axis is the proportion of positive predictions. The curve of the ideal calibrated model is thus a linear straight line from (0, 0) moving linearly.</p><p><b>Why it matters:</b> Knowing how well-calibrated your model is can help you better interpret and act upon model outputs, and can even be an indicator of generalization. A greater difference between reference and evaluation curves could indicate a lack of generalizability. In addition, a change in calibration could indicate that decision-making or thresholding conducted upstream needs to change as it is behaving differently on held-out data.</p><p><b>Configuration:</b> By default, this test runs over the predictions and labels.</p><p><b>Example:</b> Suppose the model’s task is binary classification and predicts whether or not a data point is fraudulent. If we have a reference set in which <span>1%</span> of the data points are fraudulent, but an evaluation set where <span>50%</span> are fraudulent, then our model may not be well calibrated, and the MSE difference in the curves will be large, resulting in a failing test.</p>
</section>
<section id="f1">
<h3>F1<a class="headerlink" href="#f1" title="Permalink to this heading"></a></h3>
<p>This test checks the F1 metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of F1 has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p><p><b>Why it matters:</b> During production, factors like distribution shift or a change in <span>p(y|x)</span> may cause model performance to decrease significantly.</p><p><b>Configuration:</b> By default, this test runs over the F1 metric with the below thresholds set for the absolute and degradation tests.</p><p><b>Example:</b> Assume that on the reference set the model obtained <span>0.85</span> AUC but on the evaluation set the model obtained <span>0.5</span> AUC. Then this test raises a warning.</p>
</section>
<section id="precision">
<h3>Precision<a class="headerlink" href="#precision" title="Permalink to this heading"></a></h3>
<p>This test checks the Precision metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of Precision has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p><p><b>Why it matters:</b> During production, factors like distribution shift or a change in <span>p(y|x)</span> may cause model performance to decrease significantly.</p><p><b>Configuration:</b> By default, this test runs over the Precision metric with the below thresholds set for the absolute and degradation tests.</p><p><b>Example:</b> Assume that on the reference set the model obtained <span>0.85</span> AUC but on the evaluation set the model obtained <span>0.5</span> AUC. Then this test raises a warning.</p>
</section>
<section id="recall">
<h3>Recall<a class="headerlink" href="#recall" title="Permalink to this heading"></a></h3>
<p>This test checks the Recall metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of Recall has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p><p><b>Why it matters:</b> During production, factors like distribution shift or a change in <span>p(y|x)</span> may cause model performance to decrease significantly.</p><p><b>Configuration:</b> By default, this test runs over the Recall metric with the below thresholds set for the absolute and degradation tests.</p><p><b>Example:</b> Assume that on the reference set the model obtained <span>0.85</span> AUC but on the evaluation set the model obtained <span>0.5</span> AUC. Then this test raises a warning.</p>
</section>
<section id="average-number-of-predicted-boxes">
<h3>Average Number of Predicted Boxes<a class="headerlink" href="#average-number-of-predicted-boxes" title="Permalink to this heading"></a></h3>
<p>This test checks the Average Number of Predicted Boxes metric to see both if its performance on the evaluation set alone is satisfactory, as well as if performance in terms of Average Number of Predicted Boxes has degraded from the reference to evaluation set. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p><p><b>Why it matters:</b> During production, factors like distribution shift or a change in <span>p(y|x)</span> may cause model performance to decrease significantly.</p><p><b>Configuration:</b> By default, this test runs over the Average Number of Predicted Boxes metric with the below thresholds set for the absolute and degradation tests.</p><p><b>Example:</b> Assume that on the reference set the model obtained <span>0.85</span> AUC but on the evaluation set the model obtained <span>0.5</span> AUC. Then this test raises a warning.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="image_classification_tests.html" class="btn btn-neutral float-left" title="Image Classification Tests" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../../../local_trial/index.html" class="btn btn-neutral float-right" title="Local Trial" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Robust Intelligence.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>