# Smart Feature Sampling and Other Performance Optimizations

## Smart Feature Sampling
For large datasets, Robust Intelligence can be optimized by running on a subset of features.
This works by specifying the number of features to test on. Robust Intelligence automatically calculates feature
importances in both the with-model case and without-model case (as long as labels or predictions are provided),
and runs its suite of tests across the most important features.

To set up smart feature sampling, specify the `num_feats_to_profile` option in the `data_profiling` parameter.
The maximum number of features that Robust Intelligence can be run on is 750.

More information can be found in [Data Profiling Configuration](configuring_data_profiling.md).

## Profile Caching
Robust Intelligence must understand properties of the datasets and models before tests can be performed.
These are called profiles.  Since it is common to run Robust Intelligence multiple times over the same datasets and models,
these profiles are cached and then re-used to make subsequent runs faster.  For example, the same datasets might
be used to evaluate multiple models. In this case, the dataset profiles will not be recomputed for the second run or any run after that.

Caching is specific to each unique dataset/model ID. Therefore it is best to re-use datasets and models and not
re-register them where it is not necessary. This will allow subsequent runs to be faster.
On the flip-side, the cache can be purposely avoided and refreshed by re-registering your data.

## Smart Dataset Sampling

If a dataset cannot be contained in memory, Robust Intelligence will attempt to take and process as large of a sample as
possible that will fit in memory. This is done by first reading in a subset of data and seeing how much
memory it takes up. The maximal dataset that can be in memory is then extrapolated based on this number and the
total memory available.

The following tables show Robust Intelligence's limits in reference to data in CSV format.
Once these limits are hit the data will be sampled.

### 8GB Memory Ceiling

| Feature Count  | Row Limit Before Sampling |
| ------------- | ------------- |
| 25  | 13,000,000  |
| 50  | 6,400,000  |
| 75  | 4,200,000  |
| 100  | 3,000,000  |
| 200  | 1,600,000  |
| 300  | 1,000,000  |
| 400  | 750,000  |
| 500  | 600,000  |
| 750  | 175,000  |

### 16GB Memory Ceiling

| Feature Count  | Row Limit Before Sampling |
| ------------- | ------------- |
| 25  | 26,000,000  |
| 50  | 12,800,000  |
| 75  | 8,400,000  |
| 100  | 6,000,000  |
| 200  | 3,200,000  |
| 300  | 2,000,000  |
| 400  | 1,500,000  |
| 500  | 1,200,000  |
| 750  | 350,000  |

### 32GB Memory Ceiling

| Feature Count  | Row Limit Before Sampling |
| ------------- | ------------- |
| 25  | 50,000,000  |
| 50  | 24,500,000  |
| 75  | 16,200,000  |
| 100  | 11,500,000  |
| 200  | 6,100,000  |
| 300  | 3,800,000  |
| 400  | 2,800,000  |
| 500  | 2,200,000  |
| 750  | 650,000  |

## Memory Recommendations
Please refer to the below benchmarks to help you determine how much memory to request for your test runs. The numbers in the table represent the maximum dataset size (rows * features) that our tests can accommodate before going out of memory.

The benchmarks assume that dataset sampling is enabled. If you are not using dataset sampling, you will need to request more memory. In addition, the below numbers are meant to be used as a rough guide. Actual memory usage will vary depending on the dataset and model.

### Stress Tests

| Memory | Tabular Regression | Tabular Ranking | Tabular Binary Classification | NLP Multiclass Classification | NLP Inference | Image Multiclass Classification<sup>1</sup> |
| ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- |
| 2GB | 18,000,000 | 100,000,000 | 75,000,000 | - | - | - |
| 4GB | 36,000,000 | 200,000,000 | 150,000,000 | 40,000,000 | 20,000,000 | - |
| 8GB | 70,000,000 | 400,000,000 | 300,000,000 |  80,000,000 | 40,000,000 | - |
| 16GB | 140,000,000 | 800,000,000 | 600,000,000 | 150,000,000| 80,000,000 | 4,000,000 |
| 32GB | 280,000,000 | 1,500,000,000 | 1,000,000,000 | 300,000,000 | 150,000,000 | 8,000,000 |
| 64GB | 550,000,000 | 3,000,000,000 | 2,000,000,000 | 600,000,000 | 300,000,000 | 16,000,000 |
| 128GB | 1,100,000,000 | 6,000,000,000 | 4,000,000,000 | 1,200,000,000 | 600,000,000 | 32,000,000 |

<sup>1</sup>Each image is counted as a single feature. This benchmark leverages Robust Intelligence's image loader and dataset sampling. As such, only a small portion of the images are loaded into memory and embedded. If images are already embedded in your dataset, you will need to request more memory.

### Continuous Tests

| Memory | Tabular Regression | Tabular Ranking | Tabular Binary Classification | NLP Multiclass Classification | NLP Inference | Image Multiclass Classification<sup>1</sup> |
| ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- |
| 2GB | 1,000,000,000 | 1,000,000,000 | 400,000,000 | 7,500,000 | 200,000,000 | - |
| 4GB | 2,000,000,000 | 2,000,000,000 | 800,000,000 | 15,000,000 | 400,000,000 | - |
| 8GB | 4,000,000,000 | 4,000,000,000 | 1,500,000,000 | 30,000,000 | 800,000,000 | 4,000,000 |
| 16GB | 8,000,000,000 | 8,000,000,000 | 3,000,000,000 | 60,000,000 | 1,500,000,000 | 8,000,000 |
| 32GB | 16,000,000,000 | 16,000,000,000 | 6,000,000,000 | 120,000,000 | 3,000,000,000 | 16,000,000 |
| 64GB | 32,000,000,000 | 32,000,000,000 | 12,000,000,000 | 240,000,000 | 6,000,000,000 | 32,000,000|
| 128GB | 64,000,000,000 | 64,000,000,000 | 24,000,000,000 | 480,000,000 | 12,000,000,000 | 64,000,000 |

<sup>1</sup>Each image is counted as a single feature. This benchmark leverages Robust Intelligence's image loader and dataset sampling. As such, only a small portion of the images are loaded into memory and embedded. If images are already embedded in your dataset, you will need to request more memory.

## Related Topics  
[Configuring Data Profiling](configuring_data_profiling.md)  
