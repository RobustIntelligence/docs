.. _continuous-test:

Creating a new Continuous Test
==============================

Robust Intelligence allows you to create Continuous Testing to monitor your
model for vulnerabilities over time. You can create a Continuous Test using the
Python SDK.

.. tabs::

   .. tab:: SDK

      1.  Create a project using the following :ref:`SDK command<rime-sdk>`.

            .. code-block:: python

                project = rime_client.create_project(
                    name="foo", description="bar", model_task="MODEL_TASK_BINARY_CLASSIFICATION"
                )

      2.  Register a reference dataset. Depending on where you are looking to
          pull data from you may need to :ref:`create a data integration
          <configuring-integrations>`.

            .. code-block:: python

                reference_id = project.register_dataset(
                    name=DATASET_NAME,
                    data_config={
                        "connection_info": {
                            "data_file": {
                                "path": "s3://path/to/data/file",
                            },
                        },
                        "data_params": {
                            "label_col": LABEL_COL,
                            "timestamp_col": TIMESTAMP_COL,
                        },
                    },
                    integration_id=INTEGRATION_ID,
                )

          The SDK returns the ID of the reference dataset. There are
          :ref:`additional options <preparing_your_models_and_datasets>`
          available to further customize how Continuous Testing ingests your
          data.

      3.  Register a model. You may optionally provide additional model
          information to allow Robust Intelligence to query the model during
          testing.

            .. code-block:: python

                model_id = project.register_model(
                    name="baz"
                )

          The SDK returns the model ID. There are :ref:`additional options
          <preparing_your_models_and_datasets>` available to further customize
          how Continuous Testing ingests your data.

          If you do not provide the information to query the model, you must
          register predictions of the model and dataset.

            .. code-block:: python

                project.register_predictions(
                    dataset_id=reference_id
                    model_id=model_id
                    pred_config={
                        "connection_info": {
                            "data_file": {
                                "path": "s3://path/to/predictions/file",
                            },
                        },
                        "pred_params": {"pred_col": PREDICTION_COL},
                    },
                    integration_id=INTEGRATION_ID,
                )

      4.  Create a new Continuous Test instance in your project. You may
          choose to create a Continuous Test with scheduled continuous
          testing parameters or leave those
          parameters blank. If *Scheduled CT* is set, Robust
          Intelligence will run the Continuous Test automatically for
          you (at an interval defined by the `bin_size` you set) by
          utilizing the data integrations you specify.
          Otherwise, you will need to manually run the Continuous Test
          over an evaluation dataset that you provide. The steps below
          show how to start a manual run.

            .. code-block:: python

                project.create_ct(
                    model_id=model_id, ref_data_id=reference_id, bin_size=timedelta(hours=3)
                )

      5.  Register an evaluation dataset. This dataset must have the
          ``timestamp_col`` field in the ``data_params`` dictionary.

            .. code-block:: python

                evaluation_id = project.register_dataset(
                    name="bar",
                    data_config={
                        "connection_info": {
                            "data_file": {
                                "path": "s3://path/to/data/file",
                            },
                        },
                        "data_params": {
                            "label_col": LABEL_COL,
                            "timestamp_col": TIMESTAMP_COL,
                        },
                    },
                    integration_id=INTEGRATION_ID,
                )

          The SDK returns the ID of the evaluation dataset. There are
          :ref:`additional options <preparing_your_models_and_datasets>`
          available to further customize how Continuous Testing ingests your
          data.

      6.  Register a prediction set if the information to query the model is
          not provided.

            .. code-block:: python

                project.register_predictions(
                    dataset_id=evaluation_id
                    model_id=model_id
                    pred_config={
                        "connection_info": {
                            "data_file": {
                                "path": "s3://path/to/predictions/file",
                            },
                        },
                        "pred_params": {"pred_col": PREDICTION_COL},
                    },
                    integration_id=INTEGRATION_ID,
                )


      7.  Issue the following command to start the Continuous Test, specifying
          the configuration created in the previous step and the unique ID of
          your Continuous Test. Use the argument ``override_existing_bins`` to
          specify whether this Continuous Test should override the results of
          previous time bins for which other Continuous Tests have run.

          Here, `ct` is your Continuous Test instance, which you can get using
          the `Project.get_ct()` method.

            .. code-block:: python

                ct_job = ct.start_continuous_test(
                   eval_data_id=evaluation_id,  override_existing_bins=False,
                )
                ct_job.get_status(verbose=True, wait_until_finish=True)


          Anytime new data has been gathered and you want to re-run your test,
          repeat steps 5-7 to start a new Continuous Test with your latest
          evaluation dataset and prediction set.

          If you want to automate your Continuous Test so that it runs at a
          regular interval, set its schedule as shown in the section
          *Scheduled Continuous Testing*.

..
   .. tab:: Web UI

      1.  Sign in to an RI Platform instance.
      2.  Select a workspace.
      3.  Select a project.
      4.  In the left navigation bar, click *Continuous Testing*.
      5.  Click *Start Continuous Testing*.
      6.  Select a set of tests.

          =================  ===========================================
          Test category      Test
          =================  ===========================================
          Operational risks  Overall performance
          \                  Subset performance
          \                  Drift (continuous testing focus)
          \                  Transformations (stress testing focus)
          \                  Abnormal inputs (continuous testing focus)
          Security risks     Adversarial
          Fairness risks     Compliance and fairness
          =================  ===========================================

      7.  Select an overall test sensitivity level.
      8.  Choose a model to test from the list of registered models or register a new model.
      9. (When using a registered model) Select the model from the *Select Model* drop-down.
      10. (When registering a new model) Choose whether to upload a model file or to add a model from an online registry.
      11. (When uploading a new model) Type a name and tags for the model.
      12. (When uploading a new model) Drag a *pkl* or *py* model file to the wizard or click *Select File* to browse the file system.
      13. (When registering a new model from a registry) Type the name of the registry, the URI or file path to the model in the registry, and the registry secret.
      14. Click *Next*.
      15. Choose whether to use a registered reference dataset or a dataset from a connection.
      16. (Using a registered reference dataset) Type the name of the registered dataset.
      17. (Using a reference dataset from a connection) Type a connection name, a table name, a timestamp column, and a label column
      18. (Using a reference dataset from a connection) (Optional) Activate the *Predictions (optional)* toggle and enter a connection name, table name, optional timestamp column, and prediction column.
      19. Click *Next*.
      20. Select a bin size from the *Bin Size* drop-down.
      21. Select a type of reference window from the *Reference Window Type* drop-down.
      22. (When the reference window type is Rolling) Select number of bins for the rolling reference window.
      23. In *Data Source*, type the name of the evaluation dataset.
      24. In *Table Name*, type the name of a table in the evaluation dataset.
      25. (Optional) Choose a timestamp column in the specified table from the *Timestamp Column (optional)* drop-down.
      26. Type a column name in *Label Column*.
      27. (Optional) Activate the *Predictions (optional)* toggle and enter a connection name, table name, optional timestamp column name, and prediction column name.
      28. Click *Next*.

      The wizard closes and the new Continuous Test begins initial processing.
