# Tests
## Distribution Drift

### Nulls Per Feature Drift
<p>This test measures the severity of passing to the model data points that have features with a null proportion that has drifted from the distribution observed in the reference set. The severity is a function of the impact on the model, as well as the presence of drift in the data. The model impact measures how much model performance changes due to drift in the given feature. The key detail is the p-value from a two-sample proportion test that checks if there is a statistically significant difference in the frequencies of null values between the reference and evaluation sets.</p><p><b>Why it matters:</b> Distribution drift in null values between training and inference can be caused by a variety of factors, including a change in the data generation process or a change in the preprocessing pipeline. A big shift in null value proportion could indicate a degradation in model performance and signal the need for relabeling and retraining. </p><p><b>Configuration:</b> By default, this test runs over all columns with sufficiently many samples. </p><p><b>Example:</b> Suppose that the observed frequencies of the null values for a given feature is 100/2000 in the reference set but 100/1500 in the test. Then the p-value would be 0.0425. If our p-value threshold was 0.05 then the test would fail.</p>

### Nulls Per Row Drift
<p>This test measures the severity of passing to the model data points that have proportions of null values that have drifted from the distribution observed in the reference set. The severity is a function of the impact on the model, as well as the presence of drift in the data. The model impact measures how much predictions change when the observed drift is applied to a given row. The key detail displayed is the PSI statistic that is a measure of how statistically significant the difference in the proportion of null values in a row between the reference and evaluation sets is.</p><p><b>Why it matters:</b> Distribution drift in null values between training and inference can be caused by a variety of factors, including a change in the data generation process or a change in the preprocessing pipeline. A big shift in null value proportion could indicate a degradation in model performance and signal the need for relabeling and retraining.</p><p><b>Configuration:</b> By default, this test runs over all rows.</p><p><b>Example:</b> Suppose that in the reference set 5% of rows had more than three features that were null. If we observe in the evaluation set that now 50% of rows had more than three features that were null, this test would fail, highlighting a large drift in the proportion of features within a row that were null.</p>

### Feature Correlation Drift
<p>This test measures the severity of feature correlation drift from the reference to the evaluation set for a given pair of features. The severity is a function of the mutual information drift in the data. The key detail is the difference in correlation scores between the reference and evaluation sets, along with an associated p-value. Correlation is a measure of the linear relationship between two numeric features, so this test checks for significant changes in this relationship between pairs of features in the reference and evaluation sets. To compute the p-value, we use Fisher's z-transformation to convert the distribution of sample correlations to a normal distribution, and then we run a standard two-sample test on two normal distributions.</p><p><b>Why it matters:</b> Correlation drift between training and inference can be caused by a variety of factors, including a change in the data generation process or a change in the underlying processing stage. A big shift in these dependencies could indicate shifting datasets and degradation in model performance, signalling the need for relabeling and retraining.</p><p><b>Configuration:</b> By default, this test runs over all pairs of features in the dataset.</p><p><b>Example:</b> Suppose that the correlation between <span>country</span> and <span>state</span> is 0.5 in the reference set but 0.7 in the evaluation set, and the p-value is 0.03. Then the large difference in scores indicates that the dependency between the two features has drifted. If our difference threshold was 0.2, and p-value threshold was 0.05, then the test would fail.</p>

### Mutual Information Drift (Feature-to-Feature)
<p>This test measures the severity of feature mutual information drift from the reference to the evaluation set for a given pair of features. The severity is a function of the mutual information drift in the data. The key detail is the difference in mutual information scores between the reference and evaluation sets. Mutual information is a measure of how dependent two features are, so this checks for significant changes in dependence between pairs of features in the reference and evaluation sets.</p><p><b>Why it matters:</b> Mutual information drift between training and inference can be caused by a variety of factors, including a change in the data generation process or a change in the underlying processing stage. A big shift in these dependencies could indicate shifting datasets and degradation in model performance, signalling the need for relabeling and retraining.</p><p><b>Configuration:</b> By default, this test runs over all pairs of features in the dataset.</p><p><b>Example:</b> Suppose that the mutual information between <span>country</span> and <span>state</span> is 0.5 in the reference set but 0.7 in the evaluation set. Then the large difference in scores indicates that the dependency between the two features has drifted. If our difference threshold was 0.2 then the test would fail.</p>

### Mutual Information Drift (Feature-to-Label)
<p>This test measures the severity of feature mutual information drift from the reference to the evaluation set for a given pair of features. The severity is a function of the mutual information drift in the data. The key detail is the difference in mutual information scores between the reference and evaluation sets. Mutual information is a measure of how dependent two features are, so this checks for significant changes in dependence between pairs of features in the reference and evaluation sets.</p><p><b>Why it matters:</b> Mutual information drift between training and inference can be caused by a variety of factors, including a change in the data generation process or a change in the underlying processing stage. A big shift in these dependencies could indicate shifting datasets and degradation in model performance, signalling the need for relabeling and retraining.</p><p><b>Configuration:</b> By default, this test runs over all pairs of features in the dataset.</p><p><b>Example:</b> Suppose that the mutual information between <span>country</span> and <span>state</span> is 0.5 in the reference set but 0.7 in the evaluation set. Then the large difference in scores indicates that the dependency between the two features has drifted. If our difference threshold was 0.2 then the test would fail.</p>

### Categorical Feature Drift
<p>This test measures the severity of passing to the model data points that have categorical features which have drifted from the distribution observed in the reference set. The severity is a function of the impact on the model, as well as the presence of drift in the data. The model impact measures how much model performance changes due to drift in the given feature. The key detail displayed is the PSI test statistic, which is a measure of how statistically significant the difference between the frequencies of categorical values in the reference and evaluation sets is.</p><p><b>Why it matters:</b> Distribution drift in categorical features between training and inference can be caused by a variety of factors, including a change in the data generation process or a change in the preprocessing pipeline. A big shift in categorical features towards categorical subsets that your model performs poorly in could indicate a degradation in model performance and signal the need for relabeling and retraining. </p><p><b>Configuration:</b> By default, this test runs over all categorical columns with sufficiently many samples. </p><p><b>Example:</b> Suppose that the observed frequencies of the <span>isLoggedIn</span> feature is [100, 200] in the reference set but [25, 150] in the test set. Then the PSI would be 0.201. If our PSI threshold was 0.1 then the test would fail.</p>

### Label Drift (PSI)
<p>This test checks that the difference in label distribution between the reference and evaluation sets is small, using PSI test. The key detail displayed is the PSI statistic which is a measure of how different the frequencies of the column in the reference and evaluation sets are.</p><p><b>Why it matters:</b> Label distribution shift between reference and test can indicate that the underlying data distribution has changed significantly enough to modify model decisions. This may mean that the model needs to be retrained to adjust to the new data environment. In addition, significant label distribution shift may indicate that upstream decision-making modules (e.g. thresholds) may need to be updated.</p><p><b>Configuration:</b> This test is run by default whenever both the reference and evaluation sets have associated labels.</p><p><b>Example:</b> Suppose that the observed frequencies of the label column is [100, 200] in the reference set but [25, 150] in the test set. Then the PSI would be 0.201. If our PSI threshold was 0.1 then the test would fail.</p>

### Label Drift
<p>This test checks that the difference in label distribution between the reference and evaluation sets is small, using the Kolmogorovâ€“Smirnov (K-S) test. The key detail displayed is the KS statistic which is a measure of how different the labels in the reference and evaluation sets are. Concretely, the KS statistic is the maximum difference of the empirical CDF's of the two label columns.</p><p><b>Why it matters:</b> Label distribution shift between reference and test can indicate that the underlying data distribution has changed significantly enough to modify model decisions. This may mean that the model needs to be retrained to adjust to the new data environment. In addition, significant label distribution shift may indicate that upstream decision-making modules (e.g. thresholds) may need to be updated.</p><p><b>Configuration:</b> This test is run by default whenever both the reference and evaluation sets have associated labels.</p><p><b>Example:</b> Suppose that the distribution of labels changes between the reference and evaluation sets such that the p-value for the K-S test between these two samples is <span>0.005</span> and the test statistic is <span>0.2</span>. If the p-value threshold is set to <span>0.01</span> and the model impact threshold is set to <span>0.1</span>, this test would raise a warning.</p>

### Numeric Feature Drift
<p>This test measures the severity of passing to the model data points that have numeric features that have drifted from the distribution observed in the reference set. The severity is a function of the impact on the model, as well as the presence of drift in the data. The model impact measures how much model performance changes due to drift in the given feature. The key detail is the Population Stability Index statistic. The Population Stability Index (PSI) is a measure of how different two distributions are. Given two distributions P and Q, it is computed as the sum of the KL Divergence between P and Q and the (reverse) KL Divergence between Q and P. Thus, PSI is symmetric.</p><p><b>Why it matters:</b> Distribution shift between training and inference can cause degradation in model performance. If the shift is sufficiently large, retraining the model on newer data may be necessary.</p><p><b>Configuration:</b> By default, this test runs over all numeric columns with sufficiently many samples and stored quantiles in each of the reference and evaluation sets.</p><p><b>Example:</b> Suppose that the distribution of a feature <span>Age</span> changes between the reference and evaluation sets such that the Population Stability Index between these two samples is <span>0.2</span>. If the distance threshold is set to <span>0.1</span>, this test would raise a warning.</p>

### Overall Metrics
<p>This test checks a set of overall metrics to see if any have experienced significant degradation. The key detail displays whether the given performance metric has degraded beyond a defined threshold.</p><p><b>Why it matters:</b> During production, factors like distribution shift or a change in <span>p(y|x)</span> may cause model performance to decrease significantly.</p><p><b>Configuration:</b> By default, this test runs over all metrics for this model task.</p><p><b>Example:</b> Assume that on the reference set the model obtained <span>0.85</span> AUC but on the evaluation set the model obtained <span>0.5</span> AUC. Then this test raises a warning.</p>

### Prediction Drift
<p>This test checks that the difference in the prediction distribution between the reference and evaluation sets is small, using Population Stability Index. The key detail displayed is the PSI which is a measure of how different the prediction distributions in the reference and evaluation sets are.</p><p><b>Why it matters:</b> Prediction distribution shift between reference and test can indicate that the underlying data distribution has changed significantly enough to modify model decisions. This may mean that the model needs to be retrained to adjust to the new data environment. In addition, significant prediction distribution drift may indicate that upstream decision-making modules (e.g. thresholds) may need to be updated.</p><p><b>Configuration:</b> This test is run by default whenever both the reference and evaluation sets have associated predictions. Different thresholds are associated with different severities.</p><p><b>Example:</b> Suppose that the PSI between the prediction distributions in the reference and evaluation sets is 0.201. Then if the PSI thresholds are (0.1, 0.2, 0.3), the test would fail with medium severity.</p>

### Calibration Comparison
<p>This test checks that the reference and evaluation sets have sufficiently similar calibration curves as measured by the Mean Squared Error (MSE) between the two curves. The calibration curve is a line plot where the x-axis represents the average predicted probability and the y-axis is the proportion of positive predictions. The curve of the ideal calibrated model is thus a linear straight line from (0, 0) moving linearly.</p><p><b>Why it matters:</b> Knowing how well-calibrated your model is can help you better interpret and act upon model outputs, and can even be an indicator of generalization. A greater difference between reference and evaluation curves could indicate a lack of generalizability. In addition, a change in calibration could indicate that decision-making or thresholding conducted upstream needs to change as it is behaving differently on held-out data.</p><p><b>Configuration:</b> By default, this test runs over the predictions and labels.</p><p><b>Example:</b> Suppose the modelâ€™s task is binary classification and predicts whether or not a datapoint is fraudulent. If we have a reference set in which <span>1%</span> of the datapoints are fraudulent, but an evaluation set where <span>50%</span> are fraudulent, then our model may not be well calibrated, and the MSE difference in the curves will be large, resulting in a failing test.</p>

### Predicted Label Drift (PSI)
<p>This test checks that the difference in predicted label distribution between the reference and evaluation sets is small, using PSI test. The key detail displayed is the PSI statistic which is a measure of how different the frequencies of the column in the reference and evaluation sets are.</p><p><b>Why it matters:</b> Predicted Label distribution shift between reference and test can indicate that the underlying data distribution has changed significantly enough to modify model decisions. This may mean that the model needs to be retrained to adjust to the new data environment. In addition, significant predicted label distribution shift may indicate that upstream decision-making modules (e.g. thresholds) may need to be updated.</p><p><b>Configuration:</b> This test is run by default whenever the model or predictions is provided.</p><p><b>Example:</b> Suppose that the observed frequencies of the predicted label column is [100, 200] in the reference set but [25, 150] in the test set. Then the PSI would be 0.201. If our PSI threshold was 0.1 then the test would fail.</p>

## Abnormal Input

### Must be Int
<p>This test measures the number of failing rows in your data with values not of type Integer and their impact on the model. The model impact is the difference in model performance between passing and failing rows with values not of type Integer. If labels are not provided, prediction change is used instead of model performance change.</p><p><b>Why it matters:</b> A feature may require a specific type. However, errors in the data pipeline may produce values that are outside the expected type. Failing to catch such errors may lead to errors or undefined behavior from the model.</p><p><b>Configuration:</b> By default, this test runs over all features that are inferred to be type Integer.</p><p><b>Example:</b> Say that the feature <span>X</span> requires the Integer type. This test raises a warning if we observe any values where <span>X</span> is represented as a different type instead.</p>

### Must be Float
<p>This test measures the number of failing rows in your data with values not of type Float and their impact on the model. The model impact is the difference in model performance between passing and failing rows with values not of type Float. If labels are not provided, prediction change is used instead of model performance change.</p><p><b>Why it matters:</b> A feature may require a specific type. However, errors in the data pipeline may produce values that are outside the expected type. Failing to catch such errors may lead to errors or undefined behavior from the model.</p><p><b>Configuration:</b> By default, this test runs over all features that are inferred to be type Float.</p><p><b>Example:</b> Say that the feature <span>X</span> requires the Float type. This test raises a warning if we observe any values where <span>X</span> is represented as a different type instead.</p>

### Must be String
<p>This test measures the number of failing rows in your data with values not of type String Categorical and their impact on the model. The model impact is the difference in model performance between passing and failing rows with values not of type String Categorical. If labels are not provided, prediction change is used instead of model performance change.</p><p><b>Why it matters:</b> A feature may require a specific type. However, errors in the data pipeline may produce values that are outside the expected type. Failing to catch such errors may lead to errors or undefined behavior from the model.</p><p><b>Configuration:</b> By default, this test runs over all features that are inferred to be type String Categorical.</p><p><b>Example:</b> Say that the feature <span>X</span> requires the String Categorical type. This test raises a warning if we observe any values where <span>X</span> is represented as a different type instead.</p>

### Must be Boolean
<p>This test measures the number of failing rows in your data with values not of type Boolean Categorical and their impact on the model. The model impact is the difference in model performance between passing and failing rows with values not of type Boolean Categorical. If labels are not provided, prediction change is used instead of model performance change.</p><p><b>Why it matters:</b> A feature may require a specific type. However, errors in the data pipeline may produce values that are outside the expected type. Failing to catch such errors may lead to errors or undefined behavior from the model.</p><p><b>Configuration:</b> By default, this test runs over all features that are inferred to be type Boolean Categorical.</p><p><b>Example:</b> Say that the feature <span>X</span> requires the Boolean Categorical type. This test raises a warning if we observe any values where <span>X</span> is represented as a different type instead.</p>

### Must be URL
<p>This test measures the number of failing rows in your data with values not of type URL Categorical and their impact on the model. The model impact is the difference in model performance between passing and failing rows with values not of type URL Categorical. If labels are not provided, prediction change is used instead of model performance change.</p><p><b>Why it matters:</b> A feature may require a specific type. However, errors in the data pipeline may produce values that are outside the expected type. Failing to catch such errors may lead to errors or undefined behavior from the model.</p><p><b>Configuration:</b> By default, this test runs over all features that are inferred to be type URL Categorical.</p><p><b>Example:</b> Say that the feature <span>X</span> requires the URL Categorical type. This test raises a warning if we observe any values where <span>X</span> is represented as a different type instead.</p>

### Must be Domain
<p>This test measures the number of failing rows in your data with values not of type Domain Categorical and their impact on the model. The model impact is the difference in model performance between passing and failing rows with values not of type Domain Categorical. If labels are not provided, prediction change is used instead of model performance change.</p><p><b>Why it matters:</b> A feature may require a specific type. However, errors in the data pipeline may produce values that are outside the expected type. Failing to catch such errors may lead to errors or undefined behavior from the model.</p><p><b>Configuration:</b> By default, this test runs over all features that are inferred to be type Domain Categorical.</p><p><b>Example:</b> Say that the feature <span>X</span> requires the Domain Categorical type. This test raises a warning if we observe any values where <span>X</span> is represented as a different type instead.</p>

### Must be Email
<p>This test measures the number of failing rows in your data with values not of type Email Categorical and their impact on the model. The model impact is the difference in model performance between passing and failing rows with values not of type Email Categorical. If labels are not provided, prediction change is used instead of model performance change.</p><p><b>Why it matters:</b> A feature may require a specific type. However, errors in the data pipeline may produce values that are outside the expected type. Failing to catch such errors may lead to errors or undefined behavior from the model.</p><p><b>Configuration:</b> By default, this test runs over all features that are inferred to be type Email Categorical.</p><p><b>Example:</b> Say that the feature <span>X</span> requires the Email Categorical type. This test raises a warning if we observe any values where <span>X</span> is represented as a different type instead.</p>

### Null Check
<p>This test measures the number of failing rows in your data with nulls in features that should not have nulls and their impact on the model. The model impact is the difference in model performance between passing and failing rows with nulls in features that should not have nulls. If labels are not provided, prediction change is used instead of model performance change.</p><p><b>Why it matters:</b> The model may make certain assumptions about a column depending on whether or not it had nulls in the training data. If these assumptions break during production, this may damage the model's performance. For example, if a column was never null during training then a model may not have learned to be robust against noise in that column. </p><p><b>Configuration:</b> By default, this test runs over all columns that had zero nulls in the reference set. </p><p><b>Example:</b> Suppose that the feature <span>Age</span> was never null in the reference set. This test raises a warning if <span>Age</span> was null <span>10%</span> of the time in the evaluation set or if model performance decreases on observed datapoints with nulls </p>

### Numeric Outliers
<p>This test measures the number of failing rows in your data with outliers and their impact on the model. Outliers are values which may not necessarily be outside of an allowed range for a feature, but are extreme values that are unusual and may be indicative of abnormality. The model impact is the difference in model performance between passing and failing rows with outliers. If labels are not provided, prediction change is used instead of model performance change.</p><p><b>Why it matters:</b> Outliers can be a sign of corrupted or otherwise erroneous data, and can degrade model performance if used in the training data, or lead to unexpected behaviour if input at inference time.</p><p><b>Configuration:</b> By default this test is run over each numeric feature that is neither unique nor ascending.</p><p><b>Example:</b> Suppose there is a feature <span>age</span> for which in the reference set the values <span>103</span> and <span>114</span> each appear once but every other value (with subsantial sample size) is contained within the range <span>[0, 97]</span>. Then we would infer a lower outlier threshold of <span>0</span> and an upper outlier threshold of <span>97</span>. This test raises a warning if we observe any values in the evaluation set outside these thresholds or if model performance decreases on observed datapoints with outliers.</p>

### Unseen URL
<p>This test measures the number of failing rows in your data with unseen URL values and their impact on the model. The model impact is the difference in model performance between passing and failing rows with unseen URL values. If labels are not provided, prediction change is used instead of model performance change.</p><p><b>Why it matters:</b> Unseen categorical values are a common failure point in machine learning systems; since these models are trained over a reference set, they may yield uninterpretable or undefined behavior when interacting with an unseen categorical value. In addition, such errors may expose gaps or errors in data collection.</p><p><b>Configuration:</b> By default, this test runs over all features inferred to contain URLs.</p><p><b>Example:</b> Say that the feature <span>WebURL</span> contains the values <span>['http://google.com', 'http://yahoo.com']</span> from the reference set. This test raises a warning if we observe any unseen values in the evaluation set such as <span>'http://xyzabc.com'</span>.</p>

### Unseen Domain
<p>This test measures the number of failing rows in your data with unseen domain values and their impact on the model. The model impact is the difference in model performance between passing and failing rows with unseen domain values. If labels are not provided, prediction change is used instead of model performance change.</p><p><b>Why it matters:</b> Unseen categorical values are a common failure point in machine learning systems; since these models are trained over a reference set, they may yield uninterpretable or undefined behavior when interacting with an unseen categorical value. In addition, such errors may expose gaps or errors in data collection.</p><p><b>Configuration:</b> By default, this test runs over all features inferred to contain domains.</p><p><b>Example:</b> Say that the feature <span>WebDomain</span> contains the values <span>['gmail.com', 'hotmail.com']</span> from the reference set. This test raises a warning if we observe any unseen values in the evaluation set such as <span>'xyzabc.com'</span>.</p>

### Unseen Email
<p>This test measures the number of failing rows in your data with unseen email values and their impact on the model. The model impact is the difference in model performance between passing and failing rows with unseen email values. If labels are not provided, prediction change is used instead of model performance change.</p><p><b>Why it matters:</b> Unseen categorical values are a common failure point in machine learning systems; since these models are trained over a reference set, they may yield uninterpretable or undefined behavior when interacting with an unseen categorical value. In addition, such errors may expose gaps or errors in data collection.</p><p><b>Configuration:</b> By default, this test runs over all features inferred to contain emails.</p><p><b>Example:</b> Say that the feature <span>Email</span> contains the values <span>['user1@gmail.com', 'user2@yahoo.com']</span> from the reference set. This test raises a warning if we observe any unseen values in the evaluation set such as <span>'xyz@xyzabc.com'</span>.</p>

### Out of Range
<p>This test measures the number of failing rows in your data with values outside the inferred range of allowed values and their impact on the model. The model impact is the difference in model performance between passing and failing rows with values outside the inferred range of allowed values. If labels are not provided, prediction change is used instead of model performance change.</p><p><b>Why it matters:</b> In production, the model may encounter corrupted or manipulated out of range values. It is important that the model is robust to such extremities.</p><p><b>Configuration:</b> By default, this test runs over all numeric features.</p><p><b>Example:</b> In the reference set, the <span>Age</span> feature has a range of <span>[0, 121]</span>. This test raises a warning if we observe values outside of this range in the evaluation set (eg. <span>150, 200</span>) or if model performance decreases on observed datapoints outside of this range.</p>

### Rare Categories
<p>This test measures the severity of passing to the model data points whose features contain rarely observed categories (relative the reference set). The severity is a function of the impact of these values on the model, as well as the presence of these values in the data. The model impact is the difference in model performance between passing and failing rows with rarely observed categorical values. If labels are not provided, prediction change is used instead of model performance change. The number of failing rows refers to the number of times rarely observed categorical values are observed in the evaluation set.</p><p><b>Why it matters:</b> Rare categories are a common failure point in machine learning systems because less data often means worse performance. In addition, this may expose gaps or errors in data collection.</p><p><b>Configuration:</b> By default, this test runs over all categorical features. A category is considered rare if it occurs fewer than <span>min_num_occurrences</span> times, or if it occurs less than <span>min_pct_occurrences</span> of the time. If neither of these values are specified, the rate of appearance below which a category is considered rare is <span>min_ratio_rel_uniform</span> divided by the number of classes.</p><p><b>Example:</b> Say that the feature <span>AgeGroup</span> takes on the value <span>0-18</span> twice while taking on the value <span>35-55</span> a total of <span>98</span> times. If the <span>min_num_occurences</span> is <span>5</span> and the <span>min_pct_occurrences</span> is <span>0.03</span> then the test will flag the value <span>0-18</span> as a rare category.</p>

### Empty String
<p>This test measures the number of failing rows in your data with empty string values instead of null values and their impact on the model. The model impact is the difference in model performance between passing and failing rows with empty string values instead of null values. If labels are not provided, prediction change is used instead of model performance change.</p><p><b>Why it matters:</b> In production, the model may encounter corrupted or manipulated string values. Null values and empty strings are often expected to be treated the same, but the model might not treat them that way. It is important that the model is robust to such extremities.</p><p><b>Configuration:</b> By default, this test runs over all string features with null values.</p><p><b>Example:</b> In the reference set, the <span>Name</span> feature contains nulls. This test raises a warning if we observe any empty string in the <span>Name</span> feature or if these values decrease model performance.</p>

### Inconsistencies
<p>This test measures the severity of passing to the model data points whose values are inconsistent (as inferred from the reference set). The severity is a function of the impact of these values on the model, as well as the presence of these values in the data. The model impact is the difference in model performance between passing and failing rows with data containing inconsistent feature values. If labels are not provided, prediction change is used instead of model performance change. The number of failing rows refers to the number of times data containing inconsistent feature values are observed in the evaluation set.</p><p><b>Why it matters:</b> Inconsistent values might be the result of malicious actors manipulating the data or errors in the data pipeline. Thus, it is important to be aware of inconsistent values to identify sources of manipulations or errors.</p><p><b>Configuration:</b> By default, this test runs on pairs of categorical features whose correlations exceed some minimum threshold. The default threshold for the frequency ratio below which values are considered to be inconsistent is <span>0.02</span>.</p><p><b>Example:</b> Suppose we have a feature <span>country</span> that takes on value <span>"US"</span> with frequency <span>0.5</span>, and a feature <span>time_zone</span> that takes on value <span>"Central European Time"</span> with frequency <span>0.2</span>. Then if these values appear together with frequency less than <span>0.5 * 0.2 * 0.02 = 0.002 </span>, in the reference set, rows in which these values do appear together are inconsistencies.</p>

### Capitalization
<p>This test measures the number of failing rows in your data with different types of capitalization and their impact on the model. The model impact is the difference in model performance between passing and failing rows with different types of capitalization. If labels are not provided, prediction change is used instead of model performance change.</p><p><b>Why it matters:</b> In production, models can come across the same value with different capitalizations, making it important to explicitly check that your model is invariant to such differences.</p><p><b>Configuration:</b> By default, this test runs over all categorical features.</p><p><b>Example:</b> Suppose we had a column that corresponded to country code. For a specific row, let's say the observed value in the reference set was <span>USA</span>. This test raises a warning if we observe a similar value in the evaluation set with case changes, e.g. <span>uSa</span> or if model performance decreases on observed datapoints with case changes.</p>

### Required Characters
<p>This test measures the number of failing rows in your data with strings without any required characters and their impact on the model. The model impact is the difference in model performance between passing and failing rows with strings without any required characters. If labels are not provided, prediction change is used instead of model performance change.</p><p><b>Why it matters:</b> A feature may require specific characters. However, errors in the data pipeline may allow invalid data points that lack these required characters to pass. Failing to catch such errors may lead to noisier training data or noisier predictions during inference, which can degrade model metrics.</p><p><b>Configuration:</b> By default, this test runs over all string features that are inferred to have required characters.</p><p><b>Example:</b> Say that the feature <span>email</span> requires the character <span>@</span>. This test raises a warning if we observe any values in the evaluation set where the character is missing.</p>

### Unseen Categorical
<p>This test measures the number of failing rows in your data with unseen categorical values and their impact on the model. The model impact is the difference in model performance between passing and failing rows with unseen categorical values. If labels are not provided, prediction change is used instead of model performance change.</p><p><b>Why it matters:</b> Unseen categorical values are a common failure point in machine learning systems; since these models are trained over a reference set, they may yield uninterpretable or undefined behavior when interacting with an unseen categorical value. In addition, such errors may expose gaps or errors in data collection.</p><p><b>Configuration:</b> By default, this test runs over all categorical features.</p><p><b>Example:</b> Say that the feature <span>Animal</span> contains the values <span>['Cat', 'Dog']</span> from the reference set. This test raises a warning if we observe any unseen values in the evaluation set such as <span>'Mouse'</span>.</p>

## Compliance

### Demographic Parity
<p>This test is commonly known as the demographic parity or statistical parity test in fairness literature. This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the Positive Prediction Rate of model predictions within a specific subset is significantly different than the model prediction Positive Prediction Rate over the entire 'population'. </p><p><b>Why it matters:</b> Demographic parity is one of the most well-known and strict measures of fairness. It is meant to be used in a setting where we assert that the base label rates between subgroups should be the same (even if empirically they are different). This contrasts with equality of opportunity or predictive parity tests, which permit classification rates to depend on a protected attribute. It can be useful in legal/compliance settings where we want a selection rate for any protected group to fundamentally be the same as other groups. </p><p><b>Configuration:</b> By default, the Positive Prediction Rate is computed for all protected features. </p><p><b>Example:</b> Suppose we had data with the following protected feature 'animal': <span>['cat', 'cat', 'cat', 'dog', 'dog', 'dog']</span>, and model predictions <span>[0.3, 0.3, 0.9, 0.9, 0.9, 0.3]</span>. Then regardless of the labels, the Positive Prediction Rate over the feature values ('cat', 'dog') would be (0.33, 0.66), indicating a failure in demographic parity. </p>

### Protected Feature Drift
<p>This test measures the severity of passing to the model data points that have categorical features which have drifted from the distribution observed in the reference set. The severity is a function of the impact on the model, as well as the presence of drift in the data. The model impact measures how much model performance changes due to drift in the given feature. The key detail displayed is the Î§Â² test statistic and p-value, which are measures of how statistically significant the difference between the frequencies of categorical values in the reference and evaluation sets is.</p><p><b>Why it matters:</b> Distribution drift in categorical features between training and inference can be caused by a variety of factors, including a change in the data generation process or a change in the preprocessing pipeline. A big shift in categorical features towards categorical subsets that your model performs poorly in could indicate a degradation in model performance and signal the need for relabeling and retraining. </p><p><b>Configuration:</b> By default, this test runs over all categorical columns with sufficiently many samples. </p><p><b>Example:</b> Suppose that the observed frequencies of the <span>isLoggedIn</span> feature is [100, 200] in the reference set but [75, 100] in the test set. Then the p-value would be 0.048. If our p-value threshold was 0.05 then the test would fail.</p>

### Discrimination By Proxy
<p>This test checks whether any feature is a proxy for a protected feature. It runs over categorical features, using mutual information as a measure of similarity with a protected feature. Mutual information measures any dependencies between two variables.</p><p><b>Why it matters:</b> A common strategy to try to ensure a model is not biased is to remove protected features from the training data entirely so the model cannot learn over them. However, if other features are highly dependent on those features, that could lead to the model effectively still training over those features by proxy.</p><p><b>Configuration:</b> By default, this test is run over all categorical protected columns.</p><p><b>Example:</b> Suppose we had data with a protected feature (`gender`). If there was another feature, like `title`, which was highly associated with gender, this test would raise a warning if the mutual information between those two features was particularly high.</p>

### Intersectional Group Fairness
<p>This test checks whether the model performs equally well across subgroups created from the intersection of protected groups. The test first creates unique pairs of categorical protected features. We then test whether the positive prediction rate of model predictions within a specific subset is significantly lower than the model positive prediction rate over the entire population. This will expose hidden biases against groups at the intersection of these protected features</p><p><b>Why it matters:</b> Most existing work in the fairness literature deals with a binary view of fairness - either a particular group is performing worse or not. This binary categorization misses the important nuance of the fairness field - that biases can often be amplified in subgroups that combine membership from different protected groups, especially if such a subgroup is particularly underrepresented in opportunities historically. The intersectional group fairness test is run over subsets representing this intersection between two protected groups.</p><p><b>Configuration:</b> This test runs over unique pairs of categorical protected features.</p><p><b>Example:</b> Suppose your dataset contains two protected features: race and gender. Both features pass the demographic parity test for categories women, men, white and black. However, when certain subsets of these features are combined, such as black women or white men, the positive prediction rates perform significantly worse than the overall population. This would show disparate impact towards this subgroup.</p>

### Selection Rate
<p>This test checks whether the selection rate for any subset of a feature performs as well as the best selection rate across all subsets of that feature. The selection rate is calculated as the Positive Prediction Rate. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the selection rate of model predictions within a specific subset is significantly lower than that of other subsets by taking a ratio of the rates.</p><p><b>Why it matters:</b> Assessing differences in selection rate is an important measures of fairness. It is meant to be used in a setting where we assert that the base selection rates between subgroups should be the same (even if empirically they are different). This contrasts with equality of opportunity or predictive parity tests, which permit classification rates to depend on a sensitive attribute. It can be useful in legal/compliance settings where we want a selection rate for any sensitive group to fundamentally be the same as other groups. </p><p><b>Configuration:</b> By default, the selection rate is computed for all protected features. </p><p><b>Example:</b> Suppose we had data with the following protected feature 'animal': <span>['cat', 'cat', 'cat', 'dog', 'dog', 'dog']</span>, and model predictions <span>[0.3, 0.3, 0.9, 0.9, 0.9, 0.3]</span>. Then regardless of the labels, the selection rate over the feature values ('cat', 'dog') would be (0.33, 0.66), indicating a failure because cats would be selected half as often as dogs.</p>

### Feature Independence
<p>This test checks the independence of each protected feature with the predicted label class. It runs over categorical protected features and uses the chi square test of independence to determine the feature independence. The test compares the observed data to a model that distributes the data according to the expectation that the variables are independent. Wherever the observed data does not fit the model, the likelihood that the variables are dependent becomes stronger.</p><p><b>Why it matters:</b> A test of independence assesses whether observations consisting of measures on two variables, expressed in a contingency table, are independent of each other. This can be useful when assessing how protected features impact the predicted class and helping with the feature selection process.</p><p><b>Configuration:</b> By default, this test is run over all protected categorical features.</p><p><b>Example:</b> Let's say you have a model that predicts whether or not a person will be hired or not. One protected feature is gender. If these two variables are independent then the male-female ratio across hired and not hired should be the same. The p-value is 0.06 and the chi squared value is 300. The p-value is above the threshold of 0.05 to declare independence.</p>

### Subset Sensitivity
<p>This test measures how sensitive the model is to substituting the lowest performing subset of a feature into a sample of data. The test splits the dataset into various subsets based on the feature values and finds the lowest performing subset, based on the lowest Positive Prediction Rate. The test then substitutes this subset into a sample from the original data and calculates the average prediction change. This test fails if a model predicts worse on the lowest performing subset.</p><p><b>Why it matters:</b> Assessing differences in model output is an important measure of fairness. If the model performs worse because of the value of a protected feature such as race or gender, then this could indicate bias. It can be useful in legal/compliance settings where we fundamentally want the prediction for any protected group to be the same as for other groups. </p><p><b>Configuration:</b> By default, the subset sensitivity is computed for all protected features that are strings.</p><p><b>Example:</b> Suppose the data had the following protected feature 'animal': <span>['cat', 'cat', 'cat', 'dog', 'dog', 'dog', 'horse', 'horse']</span>, and model predictions for cat were the lowest. If substituting cat for dog and horse in the other inputs causes model predictions to decrease, then this would indicate a failure because the model disadvantages cats.</p>

## Transformations

### Out of Range Substitution
<p>This test measures the impact on the model when we substitute values outside the inferred range of allowed values into clean datapoints. </p><p><b>Why it matters:</b> In production, the model may encounter corrupted or manipulated out of range values. It is important that the model is robust to such extremities.</p><p><b>Configuration:</b> By default, this test runs over all numeric features.</p><p><b>Example:</b> In the reference set, the <span>Age</span> feature has a range of <span>[0, 121]</span>. This test raises a warning if substituting values outside of this range into <span>Age</span> (eg. <span>150, 200</span>) causes model performance to decrease.</p>

### Numeric Outliers Substitution
<p>This test measures the impact on the model when we substitute outliers into clean datapoints. Outliers are values which may not necessarily be outside of an allowed range for a feature, but are extreme values that are unusual and may be indicative of abnormality. </p><p><b>Why it matters:</b> Outliers can be a sign of corrupted or otherwise erroneous data, and can degrade model performance if used in the training data, or lead to unexpected behaviour if input at inference time.</p><p><b>Configuration:</b> By default this test is run over each numeric feature that is neither unique nor ascending.</p><p><b>Example:</b> Suppose there is a feature <span>age</span> for which in the reference set the values <span>103</span> and <span>114</span> each appear once but every other value (with subsantial sample size) is contained within the range <span>[0, 97]</span>. Then we would infer a lower outlier threshold of <span>0</span> and an upper outlier threshold of <span>97</span>. This test raises a warning if substituting outliers into <span>age</span> causes model performance to decrease.</p>

### Empty String Substitution
<p>This test measures the impact on the model when we substitute empty string values instead of null values into clean datapoints. </p><p><b>Why it matters:</b> In production, the model may encounter corrupted or manipulated string values. Null values and empty strings are often expected to be treated the same, but the model might not treat them that way. It is important that the model is robust to such extremities.</p><p><b>Configuration:</b> By default, this test runs over all string features with null values.</p><p><b>Example:</b> In the reference set, the <span>Name</span> feature contains nulls. This test raises a warning if substituting empty strings instead of null values into the <span>Name</span> feature causes model performance to decrease.</p>

### Int Feature Type Change
<p>This test measures the impact on the model when we substitute values not of type Integer into features that are inferred to be Integer type from the reference set.</p><p><b>Why it matters:</b> A feature may require a specific type. However, errors in the data pipeline may produce values that are outside the expected type. Failing to catch such errors may lead to errors or undefined behavior from the model.</p><p><b>Configuration:</b> By default, this test runs over all features that are inferred to be type Integer.</p><p><b>Example:</b> Say that the feature <span>X</span> requires the Integer type. This test raises a warning if changing values in <span>X</span> to a different type causes model performance to decrease.</p>

### Float Feature Type Change
<p>This test measures the impact on the model when we substitute values not of type Float into features that are inferred to be Float type from the reference set.</p><p><b>Why it matters:</b> A feature may require a specific type. However, errors in the data pipeline may produce values that are outside the expected type. Failing to catch such errors may lead to errors or undefined behavior from the model.</p><p><b>Configuration:</b> By default, this test runs over all features that are inferred to be type Float.</p><p><b>Example:</b> Say that the feature <span>X</span> requires the Float type. This test raises a warning if changing values in <span>X</span> to a different type causes model performance to decrease.</p>

### String Feature Type Change
<p>This test measures the impact on the model when we substitute values not of type String Categorical into features that are inferred to be String Categorical type from the reference set.</p><p><b>Why it matters:</b> A feature may require a specific type. However, errors in the data pipeline may produce values that are outside the expected type. Failing to catch such errors may lead to errors or undefined behavior from the model.</p><p><b>Configuration:</b> By default, this test runs over all features that are inferred to be type String Categorical.</p><p><b>Example:</b> Say that the feature <span>X</span> requires the String Categorical type. This test raises a warning if changing values in <span>X</span> to a different type causes model performance to decrease.</p>

### Boolean Feature Type Change
<p>This test measures the impact on the model when we substitute values not of type Boolean Categorical into features that are inferred to be Boolean Categorical type from the reference set.</p><p><b>Why it matters:</b> A feature may require a specific type. However, errors in the data pipeline may produce values that are outside the expected type. Failing to catch such errors may lead to errors or undefined behavior from the model.</p><p><b>Configuration:</b> By default, this test runs over all features that are inferred to be type Boolean Categorical.</p><p><b>Example:</b> Say that the feature <span>X</span> requires the Boolean Categorical type. This test raises a warning if changing values in <span>X</span> to a different type causes model performance to decrease.</p>

### URL Feature Type Change
<p>This test measures the impact on the model when we substitute values not of type URL Categorical into features that are inferred to be URL Categorical type from the reference set.</p><p><b>Why it matters:</b> A feature may require a specific type. However, errors in the data pipeline may produce values that are outside the expected type. Failing to catch such errors may lead to errors or undefined behavior from the model.</p><p><b>Configuration:</b> By default, this test runs over all features that are inferred to be type URL Categorical.</p><p><b>Example:</b> Say that the feature <span>X</span> requires the URL Categorical type. This test raises a warning if changing values in <span>X</span> to a different type causes model performance to decrease.</p>

### Domain Feature Type Change
<p>This test measures the impact on the model when we substitute values not of type Domain Categorical into features that are inferred to be Domain Categorical type from the reference set.</p><p><b>Why it matters:</b> A feature may require a specific type. However, errors in the data pipeline may produce values that are outside the expected type. Failing to catch such errors may lead to errors or undefined behavior from the model.</p><p><b>Configuration:</b> By default, this test runs over all features that are inferred to be type Domain Categorical.</p><p><b>Example:</b> Say that the feature <span>X</span> requires the Domain Categorical type. This test raises a warning if changing values in <span>X</span> to a different type causes model performance to decrease.</p>

### Email Feature Type Change
<p>This test measures the impact on the model when we substitute values not of type Email Categorical into features that are inferred to be Email Categorical type from the reference set.</p><p><b>Why it matters:</b> A feature may require a specific type. However, errors in the data pipeline may produce values that are outside the expected type. Failing to catch such errors may lead to errors or undefined behavior from the model.</p><p><b>Configuration:</b> By default, this test runs over all features that are inferred to be type Email Categorical.</p><p><b>Example:</b> Say that the feature <span>X</span> requires the Email Categorical type. This test raises a warning if changing values in <span>X</span> to a different type causes model performance to decrease.</p>

### Capitalization Change
<p>This test measures the impact on the model when we substitute different types of capitalization into clean datapoints. </p><p><b>Why it matters:</b> In production, models can come across the same value with different capitalizations, making it important to explicitly check that your model is invariant to such differences.</p><p><b>Configuration:</b> By default, this test runs over all categorical features.</p><p><b>Example:</b> Suppose we had a column that corresponded to country code. For a specific row, let's say the observed value in the reference set was <span>USA</span>. This test raises a warning if substituting different capitalizations of <span>USA</span>, eg.<span>usa</span>, causes model performance to decrease.</p>

### Required Characters Deletion
<p>This test measures the impact on the model when we delete required characters, inferred from the reference set, from the strings of clean datapoints.</p><p><b>Why it matters:</b> A feature may require specific characters. However, errors in the data pipeline may allow invalid data points that lack these required characters to pass. Failing to catch such errors may lead to noisier training data or noisier predictions during inference, which can degrade model metrics.</p><p><b>Configuration:</b> By default, this test runs over all string features that are inferred to have required characters.</p><p><b>Example:</b> Say that the feature <span>email</span> requires the character <span>@</span>. This test raises a warning if removing <span>@</span> from values in <span>email</span> causes model performance to decrease</p>

### Unseen Categorical Substitution
<p>This test measures the impact on the model when we substitute unseen categorical values into clean datapoints. </p><p><b>Why it matters:</b> Unseen categorical values are a common failure point in machine learning systems; since these models are trained over a reference set, they may yield uninterpretable or undefined behavior when interacting with an unseen categorical value. In addition, such errors may expose gaps or errors in data collection.</p><p><b>Configuration:</b> By default, this test runs over all categorical features.</p><p><b>Example:</b> Say that the feature <span>Animal</span> contains the values <span>['Cat', 'Dog']</span> from the reference set. This test raises a warning if substituting unseen values into the feature <span>Animal</span> causes model performance to decrease.</p>

### Null Substitution
<p>This test measures the impact on the model when we substitute nulls in features that should not have nulls into clean datapoints. </p><p><b>Why it matters:</b> The model may make certain assumptions about a column depending on whether or not it had nulls in the training data. If these assumptions break during production, this may damage the model's performance. For example, if a column was never null during training then a model may not have learned to be robust against noise in that column. </p><p><b>Configuration:</b> By default, this test runs over all columns that had zero nulls in the reference set. </p><p><b>Example:</b> Suppose that the feature <span>Age</span> was never null in the reference set. This test raises a warning if substituting nulls into the <span>Age</span> feature causes model performance to decrease. </p>

### Unseen URL Substitution
<p>This test measures the impact on the model when we substitute unseen URL values into clean datapoints. </p><p><b>Why it matters:</b> Unseen categorical values are a common failure point in machine learning systems; since these models are trained over a reference set, they may yield uninterpretable or undefined behavior when interacting with an unseen categorical value. In addition, such errors may expose gaps or errors in data collection.</p><p><b>Configuration:</b> By default, this test runs over all features inferred to contain URLs.</p><p><b>Example:</b> Say that the feature <span>WebURL</span> contains the values <span>['http://google.com', 'http://yahoo.com']</span> from the reference set. This test raises a warning if substituting unseen values into the feature <span>WebURL</span> causes model performance to decrease.</p>

### Unseen Domain Substitution
<p>This test measures the impact on the model when we substitute unseen domain values into clean datapoints. </p><p><b>Why it matters:</b> Unseen categorical values are a common failure point in machine learning systems; since these models are trained over a reference set, they may yield uninterpretable or undefined behavior when interacting with an unseen categorical value. In addition, such errors may expose gaps or errors in data collection.</p><p><b>Configuration:</b> By default, this test runs over all features inferred to contain domains.</p><p><b>Example:</b> Say that the feature <span>WebDomain</span> contains the values <span>['gmail.com', 'hotmail.com']</span> from the reference set. This test raises a warning if substituting unseen values into the feature <span>WebDomain</span> causes model performance to decrease.</p>

### Unseen Email Substitution
<p>This test measures the impact on the model when we substitute unseen email values into clean datapoints. </p><p><b>Why it matters:</b> Unseen categorical values are a common failure point in machine learning systems; since these models are trained over a reference set, they may yield uninterpretable or undefined behavior when interacting with an unseen categorical value. In addition, such errors may expose gaps or errors in data collection.</p><p><b>Configuration:</b> By default, this test runs over all features inferred to contain emails.</p><p><b>Example:</b> Say that the feature <span>Email</span> contains the values <span>['user1@gmail.com', 'user2@yahoo.com']</span> from the reference set. This test raises a warning if substituting unseen values into the feature <span>Email</span> causes model performance to decrease.</p>

## Attacks

### Single-Feature Changes
<p>This test measures the severity of passing to the model data points that have been manipulated across a single feature in an unbounded manner. The severity is a function of the impact of these manipulations on the model.</p><p><b>Why it matters:</b> In production, your model will likely come across inputs that are out-of-distribution with respect to the training data, and it is often difficult to know ahead of time how your model will behave on such inputs. 'Attacking' a model in the manner of this test is a technique for finding the out-of-distribution regions of the input space where your model most severely misbehaves, <i>before</i> putting it into production. Restricting ourselves to changing a single feature at a time is one proxy for what 'realistic' out-of-distribution data can look like.</p><p><b>Configuration:</b> By default, for a given input we aim to change your model's prediction in the opposite direction of the true label. This test raises a warning if the average prediction change that can be achieved exceeds an acceptable threshold.</p><p><b>Example:</b> Suppose your model has an <span>Age</span> feature with observed range <span>0</span> to <span>120</span>. For every row in some sample, this test would search for the value of <span>Age</span> in <span>0</span> to <span>120</span> that caused the maximal change in prediction in the desired direction.</p>

### Bounded Single-Feature Changes
<p>This test measures the severity of passing to the model data points that have been manipulated across a single feature in an bounded manner. The severity is a function of the impact of these manipulations on the model.We bound the manipulations to be less than some fraction of the range of the given feature.</p><p><b>Why it matters:</b> In production, your model will likely come across inputs that are out-of-distribution with respect to the training data, and it is often difficult to know ahead of time how your model will behave on such inputs. 'Attacking' a model in the manner of this test is a technique for finding the out-of-distribution regions of the input space where your model most severely misbehaves, <i>before</i> putting it into production. Restricting ourselves to changing a single feature by a small amount is one proxy for what 'realistic' out-of-distribution data can look like.</p><p><b>Configuration:</b> By default, for a given input we aim to change your model's prediction in the opposite direction of the true label. This test raises a warning if the average prediction change that can be achieved exceeds an acceptable threshold. This test runs only over numeric features.</p><p><b>Example:</b> Suppose your model has an <span>Age</span> feature with observed range <span>0</span> to <span>120</span>, and we restricted ourselves to changes that were no greater than <span>10</span>% of the feature range. For every row in some sample, this test would search for the value of <span>Age</span> that was at most <span>12</span> away from the row's initial <span>Age</span> value and that caused the maximal change in prediction in the desired direction.</p>

### Multi-Feature Changes
<p>This test measures the severity of passing to the model data points that have been manipulated across multiple features in an unbounded manner. The severity is a function of the impact of these manipulations on the model.</p><p><b>Why it matters:</b> In production, your model will likely come across inputs that are out-of-distribution with respect to the training data, and it is often difficult to know ahead of time how your model will behave on such inputs. 'Attacking' a model in the manner of this test is a technique for finding the out-of-distribution regions of the input space where your model most severely misbehaves, <i>before</i> putting it into production. Restricting the number of features that can be changed is one proxy for what 'realistic' out-of-distribution data can look like.</p><p><b>Configuration:</b> By default, for a given input we aim to change your model's prediction in the opposite direction of the true label. This test raises a warning if the average prediction change that can be achieved exceeds an acceptable threshold.</p><p><b>Example:</b> Suppose we restricted ourselves to changing <span>5</span> features. This means for each input we would search for the <span>5</span> feature values change that, when performed together, caused the largest possible change in your model's prediction on that input.</p>

### Bounded Multi-Feature Changes
<p>This test measures the severity of passing to the model data points that have been manipulated across multiple features in an bounded manner. The severity is a function of the impact of these manipulations on the model.We bound the manipulations to be less than some fraction of the range of the given feature.</p><p><b>Why it matters:</b> In production, your model will likely come across inputs that are out-of-distribution with respect to the training data, and it is often difficult to know ahead of time how your model will behave on such inputs. 'Attacking' a model in the manner of this test is a technique for finding the out-of-distribution regions of the input space where your model most severely misbehaves, <i>before</i> putting it into production. Restricting the number of features that can be changed and the magnitude of the change that can be made to each feature is one proxy for what 'realistic' out-of-distribution data can look like.</p><p><b>Configuration:</b> By default, for a given input we aim to change your model's prediction in the opposite direction of the true label. This test raises a warning if the average prediction change that can be achieved exceeds an acceptable threshold. This test runs only over numeric features.</p><p><b>Example:</b> Suppose we restricted ourselves to changing <span>5</span> features, each by no more than <span>10</span>% of the range of the given feature. This means for each input we would search for the <span>5</span> restricted feature values change that, when performed together, caused the largest possible change in your model's prediction on that input.</p>

## Data Cleanliness

### Duplicate Row
<p>This test checks if there are any duplicate rows in your dataset. The key detail displays the number of duplicate rows in your dataset.</p><p><b>Why it matters:</b> Duplicate rows are potentially a sign of a broken data pipeline or an otherwise corrupted input.</p><p><b>Configuration:</b> By default this test is run over all features, meaning two rows are considered duplicates only if they match across all features.</p><p><b>Example:</b> Suppose we had two rows that were the same across every feature except an <span>ID</span> feature. By default these two rows would not be flagged as duplicates. If we exclude the <span>ID</span> feature, then these two rows would be flagged as duplicates.</p>

### Required Features
<p>This test checks that the features of a dataset are as expected.</p><p><b>Why it matters:</b> Errors in data collection and processing can lead to invalid missing (or extra) features. In the case of missing features, this can cause failures in models. In the case of extra features, this can lead to unneccessary storage and computation.</p><p><b>Configuration:</b> This test runs only when required features are specified.</p><p><b>Example:</b> Suppose we had a few features (<span>Age</span>, <span>Location</span>, etc.) that we always expected to be present in the dataset. We can configure this test to check that those columns are there.</p>

### Feature Leakage
<p>Feature leakage occurs when a model is trained on features that include information about the label that is not normally present during production.This tests flags a likely data leakage issue if both of the following occur:<ul><li>the normalized mutual information between the feature and the label is too high in the reference set</li><li>the normalized mutual information for the reference set is much higher than for the evaluation set</li></ul> The first criteria is an indicator that this feature has unreasonably high predictive power for the label during training, and the second criteria checks that this feature is no longer a good predictor in the evaluation set. One requirement for this test to flag data leakage is that the evaluation set labels and features are collected properly.</p><p><b>Why it matters:</b> Errors in data collection and processing can lead to the some features containing information about the label in the reference set that do not appear in the evaluation set. This causes the model to under-perform during production.</p><p><b>Configuration:</b> By default, this test always runs on all categorical features.</p><p><b>Example:</b> Consider a lending model that is trying to predict a boolean variable <span>loan given</span> that reports whether or not a bank will issue this loan to a potential borrower, and suppose one of the features is <span>total debt</span>. An error during the data processing causes the model to be trained on a data set where <span>total debt</span> is calculated after the loan has already been given, resulting in the model predicting <span>loan given</span> to be true whenever <span>total debt</span> is large. However, when the model is deployed, the feature <span>total debt</span> must be calculated before the <span>loan given</span> prediction can be made.<br>The normalized mutual information between these columns might be 0.3 in the reference set but only 0.1 in the evaluation set. This test would then flag a likely feature leakage issue where <span>total debt</span> is leaking into the variable <span>loan given</span> during training.</p>

## Subset Performance

### Subset AUC
<p>This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the Area Under Curve (AUC) of model predictions within a specific subset is significantly lower than the model prediction Area Under Curve (AUC) over the entire 'population'. </p><p><b>Why it matters:</b> Having similar AUC between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. </p><p><b>Configuration:</b> By default, AUC is computed over all predictions/labels. Note that we compute AUC of the Receiver Operating Characteristic (ROC) curve.</p><p><b>Example:</b> Suppose we had data with 2 features: <span>[['cat', 0.2], ['dog', 0.3], ['cat', 0.5], ['dog', 0.7], ['cat', 0.7], ['dog', 0.2]]</span>, model predictions <span>[0.3, 0.51, 0.7, 0.49, 0.9, 0.58]</span>, and labels <span>[1, 0, 1, 0, 0, 1]</span>. Then, the AUC over the feature subset value 'cat' would be 0.0, compared to the overall metric of 0.44.</p>

### Subset Accuracy
<p>This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the accuracy of model predictions within a specific subset is significantly lower than the model prediction accuracy over the entire 'population'. </p><p><b>Why it matters:</b> Having similar accuracy between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. Accuracy can be thought of as a 'weaker' metric of model bias compared to measuring false positive rate (predictive equality) or false negative rate (equal opportunity). This is because we can have similar accuracy between group A and group B; yet group A actually has higher false positive rate, while group B has higher false negative rate (e.g. we reject qualified applicants in group A but accept non-qualified applicants in group B). Nevertheless, accuracy is a standard metric used during evaluation and should be considered as part of performance bias testing.</p><p><b>Configuration:</b> By default, accuracy is computed over all predictions/labels. Note we round predictions to 0/1 to compute accuracy.</p><p><b>Example:</b> Suppose we had data with 2 features: <span>[['cat', 0.2], ['dog', 0.3], ['cat', 0.5], ['dog', 0.7], ['cat', 0.7], ['dog', 0.2]]</span>, model predictions <span>[0.3, 0.51, 0.7, 0.49, 0.9, 0.58]</span>, and labels <span>[1, 0, 1, 0, 0, 1]</span>. Then, the accuracy over the feature subset value 'cat' would be 0.33, compared to the overall metric of 0.5.</p>

### Subset Accuracy
<p>This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the accuracy of model predictions within a specific subset is significantly lower than the model prediction accuracy over the entire 'population'. </p><p><b>Why it matters:</b> Having similar accuracy between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. Accuracy can be thought of as a 'weaker' metric of model bias compared to measuring false positive rate (predictive equality) or false negative rate (equal opportunity). This is because we can have similar accuracy between group A and group B; yet group A actually has higher false positive rate, while group B has higher false negative rate (e.g. we reject qualified applicants in group A but accept non-qualified applicants in group B). Nevertheless, accuracy is a standard metric used during evaluation and should be considered as part of performance bias testing.</p><p><b>Configuration:</b> By default, accuracy is computed over all predictions/labels. Note we round predictions to 0/1 to compute accuracy.</p><p><b>Example:</b> Suppose we had data with 2 features: <span>[['cat', 0.2], ['dog', 0.3], ['cat', 0.5], ['dog', 0.7], ['cat', 0.7], ['dog', 0.2]]</span>, model predictions <span>[0.3, 0.51, 0.7, 0.49, 0.9, 0.58]</span>, and labels <span>[1, 0, 1, 0, 0, 1]</span>. Then, the accuracy over the feature subset value 'cat' would be 0.33, compared to the overall metric of 0.5.</p>

### Subset Multiclass AUC
<p>In the multiclass setting, we compute one vs. one area under the curve (AUC), which computes the AUC between every pairwise combination of classes. This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the Area Under Curve (AUC) of model predictions within a specific subset is significantly lower than the model prediction Area Under Curve (AUC) over the entire 'population'. </p><p><b>Why it matters:</b> Having similar AUC between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. </p><p><b>Configuration:</b> By default, AUC is computed over all predictions/labels. Note that we compute AUC of the Receiver Operating Characteristic (ROC) curve.</p><p><b>Example:</b> Suppose we are differentiating between cats, bears, and dogs. Assume that across the data points where <span>height=2</span> the predictions are <span>[0.9, 0.1, 0], [0.1, 0.9, 0], [0.2, 0.1, 0.7]</span> and the labels are <span>[1, 0, 0], [1, 0, 0], [0, 0, 1]</span> (where the first index corresponds to cat, the second corresponds to bear, and the third corresponds to dog). Then the AUC (one vs. one) across this subset is <span>0.75</span>. If the overall AUC (one vs. one) across all subsets is <span>0.9</span> then this test raises a warning.</p>

### Subset F1
<p>This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the F1 of model predictions within a specific subset is significantly lower than the model prediction F1 over the entire 'population'. </p><p><b>Why it matters:</b> Having similar F1 between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. </p><p><b>Configuration:</b> By default, F1 is computed over all predictions/labels. Note that we round predictions to 0/1 to compute F1 score.</p><p><b>Example:</b> Suppose we had data with 2 features: <span>[['cat', 0.2], ['dog', 0.3], ['cat', 0.5], ['dog', 0.7], ['cat', 0.7], ['dog', 0.2]]</span>, model predictions <span>[0.3, 0.51, 0.7, 0.49, 0.9, 0.58]</span>, and labels <span>[1, 0, 1, 0, 0, 1]</span>. Then, the F1 over the feature subset value 'cat' would be 0.5, compared to the overall metric of 0.57.</p>

### Subset Macro F1
<p>F1 is a holistic measure of both precision and recall. When transitioning to the multiclass setting we can use macro F1 which computes the F1 of each class and averages them. This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the macro F1 of model predictions within a specific subset is significantly lower than the model prediction macro F1 over the entire 'population'. </p><p><b>Why it matters:</b> Having similar macro F1 between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. </p><p><b>Configuration:</b> By default, macro F1 is computed over all predictions/labels. Note that the predicted label is the label with the largest predicted probability.</p><p><b>Example:</b> Suppose we are differentiating between cats, bears, and dogs. Assume that across the data points where <span>height=2</span> the predictions are <span>[0.9, 0.1, 0], [0.1, 0.9, 0], [0.2, 0.1, 0.7]</span> and the labels are <span>[1, 0, 0], [1, 0, 0], [0, 0, 1]</span> (where the first index corresponds to cat, the second corresponds to bear, and the third corresponds to dog). Then the macro F1 across this subset is <span>0.78</span>. If the overall macro F1 across all subsets is <span>0.9</span> then this test raises a warning.</p>

### Subset Precision
<p>The precision test is also popularly referred to positive predictive parity in fairness literature. This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the Precision of model predictions within a specific subset is significantly lower than the model prediction Precision over the entire 'population'. </p><p><b>Why it matters:</b> Having similar precision (e.g. false discovery rates) between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. Unlike demographic parity, this test permits assuming different base label rates but flags differing mistake rates between different subgroups. Note that positive predictive parity does not necessarily indicate equal opportunity or predictive equality: as a hypothetical example, imagine that a loan qualification classifier flags 100 entries for group A and 100 entries for group B, each with a precision of 100%, but there are 100 actual qualified entries in group A and 9000 in group B. This would indicate disparities in opportunities given to each subgroup.</p><p><b>Configuration:</b> By default, Precision is computed over all predictions/labels. Note that we round predictions to 0/1 to compute precision.</p><p><b>Example:</b> Suppose we had data with 2 features: <span>[['cat', 0.2], ['dog', 0.3], ['cat', 0.5], ['dog', 0.7], ['cat', 0.7], ['dog', 0.2]]</span>, model predictions <span>[0.3, 0.51, 0.7, 0.49, 0.9, 0.58]</span>, and labels <span>[1, 0, 1, 0, 0, 1]</span>. Then, the Precision over the feature subset value 'cat' would be 0.5, compared to the overall metric of 0.5.</p>

### Subset Macro Precision
<p>The precision test is also popularly referred to as positive predictive parity in fairness literature. When transitioning to the multiclass setting, we can compute macro precision which computes the precisions of each class individually and then averages them.This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the Macro Precision of model predictions within a specific subset is significantly lower than the model prediction Macro Precision over the entire 'population'. </p><p><b>Why it matters:</b> Having similar macro precision (e.g. false discovery rates) between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. Unlike demographic parity, this test permits assuming different base label rates but flags differing mistake rates between different subgroups. Note that positive predictive parity does not necessarily indicate equal opportunity or predictive equality: as a hypothetical example, imagine that a loan qualification classifier flags 100 entries for group A and 100 entries for group B, each with a precision of 100%, but there are 100 actual qualified entries in group A and 9000 in group B. This would indicate disparities in opportunities given to each subgroup.</p><p><b>Configuration:</b> By default, Macro Precision is computed over all predictions/labels. Note that the predicted label is the label with the greatest predicted probability.</p><p><b>Example:</b> Suppose we are differentiating between cats, bears, and dogs. Assume that across the data points where <span>height=2</span> the predictions are <span>[0.9, 0.1, 0], [0.1, 0.9, 0], [0.2, 0.1, 0.7]</span> and the labels are <span>[1, 0, 0], [1, 0, 0], [0, 0, 1]</span> (where the first index corresponds to cat, the second corresponds to bear, and the third corresponds to dog). Then the Macro Precision across this subset is <span>0.67</span>. If the overall Macro Precision across all subsets is <span>0.9</span> then this test raises a warning.</p>

### Subset False Positive Rate
<p>The false positive error rate test is also popularly referred to as as predictive equality in fairness literature. This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the false positive rate of model predictions within a specific subset is significantly higher than the model prediction false positive rate over the entire 'population'. </p><p><b>Why it matters:</b> Having similar false positive rates (e.g. predictive equality) between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. Unlike demographic parity, this test permits assuming different base label rates but flags differing mistake rates between different subgroups. As an intuitive example, consider the case when the label indicates an undesirable attribute: if predicting whether a person will default on their loan, make sure that for people who didn't default, the rate at which the model incorrectly predicts positive is similar for group A and B. </p><p><b>Configuration:</b> By default, false positive rate is computed over all predictions/labels. Note that we round predictions to 0/1 to compute false positive rate.</p><p><b>Example:</b> Suppose we had data with 2 features: <span>[['cat', 0.2], ['dog', 0.3], ['cat', 0.5], ['dog', 0.7], ['cat', 0.7], ['dog', 0.2]]</span>, model predictions <span>[0.3, 0.51, 0.7, 0.49, 0.9, 0.58]</span>, and labels <span>[1, 0, 1, 0, 0, 1]</span>. Then, the false positive rate over the feature subset value 'cat' would be 1.0, compared to the overall metric of 0.67.</p>

### Subset Recall
<p>The recall test is more popularly referred to as equal opportunity or false negative error rate balance in fairness literature. This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the Recall of model predictions within a specific subset is significantly lower than the model prediction Recall over the entire 'population'. </p><p><b>Why it matters:</b> Having similar true positive rates (e.g. equal opportunity) between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. Unlike demographic parity, this test permits assuming different base label rates but flags differing mistake rates between different subgroups. An intuitive example is when the label indicates a positive attribute: if predicting whether to interview a given candidate, make sure that out of qualified candidates, the rate at which the model predicts a rejection is similar to group A and B. </p><p><b>Configuration:</b> By default, Recall is computed over all predictions/labels. Note that we round predictions to 0/1 to compute recall.</p><p><b>Example:</b> Suppose we had data with 2 features: <span>[['cat', 0.2], ['dog', 0.3], ['cat', 0.5], ['dog', 0.7], ['cat', 0.7], ['dog', 0.2]]</span>, model predictions <span>[0.3, 0.51, 0.7, 0.49, 0.9, 0.58]</span>, and labels <span>[1, 0, 1, 0, 0, 1]</span>. Then, the Recall over the feature subset value 'cat' would be 0.5, compared to the overall metric of 0.66.</p>

### Subset Macro Recall
<p>The recall test is more popularly referred to as equal opportunity or false negative error rate balance in fairness literature. When transitioning to the multiclass setting we can use macro recall which computes the recall of each individual class and then averages these numbers.This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the Macro Recall of model predictions within a specific subset is significantly lower than the model prediction Macro Recall over the entire 'population'. </p><p><b>Why it matters:</b> Having similar true positive rates (e.g. equal opportunity) between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. Unlike demographic parity, this test permits assuming different base label rates but flags differing mistake rates between different subgroups. An intuitive example is when the label indicates a positive attribute: if predicting whether to interview a given candidate, make sure that out of qualified candidates, the rate at which the model predicts an interview is similar to group A and B. </p><p><b>Configuration:</b> By default, Macro Recall is computed over all predictions/labels. Note that the predicted label is the label with the largest predicted class probability.</p><p><b>Example:</b> Suppose we are differentiating between cats, bears, and dogs. Assume that across the data points where <span>height=2</span> the predictions are <span>[0.9, 0.1, 0], [0.1, 0.9, 0], [0.2, 0.1, 0.7]</span> and the labels are <span>[1, 0, 0], [1, 0, 0], [0, 0, 1]</span> (where the first index corresponds to cat, the second corresponds to bear, and the third corresponds to dog). Then the Macro Recall across this subset is <span>0.67</span>. If the overall Macro Recall across all subsets is <span>0.9</span> then this test raises a warning.</p>

### Subset Prediction Variance (Positive Labels)
<p>The subset variance test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the variance of model predictions within a specific subset is significantly higher than model prediction variance of the entire 'population'. In this test, the population refers to all data with positive ground-truth labels.</p><p><b>Why it matters:</b> High variance within a feature subset compared to the overall population could mean a few different things, and should be analyzed with other subset performance tests (accuracy, AUC) for a more clear view. In the variance metric over positive/negative labels, this could mean the model is much more uncertain about the given subset. When paired with a decrease in AUC, this implies the model underperforms on this subset.</p><p><b>Configuration:</b> By default, the variance is computed over all predictions with a positive ground-truth label.</p><p><b>Example:</b> Suppose we had data with 2 features: [['cat', 0.2], ['dog', 0.3], ['cat', 0.5], ['dog', 0.7], ['cat', 0.7], ['dog', 0.2]] and model predictions [0.3, 0.51, 0.7, 0.49, 0.9, 0.48]. Assume the labels are [1, 0, 1, 0, 0, 0].Then the prediction variance for feature column 1, subset 'cat' with positive labels would be 0.04.</p>

### Subset Prediction Variance (Negative Labels)
<p>The subset variance test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the variance of model predictions within a specific subset is significantly higher than model prediction variance of the entire 'population'. In this test, the population refers to all data with negative ground-truth labels.</p><p><b>Why it matters:</b> High variance within a feature subset compared to the overall population could mean a few different things, and should be analyzed with other subset performance tests (accuracy, AUC) for a more clear view. In the variance metric over positive/negative labels, this could mean the model is much more uncertain about the given subset. When paired with a decrease in AUC, this implies the model underperforms on this subset.</p><p><b>Configuration:</b> By default, the variance is computed over all predictions with a negative ground-truth label.</p><p><b>Example:</b> Suppose we had data with 2 features: [['cat', 0.2], ['dog', 0.3], ['cat', 0.5], ['dog', 0.7], ['cat', 0.7], ['dog', 0.2]] and model predictions [0.3, 0.51, 0.7, 0.49, 0.9, 0.48]. Assume the labels are [1, 0, 1, 0, 0, 0].Then the prediction variance for feature column 1, subset 'cat' with negative labels would be 0.</p>

### Subset Mean-Absolute Error (MAE)
<p>This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the MAE of model predictions within a specific subset is significantly higher than the model prediction MAE over the entire 'population'. </p><p><b>Why it matters:</b> Having similar mean-absolute error between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. </p><p><b>Configuration:</b> By default, mean-absolute error is computed over all predictions/labels. </p><p><b>Example:</b> Suppose we had data with 2 features: <span>[[0.4, 0.2], [0.5, 0.3], [0.7, 0.5], [0.6, 0.7], [0.8, 0.7]]</span>, model predictions <span>[0.3, 0.4, 0.8, 0.8, 0.9]</span>, and labels <span>[0.5, 0.5, 1.5, 1.5, 1.5]</span>. Then, the Mean-absolute error over the feature subset (0.0, 0.5] for the first feature would be 0.15, compared to the overall metric of 0.46.</p>

### Subset Root-Mean-Square Error (RMSE)
<p>This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the RMSE of model predictions within a specific subset is significantly higher than the model prediction RMSE over the entire 'population'. </p><p><b>Why it matters:</b> Having similar RMSE between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. </p><p><b>Configuration:</b> By default, RMSE is computed over all predictions/labels. </p><p><b>Example:</b> Suppose we had data with 2 features: <span>[[0.4, 0.2], [0.5, 0.3], [0.7, 0.5], [0.6, 0.7], [0.8, 0.7]]</span>, model predictions <span>[0.3, 0.4, 0.8, 0.8, 0.9]</span>, and labels <span>[0.5, 0.5, 1.5, 1.5, 1.5]</span>. Then, the RMSE over the feature subset (0.0, 0.5] for the first feature would be 0.158, compared to the overall metric of 0.527.</p>

### Subset Prediction Variance
<p>The subset variance test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the variance of model predictions within a specific subset is significantly higher than model prediction variance of the entire 'population'. In this test, the population refers to all data with both positive/negative ground-truth labels.</p><p><b>Why it matters:</b> High variance within a feature subset compared to the overall population could mean a few different things, and should be analyzed with other subset performance tests (accuracy, AUC) for a more clear view. In this variance metric over all labels, it could mean the label variance itself is higher within a subgroup. It could mean the model is much more uncertain about the given subset (especially when paired with a decrease in AUC). On the other hand it could mean the model has gained predictive power on the subset (imagine the model outputting accurate predictions close to 0 and 1 within the subset, and 0.5 everywhere else). </p><p><b>Configuration:</b> By default, the variance is computed over all predictions across all ground-truth labels.</p><p><b>Example:</b> Suppose we had data with 2 features: [['cat', 0.2], ['dog', 0.3], ['cat', 0.5], ['dog', 0.7], ['cat', 0.7], ['dog', 0.2]] and model predictions [0.3, 0.51, 0.7, 0.49, 0.9, 0.48]. Then the prediction variance for feature column 1, subset 'cat' would be 0.062.</p>

### Subset Rank Correlation
<p>This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the rank correlation of model predictions within a specific subset is significantly lower than the model prediction rank correlation over the entire 'population'. </p><p><b>Why it matters:</b> Having similar rank correlation between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. </p><p><b>Configuration:</b> By default, rank correlation is computed over all predictions/labels. </p><p><b>Example:</b> Suppose we had the following query-document pairs: <span>[[(qid: 1), 'A'], [(qid: 1), 'A'], [(qid: 2), 'B'], [(qid: 2), 'B']]</span>, model predictions <span>[2, 1, 1, 2]</span>, and true relevance ranks <span>[1,2,1,2]</span>. Then, the rank correlation over the feature subset 'A' would be -1, compared to the overall metric of 0.</p>

### Subset Normalized Discounted Cumulative Gain (NDCG)
<p>This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the NDCG of model predictions within a specific subset is significantly lower than the model prediction NDCG over the entire 'population'. </p><p><b>Why it matters:</b> Having similar NDCG between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. </p><p><b>Configuration:</b> By default, NDCG is computed over all predictions/labels. </p><p><b>Example:</b> Suppose we had the following query-document pairs: <span>[[(qid: 1), 'A'], [(qid: 1), 'A'], [(qid: 2), 'B'], [(qid: 2), 'B']]</span>, model predictions <span>[2, 1, 1, 2]</span>, and true relevance ranks <span>[1,2,1,2]</span>. Then, the NDCG over the feature subset 'A' would be 0.86, compared to the overall metric of 0.93.</p>

### Subset Mean Reciprocal Rank (MRR)
<p>This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the MRR of model predictions within a specific subset is significantly lower than the model prediction MRR over the entire 'population'. </p><p><b>Why it matters:</b> Having similar MRR between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. </p><p><b>Configuration:</b> By default, MRR is computed over all predictions/labels. </p><p><b>Example:</b> Suppose we had the following query-document pairs: <span>[[(qid: 1), 'A'], [(qid: 1), 'A'], [(qid: 2), 'B'], [(qid: 2), 'B']]</span>, model predictions <span>[2, 1, 1, 2]</span>, and true relevance ranks <span>[1,2,1,2]</span>. Then, the MRR over the feature subset 'A' would be 0.5, compared to the overall metric of 0.75.</p>

### Subset Precision
<p>This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the Precision of model predictions within a specific subset is significantly lower than the model prediction Precision over the entire 'population'. </p><p><b>Why it matters:</b> Having similar Precision between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. </p><p><b>Configuration:</b> By default, Precision is computed over all predictions/labels. </p><p><b>Example:</b> Suppose in our subset the ground truth has the following: <span>[Microsoft Corp.] CEO [Steve Ballmer] announced the release of [Windows 7] today</span> Suppose your actual extraction has the following: <span>[Microsoft Corp.] [CEO] [Steve] Ballmer announced the release of Windows 7 [today]</span> This has 1 true positive (<span>[Microsoft Corp.]</span>), 2 false negatives (<span>[Steve Ballmer], [Windows 7]</span>), and 3 false positives (<span>[Steve], [CEO], [today]</span>). This leads to a Precision of 0.25 on this subset of data. We then compare that to the overall Precision on the full dataset.</p>

### Subset Recall
<p>This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the Recall of model predictions within a specific subset is significantly lower than the model prediction Recall over the entire 'population'. </p><p><b>Why it matters:</b> Having similar Recall between different subgroups is an important indicator of performance bias; in general, bias is an important phenomenon in machine learning and not only contains implications for fairness and ethics, but also indicates failures in adequate feature representation and spurious correlation. </p><p><b>Configuration:</b> By default, Recall is computed over all predictions/labels. </p><p><b>Example:</b> Suppose in our subset the ground truth has the following: <span>[Microsoft Corp.] CEO [Steve Ballmer] announced the release of [Windows 7] today</span> Suppose your actual extraction has the following: <span>[Microsoft Corp.] [CEO] [Steve] Ballmer announced the release of Windows 7 [today]</span> This has 1 true positive (<span>[Microsoft Corp.]</span>), 2 false negatives (<span>[Steve Ballmer], [Windows 7]</span>), and 3 false positives (<span>[Steve], [CEO], [today]</span>). This leads to a Recall of 0.33 on this subset of data. We then compare that to the overall Recall on the full dataset.</p>

