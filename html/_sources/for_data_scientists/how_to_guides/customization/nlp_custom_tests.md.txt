# NLP Generic Custom Tests

{{ custom_tests_intro }}

This Python file must expose a class named `CustomBatchRunner` that inherits from the
`TestBatchRunner` interface specified by RIME.

Implement the following methods in the `CustomBatchRunner` class.

* `_from_config`: Takes a `RunContainer` object and a configuration. Returns an 
    initialized instance of `CustomBatchRunner` with a list of `TestCase` objects. 
    `TestCase` objects are discussed later in this section.
* `table_columns_to_show`: Specifies which columns to show in the test case table in the
    web UI.
* `description`: A short description of the custom test to display in the web UI.
* `long_description`: A long description of the custom test to display in the web UI.
* `type`: Specifies the type of test. Only used for the web UI.

`TestCase` objects are collections of test cases that are aggregated inside the overall 
batch runner. For example, a specific test that applies to each column in the dataset 
would initialize `CustomBatchRunner` with one test for each feature.

The test cases must inherit from the `BaseTest` class specified by RIME.
Test cases must implement the following method.

* `run`: Takes a `UnstructuredRunContainer` object and returns a `TestOutput` object 
    that contains the result of the test, along with a dictionary of additional 
    information used to aggregate test results in the associated `CustomBatchRunner`
    object.

## Example custom test

```python
"""Test batch runner for custom tests."""

from typing import List, Tuple

from rime.core.schema import (
    CustomConfig,
    ImportanceLevel,
    Status,
    TableColumn,
    TestCategory,
    TestOutput,
)
from rime.core.test import BaseTest
from rime.nlp.model_tests.batch_runner import TestBatchRunner
from rime.nlp.run_container import RunContainer


class CustomTest(BaseTest):
    def __init__(self, delta: int = 0):
        """Initialize with a delta between n_rows ref and eval."""
        super().__init__()
        self.delta = delta

    def run(self, run_container: RunContainer, silent_errors: bool = False) -> Tuple[TestOutput, dict]:
        ref_count = len(run_container.ref_data_container.data)
        eval_count = len(run_container.eval_data_container.data)
        if ref_count > eval_count + self.delta:
            status = Status.WARNING
            severity = ImportanceLevel.HIGH
        else:
            status = Status.PASS
            severity = ImportanceLevel.NONE
        table_info_dict = {
            "Severity": severity,
            "Info": "Compare size of datasets",
            "Key Detail": f"Size of reference data: {ref_count}",
        }
        test_output = TestOutput(self.id, status, table_info_dict, severity, [],)
        return test_output, {}


class CustomBatchRunner(TestBatchRunner):
    """TestBatchRunner for the CustomTest."""

    @classmethod
    def _from_config(
        cls, run_container: RunContainer, config: CustomConfig
    ) -> "CustomBatchRunner":
        if config.params is None:
            delta = 0
        else:
            delta = config.params["delta"]
        return cls([CustomTest(delta=delta)])

    @property
    def table_columns_to_show(self) -> List[TableColumn]:
        """Return the types of info to show in the test case table."""
        return [TableColumn(name) for name in ["Info", "Severity", "Key Detail"]]

    @property
    def description(self) -> str:
        return "This is custom test"

    @property
    def long_description(self) -> str:
        return "This is a long description of a custom test."

    @property
    def type(self) -> str:
        return "customer custom test"

```