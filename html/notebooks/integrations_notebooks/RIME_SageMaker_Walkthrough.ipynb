{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50c7e8c0",
   "metadata": {},
   "source": [
    "# Robust Intelligence SageMaker Integration Walkthrough\n",
    "\n",
    "You are a data scientist at a Payment Processing Company. The data science team has been tasked with implementing a Fraud Detection service and monitoring how that model performs over time. The performance of this fraud detection model directly impacts the costs of the company. In order to ensure the data science team develops the best model and the performance of this model doesn't degrade over time, the VP of Data Science purchases the RIME platform.\n",
    "\n",
    "Your team currently does all model development and serving on SageMaker's intelligent cloud and already uses all of its MLOps tooling.\n",
    "    \n",
    "In this Notebook Walkthrough, we will walkthrough 2 of RIME's core products - **AI Stress Testing** and **AI Firewall** and demonstrate how they integrate with SageMaker's core offerings to help you develop and maintain more robust AI models.\n",
    "\n",
    "1. **AI Stress Testing** is used in the model development stage. Using AI Stress Testing you can test the developed model. RIME goes beyond simply optimizing for basic model performance like accuracy and automatically discovers the model's weaknesses.\n",
    "2. **AI Firewall** is used after the model is deployed in production. Using AI Firewall, you can automate the monitoring, discovery and remediation of issues that occur post-deployment. Additionally it automatically flags, blocks, or imputes erroneous data in real-time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcc69a7",
   "metadata": {},
   "source": [
    "> Latest Colab version of this notebook available [here](https://colab.research.google.com/github/RobustIntelligence/docs/blob/main/notebooks/demo_notebooks/RIME_SageMaker_Walkthrough.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fad7d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install rime-sdk &> /dev/null\n",
    "%pip install catboost\n",
    "%pip install ipython &> /dev/null\n",
    "%pip install https://github.com/RobustIntelligence/ri-public-examples/archive/master.zip &> /dev/null\n",
    "%mkdir -p model_training\n",
    "%mkdir -p trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d77187b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install dependencies and download example data\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "import tarfile\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "from time import gmtime, strftime\n",
    "from typing import Any, List, Optional, Tuple, Union\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from ri_public_examples.download_files import download_files\n",
    "from sagemaker import ModelPackage, get_execution_role, image_uris\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.session import ClientError, Session\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score,\n",
    "                             recall_score, roc_auc_score)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from tqdm.notebook import tqdm\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='sklearn')\n",
    "\n",
    "sagemaker_session = Session()\n",
    "sm_client = sagemaker_session.boto_session.client(\"sagemaker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3ac1e6",
   "metadata": {},
   "source": [
    "## Define S3 Helper Functions and Prepare Data\n",
    "\n",
    "Before developing our model, we'll define a few utility functions to help download and prepare the example dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e87bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 / File Utilities\n",
    "def s3_file_exists(file: Union[str, Path], bucket: Optional[str] = None) -> bool:\n",
    "    bucket = bucket or sagemaker_session.default_bucket()\n",
    "    s3 = boto3.resource('s3') \n",
    "    try:\n",
    "        s3.Object(bucket, str(file)).load()\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response['Error']['Code'] == \"404\":\n",
    "            # The object does not exist.\n",
    "            return False\n",
    "        else:\n",
    "            # Something else has gone wrong.\n",
    "            raise\n",
    "    return True\n",
    "\n",
    "def get_s3_path(relative_path: Union[str, Path], bucket: Optional[str] = None) -> str:\n",
    "    \"\"\"Return the s3 bucket of a file.\"\"\"\n",
    "    bucket = bucket or sagemaker_session.default_bucket()\n",
    "    bucket_uri = f\"s3://{bucket}/\" + str(relative_path)\n",
    "    return bucket_uri\n",
    "\n",
    "def populate_s3_file(source_uri: str, file: Union[str, Path], bucket: Optional[str] = None) -> bool:\n",
    "    \"\"\"Populate the s3 path.\"\"\"\n",
    "    bucket_uri = get_s3_path(file, bucket)\n",
    "    pd.read_csv(source_uri).to_csv(bucket_uri, index=False)\n",
    "\n",
    "        \n",
    "def download_s3_file(object_name: str, output_path: str, bucket: Optional[str] = None) -> bool:\n",
    "    \"\"\"Upload a local directory.\"\"\"\n",
    "    bucket = bucket or sagemaker_session.default_bucket()\n",
    "    s3 = boto3.resource('s3') \n",
    "    bucket_obj = s3.Bucket(bucket)\n",
    "    bucket_obj.download_file(object_name, output_path)\n",
    "\n",
    "def make_tarfile(output_filename: str, source_dir: str):\n",
    "    \"\"\"Create a tarfile of the specified source dir.\"\"\"\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        tar.add(source_dir, arcname=Path(source_dir).parent.name)\n",
    "\n",
    "\n",
    "def create_tarball_then_upload_to_s3(local_dir: str, target_path: str, bucket: Optional[str] = None) -> bool:\n",
    "    \"\"\"Convert local directory to a *.tar.gz ball then upload to the specified bucket and target path.\"\"\"\n",
    "    bucket = bucket or sagemaker_session.default_bucket()\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket_obj = s3.Bucket(bucket) \n",
    "    with TemporaryDirectory() as tmp_dir:\n",
    "        make_tarfile(os.path.join(tmp_dir, \"code.tar.gz\"), local_dir)\n",
    "        bucket_obj.upload_file(os.path.join(tmp_dir, \"code.tar.gz\"), str(Path(target_path) / \"code.tar.gz\"))\n",
    "\n",
    "        \n",
    "# Download the example data, then upload to S3 for training + evaluation\n",
    "data_dir = Path(\"fraud_data\")\n",
    "path_dict = {\n",
    "    \"ref\": data_dir / \"ref.csv\",\n",
    "    \"eval\": data_dir / \"eval.csv\",\n",
    "    \"incremental\": data_dir / \"incremental.csv\",\n",
    "}\n",
    "download_files(\"tabular/fraud\", \"fraud_data\")\n",
    "for ds_name, path in path_dict.items():\n",
    "    if not s3_file_exists(path):\n",
    "        populate_s3_file(\n",
    "            f\"fraud_data/data/fraud_{ds_name}.csv\", path\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784e2f4e",
   "metadata": {},
   "source": [
    "## Create SageMaker Training Files\n",
    "\n",
    "SageMaker [Estimators](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) require a training script. In our case, the image requires a couple additional dependencies, which will be uploaded in the tarball in a `requirements.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e329c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model_training/requirements.txt\n",
    "catboost\n",
    "sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d6b272",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model_training/train.py\n",
    "\"\"\"Training and serving file.\"\"\"\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from typing import Any, List, Tuple\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score,\n",
    "                             recall_score, roc_auc_score)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "\n",
    "def extract_feature_types(\n",
    "    df: pd.DataFrame, label_col: str\n",
    ") -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Extract categorical and continuous features (if not provided).\"\"\"\n",
    "    dtypes = df.dtypes\n",
    "    dtypes = dtypes[~dtypes.index.isin([label_col])]\n",
    "    cat_features = list(dtypes[dtypes == object].index)\n",
    "    cont_features = list(dtypes[dtypes != object].index)\n",
    "    return cat_features, cont_features\n",
    "\n",
    "def preprocess_df(\n",
    "    df: pd.DataFrame, cat_features: List[str], cont_features: List[str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Apply string preprocessing to categorical features.\"\"\"\n",
    "    cat_df = df[cat_features].astype(str)\n",
    "    cont_df = df[cont_features]\n",
    "    return pd.concat([cat_df, cont_df], axis=1)\n",
    "\n",
    "\n",
    "def _get_eval_metrics(actual: np.ndarray, pred: np.ndarray):\n",
    "    \"\"\"Metrics to for train and test to report in our training loop.\"\"\"\n",
    "    acc = accuracy_score(actual, pred)\n",
    "    f1 = f1_score(actual, pred)\n",
    "    prec = precision_score(actual, pred)\n",
    "    recall = recall_score(actual, pred)\n",
    "    auc = roc_auc_score(actual, pred)\n",
    "    return acc, f1, prec, recall, auc\n",
    "\n",
    "   \n",
    "def _get_pipeline(net_type: str, cat_features: List[str]) -> Pipeline:\n",
    "    \"\"\"Return pipeline for specified net_type.\"\"\"\n",
    "    if net_type == \"catboost-classifier\":\n",
    "        pipeline = Pipeline(\n",
    "            [(\"clf\", CatBoostClassifier(cat_features=cat_features, verbose=False))]\n",
    "        )\n",
    "    elif net_type == \"sgd-classifier\":\n",
    "        pipeline = Pipeline(\n",
    "            [\n",
    "                (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"ohe\", OneHotEncoder(sparse=False, handle_unknown=\"ignore\")),\n",
    "                (\"normalizer\", StandardScaler()),\n",
    "                (\"clf\", SGDClassifier(loss=\"modified_huber\")),\n",
    "            ]\n",
    "        )\n",
    "    elif net_type == \"logistic-regression-classifier\":\n",
    "        pipeline = Pipeline(\n",
    "            [\n",
    "                (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"ohe\", OneHotEncoder(sparse=False, handle_unknown=\"ignore\")),\n",
    "                (\"normalizer\", StandardScaler()),\n",
    "                (\"clf\", LogisticRegression()),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported net_type {net_type}\")\n",
    "    return pipeline\n",
    "\n",
    "def train_net(\n",
    "    net_type: str,\n",
    "    train_x: pd.DataFrame,\n",
    "    train_y: pd.Series,\n",
    "    test_x: pd.DataFrame,\n",
    "    test_y: pd.Series,\n",
    "    label_col: str,\n",
    "    **kwargs: Any,\n",
    ") -> Pipeline:\n",
    "    \"\"\"Train specified pipeline.\"\"\"\n",
    "    cat_features, cont_features = extract_feature_types(train_x, label_col)\n",
    "    pipeline = _get_pipeline(net_type, cat_features) \n",
    "    _train_x = preprocess_df(train_x, cat_features, cont_features)\n",
    "    pipeline = pipeline.fit(_train_x, train_y.values.flatten())\n",
    "    _test_x = preprocess_df(test_x, cat_features, cont_features)\n",
    "    preds = pipeline.predict(_test_x)\n",
    "    (acc, f1, prec, recall, auc) = _get_eval_metrics(test_y, preds)\n",
    "    print(f\"Performance metrics for net_type={net_type}: acc={acc}, f1={f1}, prec={prec}, recall={recall}, auc={auc}\")\n",
    "    return pipeline\n",
    "\n",
    "LABEL_COL = \"label\"\n",
    "PRED_COL = \"pred\"\n",
    "TIMESTAMP_COL = \"timestamp\"\n",
    "if __name__ == \"__main__\":\n",
    "    # Pass in environment variables and hyperparameters\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--net-type\", type=str, default=os.environ.get(\"SM_HP_NET_TYPE\"))\n",
    "    parser.add_argument(\"--sm-model-dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "    parser.add_argument(\"--train\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAINING\"))\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    training_dir = args.train\n",
    "    # Read in data\n",
    "    train = pd.read_csv(training_dir + \"/train.csv\").drop(columns=[\"preds\"])\n",
    "    test = pd.read_csv(training_dir + \"/test.csv\").drop(columns=[\"preds\"])\n",
    "\n",
    "    # Get the data to prepare for training\n",
    "    train_x = train.drop([LABEL_COL], axis=1)\n",
    "    test_x = test.drop([LABEL_COL], axis=1)\n",
    "    train_y = train[[LABEL_COL]]\n",
    "    test_y = test[[LABEL_COL]]\n",
    "    cat_features, cont_features = extract_feature_types(train_x, LABEL_COL)\n",
    "    pipeline =  train_net(args.net_type, train_x, train_y, test_x, test_y, LABEL_COL)\n",
    "\n",
    "    # Save model\n",
    "    joblib.dump(pipeline, os.path.join(args.sm_model_dir, \"model.joblib\"))\n",
    "    with open(os.path.join(args.sm_model_dir, \"cat_features.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(cat_features, f)\n",
    "    with open(os.path.join(args.sm_model_dir, \"cont_features.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(cont_features, f)\n",
    "\n",
    "\n",
    "# Model serving\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Deserialize fitted model\n",
    "    \"\"\"\n",
    "    model = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    cat_features = pickle.load(open(os.path.join(model_dir, \"cat_features.pkl\"), \"rb\"))\n",
    "    cont_features = pickle.load(open(os.path.join(model_dir, \"cont_features.pkl\"), \"rb\"))\n",
    "    return model, cat_features, cont_features\n",
    "\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"\n",
    "    input_fn\n",
    "        request_body: The body of the request sent to the model.\n",
    "        request_content_type: (string) specifies the format/variable type of the request\n",
    "    \"\"\"\n",
    "\n",
    "    if request_content_type == \"application/json\":\n",
    "        request_body = json.loads(request_body)\n",
    "        inpVar = request_body[\"Input\"]\n",
    "        return inpVar\n",
    "    else:\n",
    "        raise ValueError(\"This model only supports application/json input\")\n",
    "\n",
    "\n",
    "def predict_fn(input_data, model_tuple) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    predict_fn\n",
    "        input_data: returned array from input_fn above\n",
    "        model (sklearn model) returned model loaded from model_fn above\n",
    "    \"\"\"\n",
    "    model, cat_features, cont_features = model_tuple\n",
    "    df = preprocess_df(input_data, cat_features, cont_features)\n",
    "    # Index for binary classification\n",
    "    return model.predict_proba(df)[:, 1]\n",
    "\n",
    "\n",
    "def output_fn(prediction, content_type):\n",
    "    \"\"\"\n",
    "    output_fn\n",
    "        prediction: the returned value from predict_fn above\n",
    "        content_type: the content type the endpoint expects to be returned. Ex: JSON, string\n",
    "    \"\"\"\n",
    "    respJSON = {\"Output\": list(prediction)}\n",
    "    return respJSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7a89c2",
   "metadata": {},
   "source": [
    "## Define SageMaker Training and Model Registry Helpers\n",
    "\n",
    "The following functions are unrelated to RIME. They specify how the SageMaker training jobs will be launched and provide functionality to register the resultant models in the model registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbf82b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_estimator(net_type: str, training_instance_type: str = \"ml.c4.xlarge\") -> Estimator:\n",
    "    \"\"\"Create an estimator of the specified net_type.\"\"\"\n",
    "    role = get_execution_role()\n",
    "    output_location = get_s3_path(f\"sagemaker/models/{net_type}/output\")\n",
    "    \n",
    "    print(f\"Training artifacts will be uploaded to: {output_location}\")\n",
    "    train_model_id, train_model_version, train_scope = \"catboost-classification-model\", \"*\", \"training\"\n",
    "    train_source_uri = get_s3_path(\"model_training_tar/code.tar.gz\")\n",
    "    train_image_uri = image_uris.retrieve(\n",
    "        region=None,\n",
    "        framework=None,\n",
    "        model_id=train_model_id,\n",
    "        model_version=train_model_version,\n",
    "        image_scope=train_scope,\n",
    "        instance_type=training_instance_type,\n",
    "    )\n",
    "    # hyperparameters are passed to the training script as environment variables\n",
    "    hyperparameters = {'NET_TYPE': net_type}\n",
    "    tabular_estimator = Estimator(\n",
    "        role=role,\n",
    "        image_uri=train_image_uri,\n",
    "        source_dir=train_source_uri,\n",
    "        entry_point=\"train.py\",\n",
    "        instance_count=1,\n",
    "        instance_type=training_instance_type,\n",
    "        max_run=360000,\n",
    "        hyperparameters=hyperparameters,\n",
    "        output_path=output_location,\n",
    "        dependencies=['code/requirements.txt']\n",
    "    )\n",
    "    return tabular_estimator\n",
    "\n",
    "\n",
    "def create_model_package_group(model_package_group_name = \"RIME-Fraud-Models\") -> Tuple[str, str]:\n",
    "    \"\"\"Create the model package group.\"\"\"\n",
    "    model_package_group_input_dict = {\n",
    "     \"ModelPackageGroupName\" : model_package_group_name,\n",
    "     \"ModelPackageGroupDescription\" : \"Group if fraud-detection models developed for the RIME-SageMaker example.\"\n",
    "    }\n",
    "    try:\n",
    "        create_model_package_group_response = sm_client.describe_model_package_group(ModelPackageGroupName=\"RIME-Fraud-Models\")\n",
    "    except ClientError:\n",
    "        create_model_package_group_response = sm_client.create_model_package_group(**model_package_group_input_dict)\n",
    "    return create_model_package_group_response['ModelPackageGroupArn'], model_package_group_name\n",
    "    \n",
    "def register_model(net_type: str, model_tar_uri: str, image_uri: str, model_package_group_name: str ):\n",
    "    # Specify the model source\n",
    "    modelpackage_inference_specification =  {\n",
    "        \"InferenceSpecification\": {\n",
    "          \"Containers\": [\n",
    "             {\n",
    "                \"Image\": image_uri,\n",
    "                \"ModelDataUrl\": model_tar_uri\n",
    "             }\n",
    "          ],\n",
    "          \"SupportedContentTypes\": [ \"text/csv\" ],\n",
    "          \"SupportedResponseMIMETypes\": [ \"text/csv\" ],\n",
    "       }\n",
    "     }\n",
    "\n",
    "    create_model_package_input_dict = {\n",
    "        \"ModelPackageGroupName\" : model_package_group_name,\n",
    "        \"ModelPackageDescription\" : f\"Model of type {net_type} to detect fraud\",\n",
    "        \"ModelApprovalStatus\" : \"Approved\",\n",
    "        \"CustomerMetadataProperties\": {\n",
    "            \"net_type\": net_type\n",
    "        }\n",
    "    }\n",
    "    create_model_package_input_dict.update(modelpackage_inference_specification)\n",
    "    create_model_package_response = sm_client.create_model_package(**create_model_package_input_dict)\n",
    "    return create_model_package_response[\"ModelPackageArn\"]\n",
    "\n",
    "def get_model_tar_uri(estimator: Estimator) -> str:\n",
    "    \"\"\"Return the trained model output.\"\"\"\n",
    "    return estimator.output_path + '/' + estimator.latest_training_job.name + '/output/model.tar.gz'\n",
    "\n",
    "def train_net(net_type: str, data_dir: str, logs: bool=True, wait: bool = False) -> Estimator:\n",
    "    \"\"\"Create and fit an estimator.\"\"\"\n",
    "    tabular_estimator = create_estimator(net_type)\n",
    "    training_dataset_s3_path = get_s3_path(data_dir)\n",
    "    timestamp_suffix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "    training_job_name = f\"{net_type}-{timestamp_suffix}\"\n",
    "    tabular_estimator.fit(\n",
    "            {\"training\": training_dataset_s3_path}, logs=logs, wait=wait, job_name=training_job_name\n",
    "        )\n",
    "    return {\n",
    "            'net_type': net_type,\n",
    "            'estimator': tabular_estimator,\n",
    "        }\n",
    "\n",
    "def attach_to_estimator(estimator_dict: dict) -> dict:\n",
    "    \"\"\"Attach to training job and update dict.\"\"\"\n",
    "    tabular_estimator: Estimator = estimator_dict['estimator']\n",
    "    net_type = estimator_dict['net_type']\n",
    "    tabular_estimator.attach(tabular_estimator.latest_training_job.name)\n",
    "    model_tar_uri = get_model_tar_uri(tabular_estimator)\n",
    "    model_tar_file = model_tar_uri[len(f\"s3://{sagemaker_session.default_bucket()}/\"):]\n",
    "    model_package_group_arn, model_package_group_name = create_model_package_group()\n",
    "    model_package_arn = register_model(net_type, model_tar_uri, tabular_estimator.image_uri, model_package_group_name)\n",
    "    estimator_dict['model_tar_uri'] = model_tar_uri\n",
    "    estimator_dict['model_tar_file'] = model_tar_file\n",
    "    estimator_dict['model_package_group_arn'] = model_package_group_arn\n",
    "    estimator_dict['model_package_arn'] = model_package_arn\n",
    "    return estimator_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b292a7",
   "metadata": {},
   "source": [
    "## Train Models\n",
    "\n",
    "Now that we've uploaded our model and data to S3, we can asynchronously train models to be evaluated by RIME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f7971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the training scripts to be used by estimator\n",
    "create_tarball_then_upload_to_s3('model_training', 'model_training_tar')\n",
    "\n",
    "net_types = [\"catboost-classifier\", \"sgd-classifier\", \"logistic-regression-classifier\"]\n",
    "estimators = []\n",
    "for net_type in net_types:\n",
    "    estimators.append(train_net(net_type, str(data_dir), logs=True, wait=False))  \n",
    "\n",
    "# Wait for training to complete, then update model registry\n",
    "for estimator_dict in estimators:\n",
    "    attach_to_estimator(estimator_dict)\n",
    "\n",
    "# Download trained models\n",
    "for estimator_dict in estimators:\n",
    "    training_job_name = estimator_dict['estimator'].latest_training_job.name\n",
    "    os.makedirs(f\"trained_models/{training_job_name}\", exist_ok=True)\n",
    "    local_model_tar = \"trained_models/\" + training_job_name + '/model.tar.gz'\n",
    "    download_s3_file(estimator_dict['model_tar_file'], local_model_tar)\n",
    "    estimator_dict['local_model_tar'] = local_model_tar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590fb7e0",
   "metadata": {},
   "source": [
    "## Connecting to RIME\n",
    "Now for the fun. To connect to RIME, create an API key and assign to the variable `API_KEY` below. You can generate a new API key within the 'Workspace settings' page on your RIME cluster's website.\n",
    "\n",
    "Additionally, copy the url of the cluster (e.g., '`rime.<cluster-name>.rime.dev`') to the `RIME_CLUSTER_URL` variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b49c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rime_sdk import Client, Firewall, Job\n",
    "\n",
    "RIME_CLUSTER_URL = \"Add Cluster Here\"\n",
    "API_KEY = \"Add API Key Here\"\n",
    "\n",
    "client = Client(RIME_CLUSTER_URL, API_KEY)\n",
    "project = client.create_project(name='SageMaker Demo', description='Creating an e2e RIME Demo using SageMaker.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a85ba38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some constants to load / work with the data\n",
    "# There is no need to alter any of these\n",
    "LABEL_COL = \"label\"\n",
    "PRED_COL = \"pred\"\n",
    "TIMESTAMP_COL = \"timestamp\"\n",
    "\n",
    "# Get the data to prepare for training\n",
    "train = pd.read_csv(get_s3_path(path_dict[\"ref\"])).drop(columns=[\"preds\"])\n",
    "test = pd.read_csv(get_s3_path(path_dict[\"eval\"])).drop(columns=[\"preds\"])\n",
    "train_x = train.drop([LABEL_COL], axis=1)\n",
    "test_x = test.drop([LABEL_COL], axis=1)\n",
    "train_y = train[[LABEL_COL]]\n",
    "test_y = test[[LABEL_COL]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb089cde",
   "metadata": {},
   "source": [
    "## Prepare RIME Stress Testing Helper Functions\n",
    "\n",
    "Below, define the `fraud_model.py` file to be uploaded to the RIME testing cluster. \n",
    "Additionally, create helper functions to help:\n",
    "- Upload the model directory to the cluster\n",
    "- Upload a dataset to the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e7aa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile fraud_model.py\n",
    "\"\"\"This is an example model.py file.\"\"\"\n",
    "import joblib\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "model_dir = Path(__file__).parent / \"model_extras\"\n",
    "if model_dir.exists():\n",
    "    cat_features = pickle.load(open(model_dir / \"cat_features.pkl\", \"rb\"))\n",
    "    cont_features = pickle.load(open(model_dir / \"cont_features.pkl\", \"rb\"))\n",
    "    pipeline = joblib.load(model_dir / \"model.joblib\")\n",
    "else:\n",
    "    raise ValueError(\"model files do not exist in desired format\")\n",
    "\n",
    "\n",
    "def preprocess_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Apply string preprocessing to categorical features.\"\"\"\n",
    "    cat_df = df[cat_features].astype(str)\n",
    "    cont_df = df[cont_features]\n",
    "    return pd.concat([cat_df, cont_df], axis=1)\n",
    "\n",
    "\n",
    "def predict_df(df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"Return predicted probabilities.\"\"\"\n",
    "    df = preprocess_df(df)\n",
    "    # Index for binary classification\n",
    "    return pipeline.predict_proba(df)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ff4383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following utilities help launch stress testing jobs\n",
    "# and log test results\n",
    "\n",
    "def get_model_file_contents() -> str:\n",
    "    \"\"\"Get the example `fraud_model.py` contents.\"\"\"\n",
    "    with open('fraud_model.py', 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "def upload_model_from_tarfile(\n",
    "    local_model_tar: str,\n",
    "    net_type: str,\n",
    "    client: Client,\n",
    "    ) -> str:\n",
    "    \"\"\"Upload trained model to RIME cluster and return the resultant model directory path.\"\"\"\n",
    "    upload_path = f\"sagemaker_demo_{net_type}\"\n",
    "    with TemporaryDirectory() as d:\n",
    "        _dir = Path(d) / net_type        \n",
    "        extras_dir = _dir / \"model_extras\"\n",
    "        extras_dir.mkdir(parents=True)\n",
    "        with tarfile.open(local_model_tar, \"r:gz\") as tar:\n",
    "            tar.extractall(_dir)\n",
    "        for file in _dir.glob(\"**/*\"):\n",
    "            if file.is_file():\n",
    "                file.rename(extras_dir / file.name)\n",
    "        shutil.copyfile(\"fraud_model.py\",  _dir / \"model.py\")\n",
    "        uploaded_path = client.upload_directory(str(_dir), upload_path=upload_path)\n",
    "    return uploaded_path + \"/model.py\"\n",
    "\n",
    "def predict_from_tarfile(\n",
    "    local_model_tar: str,\n",
    "    df: pd.DataFrame\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"Return predictions for model in the downloaded tarfile.\"\"\"\n",
    "    with TemporaryDirectory() as d:\n",
    "        with tarfile.open(local_model_tar, \"r:gz\") as tar:\n",
    "            tar.extractall(d)\n",
    "        model_dir = Path(d) # / \"model\"\n",
    "        with open(model_dir / \"cat_features.pkl\", \"rb\") as f:\n",
    "            cat_features = pickle.load(f)\n",
    "        with open(model_dir / \"cont_features.pkl\", \"rb\") as f:\n",
    "            cont_features = pickle.load(f)\n",
    "        pipeline = joblib.load(model_dir / \"model.joblib\")\n",
    "    cat_df = df[cat_features].astype(str)\n",
    "    cont_df = df[cont_features]\n",
    "    preprocessed_df = pd.concat([cat_df, cont_df], axis=1)\n",
    "    return pipeline.predict_proba(preprocessed_df)[:, 1]\n",
    "\n",
    "\n",
    "def upload_dataset_file(client: Client, df: pd.DataFrame, split: str) -> str:\n",
    "    \"\"\"Upload dataframe to RIME cluster.\"\"\"\n",
    "    upload_path = f\"sagemaker_walkthrough_{split}\"\n",
    "    with TemporaryDirectory() as d:\n",
    "        f = Path(d) / \"data.csv\"\n",
    "        df.to_csv(f, index=False)\n",
    "        uploaded_name = client.upload_file(f.resolve(), upload_path=upload_path)\n",
    "    return uploaded_name\n",
    "\n",
    "\n",
    "def add_preds_and_upload_dataset_file(\n",
    "    local_model_tar: str, net_type: str, df: pd.DataFrame, client: Client, pred_col: str, split: str\n",
    "):\n",
    "    \"\"\"Make predictions and upload.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[pred_col] = predict_from_tarfile(local_model_tar, df)\n",
    "    dataset_split = f\"{net_type}_{split}\"\n",
    "    return upload_dataset_file(client, df, dataset_split)\n",
    "\n",
    "\n",
    "def join_rime_stress_tests(\n",
    "    stress_test_job: Job\n",
    ") -> dict:\n",
    "    \"\"\"Initiate a RIME test run.\"\"\"\n",
    "    stress_test_job_run = stress_test_job.get_status(\n",
    "        verbose=True, wait_until_finish=True, poll_rate_sec=15\n",
    "    )\n",
    "    if stress_test_job_run[\"status\"] == \"JOB_STATUS_SUCCEEDED\":\n",
    "        return stress_test_job_run\n",
    "    raise Exception(f\"Stress test job run failing. {stress_test_job_run}\")\n",
    "\n",
    "\n",
    "def evaluate_model(net_type: str, local_model_tar: str, train: pd.DataFrame, test: pd.DataFrame) -> Job:\n",
    "    \"\"\"Train and Evaluate a Classifier.\"\"\"\n",
    "    test_config = {\n",
    "        \"run_name\": f\"{net_type} SageMaker Experiment\",\n",
    "        \"data_info\": {\n",
    "            \"label_col\": LABEL_COL,\n",
    "            \"pred_col\": PRED_COL,\n",
    "            \"ref_path\": add_preds_and_upload_dataset_file(local_model_tar, net_type, train, client, PRED_COL, \"ref\"),\n",
    "            \"eval_path\": add_preds_and_upload_dataset_file(local_model_tar, net_type, test, client, PRED_COL, \"eval\")\n",
    "        },\n",
    "        \"model_info\": {\n",
    "            \"path\": upload_model_from_tarfile(local_model_tar, net_type, client)\n",
    "        },\n",
    "        \"model_task\": \"Binary Classification\"\n",
    "    }\n",
    "    return client.start_stress_test(\n",
    "            test_config, project_id=project.project_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365f9328",
   "metadata": {},
   "source": [
    "## Train and Evaluate Models\n",
    "\n",
    "After uploading the datasets to the testing cluster, train models and start stress test jobs to compare behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30052589",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Launch Stress Test Jobs\n",
    "for estimator_dict in estimators:\n",
    "    net_type, local_model_tar = estimator_dict['net_type'], estimator_dict['local_model_tar']\n",
    "    estimator_dict['stress_job'] = evaluate_model(net_type, local_model_tar, train=train, test=test)\n",
    "\n",
    "# Wait for jobs to finish\n",
    "for estimator_dict in tqdm(estimators):\n",
    "    estimator_dict[\"status\"] = join_rime_stress_tests(estimator_dict[\"stress_job\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07d70d2",
   "metadata": {},
   "source": [
    "## Reviewing the Stress Tests\n",
    "\n",
    "Stress tests are grouped into categories that measure various aspects of model robustness (model behavior, distribution drift, abnormal input, transformations, adversarial attacks, data cleanliness). Suggestions to improve your model are aggregated on the category level as well. Tests are ranked by default by a shared severity metric. Clicking on an individual test surfaces more detailed information.\n",
    "\n",
    "You can view the detailed results in the UI by running the below cell and redirecting to the generated links. This page shows granular results for a given AI Stress Test run.\n",
    "\n",
    "Additionally, you can compare the trained models by navigating to the project page and clicking the \"Compare\" button.\n",
    "\n",
    "We have also added the RIME metrics to the MLFlow experiments. To view, click the \"Experiment\" icon at the top right of this notebook or navigate to the appropriate experiments section using the command bar on the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401f0257",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators[0]['stress_job'].get_test_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bd434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators[1]['stress_job'].get_test_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e199b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators[2]['stress_job'].get_test_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beca1d4",
   "metadata": {},
   "source": [
    "## **Deploy to Production and Create the AI Firewall**\n",
    "\n",
    "Suppose that we wish select the most \"performant\" model based on the number of RIME tests it passes. We will select and deploy the associated model wrapped with the AI Firewall. The AI Firewall operates on both a datapoint and batch level. It automatically protects your model in real-time from “bad” incoming data and also alerts on statistically significant distributional drift. \n",
    "\n",
    "In this scenario, the data scientist is short on time and decided to deploy the existing model to production. The data scientist also creates a firewall to monitor the model and data behavior. The AI Firewall is automatically configured based on the failures identified by AI Stress testing to protect the tested model in Production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac2d2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model_with_highest_pass_rate(estimators: List[dict]) -> dict:\n",
    "    \"\"\"Select model naively using overall test pass rate.\"\"\"\n",
    "    pass_rates = []\n",
    "    for run_result in estimators:\n",
    "        result_df: pd.DataFrame = run_result['stress_job'].get_test_run().get_result_df()\n",
    "        pass_rate = result_df['metrics.summary_counts.pass'] / result_df['metrics.summary_counts.total']\n",
    "        pass_rates.append(pass_rate)\n",
    "    return estimators[np.argmax(pass_rates)]\n",
    "\n",
    "def deploy_model(model_package_arn: str, role: str, instance_type: str = 'ml.t2.medium'):\n",
    "    \"\"\"Deploy a model from the registry.\"\"\"\n",
    "    model = ModelPackage(role=role, \n",
    "                         model_package_arn=model_package_arn, \n",
    "                         sagemaker_session=sagemaker.Session())\n",
    "    endpoint_name = \"RIME-fraud-inference-pipeline-endpoint\" \n",
    "    return model.deploy(initial_instance_count=1, instance_type=instance_type, endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc58281a",
   "metadata": {},
   "source": [
    "Next, the data scientist creates the RIME Firewall to connect with the production endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb91912",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_run = select_model_with_highest_pass_rate(estimators)\n",
    "production_model = best_model_run[\"local_model_tar\"]\n",
    "test_run = best_model_run['stress_job'].get_test_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8d135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_run = select_model_with_highest_pass_rate(estimators)\n",
    "production_model = best_model_run[\"local_model_tar\"]\n",
    "test_run = best_model_run['stress_job'].get_test_run()\n",
    "try:\n",
    "    firewall = project.create_firewall(\"SageMaker Firewall\", bin_size=\"day\", test_run_id=test_run.test_run_id)\n",
    "except ValueError:\n",
    "    print(f\"Updating to target model from stress test {test_run.test_run_id}\")\n",
    "    firewall = project.get_firewall()\n",
    "    firewall.update_firewall_stress_test_run(test_run.test_run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f94872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model\n",
    "# SageMaker deployment is frequently too slow (>1.5 hrs)\n",
    "# So we will continue to use the model files\n",
    "do_deployment = False\n",
    "if do_deployment:\n",
    "    role = get_execution_role()\n",
    "    instance_type = 'ml.t2.medium'\n",
    "    production_model = deploy_model(best_model_run['model_package_arn'], role, instance_type=instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c456f228",
   "metadata": {},
   "source": [
    " ## Uploading a Batch of Production Data & Model Predictions to Firewall\n",
    "\n",
    "The fraud detection model has been in production for 30 days. Production data and model predictions have been collected and stored for the past 30 days. Now, we will use Firewall to track how the model performed across the last 30 days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c5daff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_firewall_on_bin(production_model_tar: str, df: pd.DataFrame, firewall: Firewall, group_split: str) -> Job:\n",
    "    \"\"\"Run the firewall on a bin of data.\"\"\"\n",
    "    # A real deployment would already have predictions associated with the data, but we will \n",
    "    # make the predictions here for the sake of convenience\n",
    "    incremental_config = {\n",
    "        \"eval_path\": add_preds_and_upload_dataset_file(production_model_tar, \"prod\",  df, client, PRED_COL, group_split),\n",
    "        \"timestamp_col\": \"timestamp\"\n",
    "    }\n",
    "    ct_job = firewall.start_continuous_test(test_run_config=incremental_config, disable_firewall_events=False)\n",
    "    ct_job.get_status(verbose=True, wait_until_finish=True, poll_rate_sec=15.0)    \n",
    "    return ct_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff74f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load production data \n",
    "incremental_df = pd.read_csv(get_s3_path(path_dict[\"incremental\"])).drop(columns=[\"preds\"])\n",
    "\n",
    "# Group by week to simulate running weekly batches\n",
    "timestamps_dt = pd.DatetimeIndex(incremental_df[TIMESTAMP_COL])\n",
    "for name, group in incremental_df[TIMESTAMP_COL].groupby(timestamps_dt.to_period(\"W-SUN\").to_timestamp()):\n",
    "    group_split = name.split(' ')[0]\n",
    "    print(f\"Running batch for week {name}\")\n",
    "    batch_df = incremental_df.loc[group.index]\n",
    "    job = run_firewall_on_bin(production_model, batch_df, firewall, group_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b98d3de",
   "metadata": {},
   "source": [
    "## **Firewall CT Results**\n",
    "\n",
    "The AI Firewall’s Continuous Tests operate at the batch level and provide a mechanism to monitor the health of ML deployments in production. They allow the user to understand when errors begin to occur and surface the underlying drivers of such errors. \n",
    "\n",
    "You can explore the results in the UI by running the below cell and redirecting to the generated link.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ca41a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up any and all deployments if performed\n",
    "endpoint_prefix = \"RIME-fraud-inference-\"\n",
    "sm_client = sagemaker_session.boto_session.client(\"sagemaker\")\n",
    "for endpoint in sm_client.list_endpoints()['Endpoints']:\n",
    "    if endpoint['EndpointName'].startswith(endpoint_prefix):\n",
    "        sm_client.delete_endpoint(EndpointName=endpoint['EndpointName'])\n",
    "\n",
    "for endpoint_config in sm_client.list_endpoint_configs()['EndpointConfigs']:\n",
    "    sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config['EndpointConfigName'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
