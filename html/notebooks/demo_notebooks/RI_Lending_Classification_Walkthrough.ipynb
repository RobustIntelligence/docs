{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K05kWwxAT9uN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **RI Lending Classification Walkthrough**\n",
    "\n",
    "> ▶️ **Try this in Colab!** Run the [RI Lending Classification Walkthrough in Google Colab](https://colab.research.google.com/github/RobustIntelligence/docs/blob/2.4-stable/notebooks/demo_notebooks/RI_Lending_Classification_Walkthrough.ipynb). \n",
    "\n",
    "You are a data scientist at a Bank. The data science team has been tasked with implementing a binary classification model to predict whether an individual will default on a loan. The goal of this project is two-fold: we want to monitor how that model performs over time as well as ensure that the model is compliant with financial regulations. In order to accomplish the latter, we will be testing whether the model is biased against certain protected features. One could imagine such models being used downstream for various purposes, such as loan approval or funding allocation. A biased model could yield disadvantageous outcomes for protected groups. For instance, we may find that an individual with a specific race or race/gender combination causes the model to consistently predict a higher probability of them defaulting, causing a higher rate of loan rejection.\n",
    "    \n",
    "\n",
    "In this Notebook Walkthrough, we will walkthrough our core products of **AI Stress Testing** and **AI Continuous Testing** in a *Bias and Fairness* setting. RIME AI Stress Testing allows you to test the developed model and datasets. With this compliance-focused setting, you will be able to verify your AI model for bias and fairness issues. RIME AI Continuous Testing allows you to continue monitoring your deployed model for bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APMTB6F7byjr",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **Install Dependencies, Import Libraries and Download Data**\n",
    "Run the cell below to install libraries to recieve data, install our SDK, and load analysis libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Qr8pHVOcNz7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install rime-sdk &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f03qjornDwbr",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from rime_sdk import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UnbHm8sJYHUD",
    "outputId": "b8ec9a17-2b4c-482b-ff1d-363c1530b384",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install https://github.com/RobustIntelligence/ri-public-examples/archive/master.zip \n",
    "from ri_public_examples.download_files import download_files\n",
    "download_files('tabular-2.0/lending', 'lending')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90zo7hdkmKyW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **Establish the RIME Client**\n",
    "\n",
    "To get started, provide the API credentials and the base domain/address of the RIME service. You can generate and copy an API token from the API Access Tokens Page under Workspace settings. For the domian/address of the RIME service, contact your admin. \n",
    "\n",
    "![img_1](https://drive.google.com/uc?id=1vMDhZii8yq22iuqSM8-Vqt3sZ2F3tPyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ns6m8I-qsCzF",
    "outputId": "266cee4c-bcab-4d19-a6bc-eb6789e62202",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "API_TOKEN = '' # PASTE API_KEY \n",
    "CLUSTER_URL = '' # PASTE DEDICATED DOMAIN OF RIME SERVICE (eg: rime.stable.rbst.io)\n",
    "AGENT_ID = '' # PASTE AGENT_ID IF USING AN AGENT THAT IS NOT THE DEFAULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(CLUSTER_URL, API_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIjQx9KUvWCF",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **Create a New Project**\n",
    "\n",
    "You can create projects in RIME to organize your test runs. Each project represents a workspace for a given machine learning task. It can contain multiple candidate models, but should only contain one promoted production model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YEghhVY8vy6K",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "description = (\n",
    "    \"Run Stress Testing and Continuous Testing on a tabular\"\n",
    "    \" binary classification model and dataset.\"\n",
    "    \" Demonstration uses the Lending Club dataset, which is used\"\n",
    "    \" to predict whether someone will repay a loan.\"\n",
    ")\n",
    "project = client.create_project(\n",
    "    name=\"Lending Classification Continuous Testing Demo\", \n",
    "    description=description,\n",
    "    model_task=\"MODEL_TASK_BINARY_CLASSIFICATION\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "4CQU6vkeDwbz",
    "outputId": "425104d7-8340-4957-ab40-0d581af953e7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "project.project_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXZ6e7kiTit6",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Go back to the UI to see the new `Lending Classification Continuous Testing Demo` project.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lsxeLehwN7Y",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **Training a Lending Model and Uploading the Model + Datasets**\n",
    "\n",
    "Let's first take a lot at what the dataset looks like. We can observe that the data consists of a mix of categorical and numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('lending/data/ref.csv').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Mbokt95ZPfe",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For this demo, we are going to use a pretrained CatBoostClassifier Model. \n",
    "\n",
    "The model predicts whether an individual will default on their loan.\n",
    "\n",
    "We now want to kick off RIME Stress Tests, in a compliance setting, that will help us determine if the model is biased against protected attributes. In order to do this, we will upload this pre-trained model, the reference dataset the model was trained on, and the evaluation dataset the model was evaluated on to an S3 bucket that can be accessed by RIME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U1mcyn9JwdWQ",
    "outputId": "59b08804-3542-43f5-f5ba-a89078951e78",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "upload_path = \"ri_public_examples_lending\"\n",
    "\n",
    "model_s3_dir = client.upload_directory(\n",
    "    Path('lending/models'), upload_path=upload_path\n",
    ")\n",
    "model_s3_path = model_s3_dir + \"/model.py\"\n",
    "\n",
    "ref_s3_path = client.upload_file(\n",
    "    Path('lending/data/ref.csv'), upload_path=upload_path\n",
    ")\n",
    "eval_s3_path = client.upload_file(\n",
    "    Path('lending/data/eval.csv'), upload_path=upload_path\n",
    ")\n",
    "\n",
    "ref_preds_s3_path = client.upload_file(\n",
    "    Path(\"lending/data/ref_preds.csv\"), upload_path=upload_path\n",
    ")\n",
    "eval_preds_s3_path = client.upload_file(\n",
    "    Path(\"lending/data/eval_preds.csv\"), upload_path=upload_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data and model are uploaded to S3, we can register them to RIME. In this bias and fairness setting, we require some additional information when registering datasets. Within the `data_params` parameter in the registering function, we include the protected features present in the data such that we can run our bias and fairness tests on those features. Once the datasets and models are registered, we can refer to these resources using their RIME-generated ID's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "dt = str(datetime.now())\n",
    "\n",
    "# Note: models and datasets need to have unique names.\n",
    "model_id = project.register_model_from_path(f\"model_{dt}\", model_s3_path, agent_id=AGENT_ID)\n",
    "\n",
    "ref_dataset_id = project.register_dataset_from_file(f\"ref_dataset_{dt}\",\n",
    "                                                    ref_s3_path,\n",
    "                                                    data_params={\"label_col\": \"loan_status\",\n",
    "                                                                 \"protected_features\": [\"sex\", \"race\", \"addr_state\"]\n",
    "                                                                },\n",
    "                                                   agent_id=AGENT_ID)\n",
    "eval_dataset_id = project.register_dataset_from_file(f\"eval_dataset_{dt}\",\n",
    "                                                     eval_s3_path,\n",
    "                                                     data_params={\"label_col\": \"loan_status\",\n",
    "                                                                  \"protected_features\": [\"sex\", \"race\", \"addr_state\"]\n",
    "                                                                 },\n",
    "                                                    agent_id=AGENT_ID)\n",
    "ref_pred_id = project.register_predictions_from_file(ref_dataset_id, model_id, ref_preds_s3_path, agent_id=AGENT_ID)\n",
    "\n",
    "eval_pred_id = project.register_predictions_from_file(eval_dataset_id, model_id, eval_preds_s3_path, agent_id=AGENT_ID)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCasNMsy0Avt",
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## **Running a Stress Test with Bias and Fairness**\n",
    "\n",
    "AI Stress Tests allow you to test your data and model before deployment. They are a comprehensive suite of hundreds of tests that automatically identify implicit assumptions and weaknesses of pre-production models. Each stress test is run on a single model and its associated reference and evaluation datasets. \n",
    "\n",
    "To run Stress Tests with the Bias & Fairness mode, there are two main changes to make. The first has been done already, namely specifying a set of `protected_features` in the `data_param` parameters of both datasets. The protected features are the specific features that you want Stress Tests to run over in order to test your model for signs of bias. Additionally, you will want to specify the Bias and Fairness Category in stress test config. This category does not run by default so specifying as such is necessary:\n",
    "\n",
    "```python\n",
    "stress_test_config = {\n",
    "        # rest of configuration ...\n",
    "        \"categories\": [\"TEST_CATEGORY_TYPE_BIAS_AND_FAIRNESS\"]\n",
    "}\n",
    "```\n",
    "\n",
    "Note how the \"categories\" field contains \"Bias and Fairness\", along with other categories to test Security and Operational Risk.\n",
    "\n",
    "\n",
    "Below is a sample configuration of how to setup and run a RIME Stress Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MA-mVn3pzVA1",
    "outputId": "c75f28b8-93a7-44e8-d451-1fcec26f7ff6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stress_test_config = {\n",
    "    \"data_info\": {\n",
    "        \"ref_dataset_id\": ref_dataset_id,\n",
    "        \"eval_dataset_id\": eval_dataset_id\n",
    "    },\n",
    "    \"model_id\": model_id,\n",
    "    \"run_name\": \"Loan Default Prediction - Lending Club\",\n",
    "    \"categories\": [\n",
    "            \"TEST_CATEGORY_TYPE_ADVERSARIAL\",\n",
    "            \"TEST_CATEGORY_TYPE_SUBSET_PERFORMANCE\",\n",
    "            \"TEST_CATEGORY_TYPE_TRANSFORMATIONS\",\n",
    "            \"TEST_CATEGORY_TYPE_BIAS_AND_FAIRNESS\",\n",
    "            \"TEST_CATEGORY_TYPE_ABNORMAL_INPUTS\",\n",
    "            \"TEST_CATEGORY_TYPE_DATA_CLEANLINESS\"\n",
    "    ]\n",
    "}\n",
    "stress_job = client.start_stress_test(test_run_config=stress_test_config, project_id=project.project_id, agent_id=AGENT_ID)\n",
    "stress_job.get_status(verbose=True, wait_until_finish=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFB5nVEaTtnB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Wait for a couple minutes and your results will appear in the UI.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovrKDoifIxIm",
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## **Stress Test Results**\n",
    "\n",
    "Stress tests are grouped into categories that measure various aspects of model robustness (model behavior, distribution drift, abnormal input, transformations, adversarial attacks, bias and fairness). Suggestions to improve your model are aggregated on the category level as well. Tests are ranked by default by a shared severity metric. Clicking on an individual test surfaces more detailed information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2jQYUDmknsU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![img_3](https://drive.google.com/uc?id=1dYI1YLTZqOMcuk_KSRfJOavdS_26CKC3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Similar to running RIME Stress Tests in the default setting, we surface an overall distribution of test severities, model metrics, as well as key insights to the right. This test suite comprises a selection of Bias and Fairness tests over any protected features, Attack tests over all features,\n",
    "and Abnormal Inputs tests over all features. These tests align with financial regulatory standards. Lets take a closer look at the Demographic Parity test:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2jQYUDmknsU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![img_4](https://drive.google.com/uc?id=1sBlvaUHu_Gwut_hmFN0tR_WJmcbpfTLZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This test is commonly known as the demographic parity or statistical parity test in fairness literature. This test checks whether the model performs equally well across a given subset of rows as it does across the whole dataset. The key detail displays the performance difference between the lowest performing subset and the overall population. The test first splits the dataset into various subsets depending on the quantiles of a given feature column. If the feature is categorical, the data is split based on the feature values. We then test whether the Positive Prediction Rate of model predictions within a specific subset is significantly different than the model prediction Positive Prediction Rate over the entire 'population'.\n",
    "\n",
    "We can see that the model fails the demographic parity test for two of the three protected features that we configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "h2iYOcGiI16-",
    "outputId": "2f6c190f-9081-4b65-f29b-8743879b1de6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_run = stress_job.get_test_run()\n",
    "test_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_case_result = test_run.get_result_df()\n",
    "test_case_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGivw_Weookf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **Programmatically Querying the Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HB4A2GouWysL",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "RIME not only provides you with an intuitive UI to visualize and explore these results, but also allows you to programmatically query these results. This allows customers to integrate with their MLOps pipeline, log results to experiment management tools like MLFlow, bring automated decision making to their ML practicies, or store these results for future references. \n",
    "\n",
    "Run the below cell to programmatically query the results. The results are outputed as a pandas dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTWLakM8VQj6",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Access results at the a test run overview level**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dgawCIZWLFST",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_run_result = test_run.get_result_df()\n",
    "test_run_result.to_csv(\"Lending_Test_Run_Results.csv\")\n",
    "test_run_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUUcNh7XVUPl",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Access detailed test results at each individual test cases level.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2qh2linVIt9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_case_result = test_run.get_test_cases_df()\n",
    "test_case_result.to_csv(\"Lending_Test_Case_Results.csv\")\n",
    "test_case_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovrKDoifIxIm",
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## **Deploy to Production and set up AI Continuous Testing**\n",
    "\n",
    "Once you have identified the best stress test run, you can deploy continuous testing for the associated model. Continuous Testing operates on both a datapoint and batch level. It automatically protects your model in real-time from “bad” incoming data and also alerts on statistically significant distributional drift. \n",
    "\n",
    "In this scenario, the data scientist is short on time and decided to deploy the existing model to production. The data scientist also sets up Continuous Testing to monitor the model. Continuous Testing is automatically configured based on the failures identified by AI Stress testing to protect the tested model in Production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "tt3uZvDtAbov",
    "outputId": "277b3104-942b-4a62-fa82-21c248ef6422",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "# Set up Continuous Testing using previously registered model and dataset IDs.\n",
    "ct = project.create_ct(model_id, ref_dataset_id, timedelta(days=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0Lw2YBXBkb4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **Uploading a Batch of Production Data & Model Predictions to Continuous Testing**\n",
    "\n",
    "The lending model has been in production for 30 days. Production data and model predictions have been collected and stored for the past 30 days. Now, we will use Continuous Testing to track how the model performed across the last 30 days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7McYwfDUztC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Upload an Incremental Batch of Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "tt3uZvDtAbov",
    "outputId": "277b3104-942b-4a62-fa82-21c248ef6422",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prod_s3_path = client.upload_file(\n",
    "    Path('lending/data/incremental.csv'), \n",
    "    upload_path=upload_path\n",
    ")\n",
    "prod_dataset_id = project.register_dataset_from_file(\n",
    "    f\"prod_dataset_{dt}\", \n",
    "    prod_s3_path, \n",
    "    data_params={\"label_col\": \"loan_status\", \n",
    "                 \"protected_features\": [\"sex\", \"race\", \"addr_state\"],\n",
    "                 \"timestamp_col\": \"timestamp\"},\n",
    ")\n",
    "prod_preds_s3_path = client.upload_file(\n",
    "    Path('lending/data/incremental_preds.csv'), \n",
    "    upload_path=upload_path\n",
    ")\n",
    "project.register_predictions_from_file(\n",
    "    prod_dataset_id, model_id, prod_preds_s3_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the Bias and Fairness category in Continuous Testing, you will want to specify the Bias and Fairness Category as a continuous test category. This category does not run by default so specifying along with other categories as such is necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.update_continuous_test_categories([\"TEST_CATEGORY_TYPE_MODEL_PERFORMANCE\",\n",
    "                                           \"TEST_CATEGORY_TYPE_SUBSET_PERFORMANCE_DEGRADATION\",\n",
    "                                           \"TEST_CATEGORY_TYPE_ABNORMAL_INPUTS\",\n",
    "                                           \"TEST_CATEGORY_TYPE_DRIFT\",\n",
    "                                           \"TEST_CATEGORY_TYPE_BIAS_AND_FAIRNESS\"\n",
    "                                          ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-ooaBT-U4yr",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Run Continuous Testing over Batch of Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "tt3uZvDtAbov",
    "outputId": "277b3104-942b-4a62-fa82-21c248ef6422",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ct_job = ct.start_continuous_test(prod_dataset_id, agent_id=AGENT_ID)\n",
    "ct_job.get_status(verbose=True, wait_until_finish=True)\n",
    "ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifxKz_p6Uc63",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Wait for a couple minutes and your results will appear in the UI.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmpxUnaLMn-u",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "## **Continuous Testing Overview**\n",
    "\n",
    "The page is the mission control for your model’s production deployment health. In it, you can see the status of continuous test runs, and see their metrics changes over time.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img_5](https://drive.google.com/uc?id=1uT3q4pakoQSPhRf9HfxS5tyL16c3dDpx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzMea8BUFOT1",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **Continuous Test Results**\n",
    "\n",
    "The Continuous Tests operate at the batch level and provide a mechanism to monitor the health of ML deployments in production. They allow the user to understand when errors begin to occur and surface the underlying drivers of such errors. \n",
    "\n",
    "You can explore the results in the UI by running the below cell and redirecting to the generated link.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qndn2zUMLhJo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ct"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RI_Lending_Classification_Walkthrough.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
