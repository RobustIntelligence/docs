{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RI Generative Stress Test Walkthrough**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this walkthrough, we will run RIME's Generative AI Stress Testing on OpenAI's Chat GPT model to demonstrate how RIME can be used with generative models. The data consists of sample text-generated information that a Chat GPT model would use as input and output which includes a title, context, question, answer, and prompt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Install Dependencies, Import Libraries and Download Data**\n",
    "Run the cell below to install libraries to receive data, install our SDK, and load analysis libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rime-sdk &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from rime_sdk import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to download and unzip a generated dataset and pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install https://github.com/RobustIntelligence/ri-public-examples/archive/master.zip    \n",
    "from ri_public_examples.download_files import download_files\n",
    "\n",
    "download_files('generative/question_answering', 'question_answering') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Establish the RIME Client**\n",
    "\n",
    "To get started, provide the API credentials and the base domain/address of the RIME service. You can generate and copy an API token from the API Access Tokens Page under Workspace settings. For the domian/address of the RIME service, contact your admin. \n",
    "\n",
    "![API_token](https://drive.google.com/uc?id=1zF1inyDbU2Q08SW9Ans2eVw0LqUnA2uP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_TOKEN = '' # PASTE API_KEY \n",
    "CLUSTER_URL = '' # PASTE DEDICATED DOMAIN OF RIME SERVICE (eg: rime.stable.rbst.io)\n",
    "AGENT_ID = '' # PASTE AGENT_ID IF USING AN AGENT THAT IS NOT THE DEFAULT\n",
    "\n",
    "client = Client(CLUSTER_URL, API_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Create a New Project**\n",
    "\n",
    "You can create projects in RIME to organize your test runs. Each project represents a workspace for a given machine learning task. It can contain multiple candidate models, but should only contain one promoted production model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = (\n",
    "    \"Run Generative Stress Testing on a\"\n",
    "    \" generative model and dataset. Demonstration uses\"\n",
    "    \" a dataset composed of short scenarios that are each\"\n",
    "    \" identifed by title and characterized by context,\"\n",
    "    \" a question, an answer, and a prompt.\"\n",
    ")\n",
    "project = client.create_project(\n",
    "    name=\"GAI Security Demo\", \n",
    "    description=description, \n",
    "    model_task=\"MODEL_TASK_QUESTION_ANSWERING\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Go back to the UI to see the new `Generative Model Stress Test Demo` project.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Register and Upload the Model + Dataset**\n",
    "\n",
    "We now want to kick off RIME Stress Tests that will help us evaluate the model in further depth beyond basic performance metrics like accuracy, precision, recall. In order to do this, we will upload this pre-trained model and the evaluation dataset the model was evaluated on to an S3 bucket that can be accessed by RIME. We also need to upload a fact sheet that will later be used to run the stress test. Futhermore, we'll need to register them with RIME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_path = \"ri_public_examples_generative\"\n",
    "\n",
    "model_s3_dir = client.upload_directory(\n",
    "    Path('question_answering/models'), upload_path=upload_path\n",
    ")\n",
    "model_s3_path = model_s3_dir + \"/chat_gpt_model.py\"\n",
    "\n",
    "eval_s3_path = client.upload_file(\n",
    "    Path('question_answering/data/squad_v2_test_with_labels.json'), upload_path=upload_path\n",
    ")\n",
    "\n",
    "fact_sheet_path = client.upload_file(\n",
    "    Path('question_answering/data/fact_sheet.txt'), upload_path=upload_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data and model are uploaded to S3, we can register them to RIME. Once they're registered, we can refer to these resources using their RIME-generated ID's. We also need to obtain an integration ID from the workspace, which is then used to register the model and aid in stress test configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "dt = str(datetime.now())\n",
    "\n",
    "integration_id = client.create_integration(\n",
    "    workspace_id=\"\", # PASTE WORKSPACE ID FROM INFO BUTTON ON WORKSPACE OVERVIEW PAGE\n",
    "    name=f\"GAI_walkthrough_{dt}\",\n",
    "    integration_type=\"INTEGRATION_TYPE_CUSTOM\", \n",
    "    integration_schema=[\n",
    "        {\n",
    "            \"name\": \"OPENAI_API_KEY\",\n",
    "            \"sensitivity\": \"VARIABLE_SENSITIVITY_WORKSPACE_SECRET\",\n",
    "            \"value\": \"\", # FILL IN YOUR OPENAI API KEY HERE\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Note: models and datasets need to have unique names.\n",
    "model_id = project.register_model(\n",
    "    name=f\"model_{dt}\",\n",
    "    model_config={\"generative_language_model\": {\n",
    "        \"model_path\": model_s3_path,\n",
    "        \"system_prompt\": \"I am ChatGPT, a large language model trained by OpenAI, based on the GPT-3.5 architecture.\\nKnowledge cutoff: 2021-09\\nCurrent date: 2023-03-28\"\n",
    "    }},    \n",
    "    model_endpoint_integration_id=integration_id,\n",
    "    skip_validation=True\n",
    ")\n",
    "\n",
    "data_params = {\n",
    "    \"label_col\": \"answer\",\n",
    "    \"prompt_col\": \"prompt\",\n",
    "    \"text_features\": [\n",
    "        \"context\",\n",
    "        \"question\"]\n",
    "}\n",
    "eval_dataset_id = project.register_dataset(\n",
    "    name=f\"eval_dataset_{dt}\",\n",
    "    data_config={\n",
    "        \"connection_info\": {\"data_file\": {\"path\": eval_s3_path}},\n",
    "        \"data_params\":data_params\n",
    "    }\n",
    ")\n",
    "\n",
    "tests_config = {\n",
    "    \"row_wise_factual_inconsistency\": {\"fact_sheet_path\": fact_sheet_path},\n",
    "}\n",
    "test_suite_config = {\"individual_tests_config\": tests_config}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Running a Generative Stress Test**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Generative Stress Test allows you to test your data and model before deployment. They are a comprehensive suite of hundreds of tests that automatically identify implicit assumptions and weaknesses of pre-production models. Each stress test is run on a single model and displays corresponding results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a sample configuration of how to setup and run a RIME Generative Stress Test. This configuration specifies relevant metadata for our dataset and model to run the stress test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_stress_test_config = {\n",
    "    \"run_name\":\"GPT-3.5\", \n",
    "    \"model_task\":\"Question Answering\",\n",
    "    \"model_id\":model_id,\n",
    "    \"eval_dataset_id\":eval_dataset_id,\n",
    "    \"integration_id\":{\"uuid\": integration_id},\n",
    "    \"run_time_info\":{\"explicit_errors\":False},\n",
    "    \"test_suite_config\":test_suite_config,\n",
    "    \"categories\": [\n",
    "        \"TEST_CATEGORY_TYPE_ADVERSARIAL\",\n",
    "        \"TEST_CATEGORY_TYPE_DATA_POISONING_DETECTION\",\n",
    "        \"TEST_CATEGORY_TYPE_SUBSET_PERFORMANCE\",\n",
    "        \"TEST_CATEGORY_TYPE_TRANSFORMATIONS\",\n",
    "        \"TEST_CATEGORY_TYPE_EVASION_ATTACK_DETECTION\",\n",
    "        \"TEST_CATEGORY_TYPE_MODEL_ALIGNMENT\",\n",
    "        \"TEST_CATEGORY_TYPE_FACTUAL_AWARENESS\",\n",
    "        \"TEST_CATEGORY_TYPE_BIAS_AND_FAIRNESS\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "stress_job = client._start_generative_stress_test(\n",
    "    test_run_config=generative_stress_test_config,\n",
    "    project_id=project.project_id\n",
    ")\n",
    "stress_job.get_status(verbose=True, wait_until_finish=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generative Stress Test Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative Stress Test Results are grouped into categories that measure various aspects of generative model robustness (adverserial, transformations, model alignment, subset performance). These categories are what determine production readiness. Suggestions to improve your model are aggregated on the category level as well. Tests are ranked by default by a shared severity metric. Clicking on an individual test surfaces more detailed information.\n",
    "\n",
    "You can view the detailed results in the UI by running the below cell and redirecting to the generated link. This page shows granular results for a given AI Generative Stress Test run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run = stress_job.get_test_run()\n",
    "test_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Analyzing the Results**\n",
    "\n",
    "Below you can see a snapshot of the results. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stress_test.png](https://drive.google.com/uc?id=1Tyn3QHU7UM9EMgE3S7WngGSEW-2Mv4BR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that each test result category articulates status of the model with respect to the category. Under each category description is a list of any 'Data Requirements' for that category to run successfully.\n",
    "\n",
    "Here are the results of the \"Transformations\" tests. These tests augment your evaluation dataset with synthetic abnormal values to proactively test your pipeline’s error-handling behavior and measure the performance degradation caused by different types of abnormal values. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![transformations_category](https://drive.google.com/uc?id=17Ggnf2fw4p3SteTtfitd6l3ZuHxO6eMH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are exploring the \"Generative Synonym Swap\" test cases. This test measures the robustness of your model to synonym swap transformations. It does this by randomly swapping synonyms in the input string and measuring the difference in model outputs between the original input and the transformed input. For the first test case, we can see the SBERT Score, which measures similarity between the original and transformed outputs, is 0.64. This means that our model has changed its output significantly and is likely not robust to synonym swaps."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![datapoint_details.png](https://drive.google.com/uc?id=19KHVhZaYhm2uv8edqJdTJg0WNef0vMof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Programmatically Querying the Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RIME not only provides you with an intuitive UI to visualize and explore these results, but also allows you to programmatically query these results. This allows customers to integrate with their MLOps pipeline, log results to experiment management tools like MLFlow, bring automated decision making to their ML practices, or store these results for future references. \n",
    "\n",
    "Run the below cell to programmatically query the results. The results are outputed as a pandas dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Access results at the test run overview level**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run_result = test_run.get_result_df()\n",
    "test_run_result.to_csv(\"QA_Test_Run_Results.csv\")\n",
    "test_run_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Access detailed test results at each individual test cases level.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case_result = test_run.get_test_cases_df()\n",
    "test_case_result.to_csv(\"QA_Test_Run_Results.csv\")\n",
    "test_case_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Appendix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Uploading a Model to RIME**\n",
    "To be able to run certain tests, RIME needs query access to your model. To give RIME access, you'll need to write a Python file that implements the `generate(system_prompt: str, prompt: str) -> str` function, and upload that file (and any objects that it loads) to the platform. Here we provide an example model file, show you how to upload this file and the relevant model artifacts, and show you how to configure stress tests to use this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile question_answering/models/FLAN_T5_base.py\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load FLAN-T5-base model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "def generate(system_prompt: str, prompt: str) -> str:\n",
    "    \"\"\"Given a system prompt and prompt, return the response as a string.\"\"\"\n",
    "    text = f\"{system_prompt}\\n{prompt}\"\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "    outputs = model.generate(input_ids, max_new_tokens=100)\n",
    "    decoded = tokenizer.decode(outputs[0])\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "dt = str(datetime.now())\n",
    "\n",
    "appendix_model_dir = client.upload_directory(\n",
    "    Path('question_answering/models'), upload_path=upload_path\n",
    ")\n",
    "appendix_model_path = appendix_model_dir + \"/FLAN_T5_base.py\"\n",
    "# Note: models need to have unique names\n",
    "appendix_model_id = project.register_model(\n",
    "    name=f\"appendix_model_{dt}\",\n",
    "    model_config={\"generative_language_model\": {\n",
    "        \"model_path\": appendix_model_path,\n",
    "        \"system_prompt\": \"I am a Hugging Face language model.\"\n",
    "    }},\n",
    "    skip_validation=True\n",
    ")\n",
    "\n",
    "appendix_fact_sheet_path = client.upload_file(\n",
    "    Path('question_answering/data/fact_sheet.txt'), upload_path=upload_path\n",
    ")\n",
    "appendix_tests_config = {\n",
    "    \"row_wise_factual_inconsistency\": {\"fact_sheet_path\": appendix_fact_sheet_path},\n",
    "}\n",
    "appendix_test_suite_config = {\"individual_tests_config\": appendix_tests_config}\n",
    "\n",
    "stress_test_with_model_config = {\n",
    "    \"run_name\":\"Uploaded Model Example\", \n",
    "    \"model_task\":\"Question Answering\",\n",
    "    \"model_id\":appendix_model_id,\n",
    "    \"eval_dataset_id\":eval_dataset_id,\n",
    "    \"run_time_info\":{\"explicit_errors\":False},\n",
    "    \"test_suite_config\":appendix_test_suite_config,\n",
    "    \"categories\": [\n",
    "        \"TEST_CATEGORY_TYPE_ADVERSARIAL\",\n",
    "        \"TEST_CATEGORY_TYPE_DATA_POISONING_DETECTION\",\n",
    "        \"TEST_CATEGORY_TYPE_SUBSET_PERFORMANCE\",\n",
    "        \"TEST_CATEGORY_TYPE_TRANSFORMATIONS\",\n",
    "        \"TEST_CATEGORY_TYPE_EVASION_ATTACK_DETECTION\",\n",
    "        \"TEST_CATEGORY_TYPE_MODEL_ALIGNMENT\",\n",
    "        \"TEST_CATEGORY_TYPE_FACTUAL_AWARENESS\",\n",
    "        \"TEST_CATEGORY_TYPE_BIAS_AND_FAIRNESS\"\n",
    "    ]\n",
    "}\n",
    "stress_job = client._start_generative_stress_test(\n",
    "    test_run_config=stress_test_with_model_config,\n",
    "    project_id=project.project_id\n",
    ")\n",
    "stress_job.get_status(verbose=True, wait_until_finish=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
