{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDInylzdEcsB",
    "tags": []
   },
   "source": [
    "# **RI Movie Ratings Ranking Data Walkthrough** üé•\n",
    "\n",
    "> ‚ñ∂Ô∏è **Try this in Colab!** Run the [RI Movie Ratings Ranking Data Walkthrough in Google Colab](https://colab.research.google.com/github/RobustIntelligence/docs/blob/2.6-stable/notebooks/demo_notebooks/RI_Ranking_Walkthrough.ipynb). \n",
    "\n",
    "In this walkthrough, you are a data scientist tasked with training a recommendation system to predict whether or not a given user will upvote a movie. From experience, the team has found that the upstream data pipelines can be brittle, and want to use RIME to:\n",
    "\n",
    "1) Proactively test how vulnerable the model is to data failures during stress testing.\n",
    "\n",
    "2) To continuously monitor and track broken inputs in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iU8NMTH-WPz6"
   },
   "source": [
    "## **Install Dependencies, Import Libraries and Download Data**\n",
    "Run the cell below to install libraries to receive data, install our SDK, and load analysis libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mcq3snDVExtE"
   },
   "outputs": [],
   "source": [
    "!pip install rime-sdk &> /dev/null\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from rime_sdk import Client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install https://github.com/RobustIntelligence/ri-public-examples/archive/master.zip\n",
    "    \n",
    "from ri_public_examples.download_files import download_files\n",
    "\n",
    "download_files('tabular-2.0/ranking', 'ranking')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YilAj33W_5y-"
   },
   "source": [
    "## **Establish the RIME Client**\n",
    "\n",
    "To get started, provide the API credentials and the base domain/address of the RIME service. You can generate and copy an API token from the API Access Tokens Page under Workspace settings. For the domian/address of the RIME service, contact your admin. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90zo7hdkmKyW"
   },
   "source": [
    "![Image of getting an API token](https://ri-documentation-public-release-images.s3.us-west-2.amazonaws.com/en/2.6-stable/_static/images/demo_notebooks/common/api_access_tokens.png)![Image of creating an API token](https://ri-documentation-public-release-images.s3.us-west-2.amazonaws.com/en/2.6-stable/_static/images/demo_notebooks/common/create_token.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPOXz25dEiDQ"
   },
   "outputs": [],
   "source": [
    "API_TOKEN = '' # PASTE API_KEY \n",
    "CLUSTER_URL = '' # PASTE DEDICATED DOMAIN OF RIME SERVICE (eg: rime.stable.rbst.io)\n",
    "AGENT_ID = '' # PASTE AGENT_ID IF USING AN AGENT THAT IS NOT THE DEFAULT\n",
    "client = Client(CLUSTER_URL, API_TOKEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UePxMUdoAFOH"
   },
   "source": [
    "## **Create a New Project**\n",
    "\n",
    "You can create projects in RIME to organize your test runs. Each project represents a workspace for a given machine learning task. It can contain multiple candidate models, but should only contain one promoted production model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Of1LzaEOAGLz"
   },
   "outputs": [],
   "source": [
    "description = (\n",
    "    \"Run Stress Testing and AI Continuous Testing on a point-wise\"\n",
    "    \" tabular ranking model and dataset. Demonstration uses a\"\n",
    "    \" movie ranking dataset.\"\n",
    ")\n",
    "project = client.create_project(\n",
    "    name='Tabular Ranking Demo',\n",
    "    description=description,\n",
    "    model_task='MODEL_TASK_RANKING'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4Z7t5FSAQLW"
   },
   "source": [
    "**Go back to the UI to see the new Ranking Demo Project.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Uploading the Model + Datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NctleDA3P05v"
   },
   "source": [
    "Next, let's take a quick look at the training data (in this case, this was the data used to train the model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rS6ST9KhZPEp"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(Path('ranking/data/ref.csv'))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KdCnsy-RNh8Q"
   },
   "outputs": [],
   "source": [
    "upload_path = \"ri_public_examples_ranking\"\n",
    "\n",
    "model_s3_dir = client.upload_directory(\n",
    "    Path('ranking/models'), upload_path=upload_path\n",
    ")\n",
    "model_s3_path = model_s3_dir + \"/model_extras/model.py\"\n",
    "\n",
    "ref_s3_path = client.upload_file(\n",
    "    Path('ranking/data/ref.csv'), upload_path=upload_path\n",
    ")\n",
    "eval_s3_path = client.upload_file(\n",
    "    Path('ranking/data/eval.csv'), upload_path=upload_path\n",
    ")\n",
    "\n",
    "ref_preds_s3_path = client.upload_file(\n",
    "    Path(\"ranking/data/ref_preds.csv\"), upload_path=upload_path\n",
    ")\n",
    "eval_preds_s3_path = client.upload_file(\n",
    "    Path(\"ranking/data/eval_preds.csv\"), upload_path=upload_path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data and model are uploaded to S3, we can register them to RIME. Once they're registered, we can refer to these resources using their RIME-generated ID's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "dt = str(datetime.now())\n",
    "\n",
    "# Note: models and datasets need to have unique names.\n",
    "model_id = project.register_model_from_path(f\"model_{dt}\", model_s3_path, agent_id=AGENT_ID)\n",
    "\n",
    "data_params = {\n",
    "    \"label_col\": \"rank_label\",\n",
    "    \"ranking_info\": {\n",
    "      \"query_col\": \"query_id\"\n",
    "    },\n",
    "    \"protected_features\": [\"Director\", \"Cast1\"],\n",
    "}\n",
    "ref_dataset_id = project.register_dataset_from_file(\n",
    "    f\"ref_dataset_{dt}\", ref_s3_path, data_params=data_params, agent_id=AGENT_ID\n",
    ")\n",
    "eval_dataset_id = project.register_dataset_from_file(\n",
    "    f\"eval_dataset_{dt}\", eval_s3_path, data_params=data_params, agent_id=AGENT_ID\n",
    ")\n",
    "pred_params = {\"pred_col\": \"pred\"}\n",
    "project.register_predictions_from_file(\n",
    "    ref_dataset_id, model_id, ref_preds_s3_path, pred_params=pred_params, agent_id=AGENT_ID\n",
    ")\n",
    "project.register_predictions_from_file(\n",
    "    eval_dataset_id, model_id, eval_preds_s3_path, pred_params=pred_params, agent_id=AGENT_ID\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vtJnRnBBqkp"
   },
   "source": [
    "## **Running a Stress Test**\n",
    "\n",
    "AI Stress Tests allow you to test your data and model before deployment. They are a comprehensive suite of hundreds of tests that automatically identify implicit assumptions and weaknesses of pre-production models. Each stress test is run on a single model and its associated reference and evaluation datasets.\n",
    "\n",
    "Below is a sample configuration of how to setup and run a RIME Stress Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PfzT-SAhpi-x"
   },
   "outputs": [],
   "source": [
    "stress_test_config = {\n",
    "    \"run_name\": \"Movie Ranking\", \n",
    "    \"data_info\": {\n",
    "        \"ref_dataset_id\": ref_dataset_id, \n",
    "        \"eval_dataset_id\": eval_dataset_id,\n",
    "    }, \n",
    "    \"model_id\": model_id,\n",
    "    \"categories\": [\"TEST_CATEGORY_TYPE_ADVERSARIAL\", \"TEST_CATEGORY_TYPE_SUBSET_PERFORMANCE\", \"TEST_CATEGORY_TYPE_TRANSFORMATIONS\", \"TEST_CATEGORY_TYPE_BIAS_AND_FAIRNESS\"]\n",
    "}\n",
    "stress_job = client.start_stress_test(\n",
    "    stress_test_config, project.project_id, agent_id=AGENT_ID\n",
    ")\n",
    "stress_job.get_status(verbose=True, wait_until_finish=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2x2K9h1B6oX"
   },
   "source": [
    "## **Stress Test Results**\n",
    "\n",
    "Stress tests are grouped first by risk categories and then into categories that measure various aspects of model robustness (model behavior, distribution drift, abnormal input, transformations, adversarial attacks, data cleanliness). Key findings to improve your model are aggregated on the category level as well. Tests are ranked by default by a shared severity metric. Clicking on an individual test surfaces more detailed information. \n",
    "\n",
    "You can view the detailed results in the UI by running the below cell and redirecting to the generated link. This page shows granular results for a given AI Stress Test run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DAfObkwHB0xy"
   },
   "outputs": [],
   "source": [
    "test_run = stress_job.get_test_run()\n",
    "test_run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8ajygi1E71f"
   },
   "source": [
    "Stress testing should be used during model development to inform us about various issues with the data and model that we might want to address before the model is deployed. The information is presented in an incident management view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zKDo7TOYYds",
    "tags": []
   },
   "source": [
    "### **Analyzing the Results**\n",
    "\n",
    "Below you can see a snapshot of the results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2jQYUDmknsU"
   },
   "source": [
    "![Image of ST results for movie ranking model](https://ri-documentation-public-release-images.s3.us-west-2.amazonaws.com/en/2.6-stable/_static/images/demo_notebooks/ri_ranking_walkthrough/movie_ranking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oO26DNFoJQV"
   },
   "source": [
    "Here are the results of the Subset Performance tests. These tests can be thought as more detailed performance tests that identify subsets of underperformance. These tests help ensure that the model works equally well across different groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nc1TzYeqkn1k"
   },
   "source": [
    "![Image of ST operational risk results for movie ranking model](https://ri-documentation-public-release-images.s3.us-west-2.amazonaws.com/en/2.6-stable/_static/images/demo_notebooks/ri_ranking_walkthrough/movie_operational_risk.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EfnKH70fpmmu"
   },
   "source": [
    "Below we are exploring the \"Subset Mean Reciprocal Rank (MRR)\" test cases for the feature \"Votes\". We can see that even though the model has an overall MRR of 0.91, it performs poorly on certain subsets with low values of the \"Votes\" feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g91I82zRkoH5"
   },
   "source": [
    "![Image of ST MRR results for movie ranking model](https://ri-documentation-public-release-images.s3.us-west-2.amazonaws.com/en/2.6-stable/_static/images/demo_notebooks/ri_ranking_walkthrough/movie_mrr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzP97mzuab3m"
   },
   "source": [
    "## **Deploy to Production and set up Continuous Testing**\n",
    "\n",
    "Once you have identified the best stress test run, you can deploy the associated model and set up Continuous Testing in order to automatically detect ‚Äúbad‚Äù incoming data and statistically significant distributional drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "swErpZi1OE80"
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "ct_instance = project.create_ct(model_id, ref_dataset_id, timedelta(days=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0Lw2YBXBkb4"
   },
   "source": [
    "## **Uploading a Batch of Production Data & Model Predictions to Continuous Testing**\n",
    "\n",
    "The model has been in production for some time, and new production data and model predictions have been collected and stored. Now, we will use Continuous Testing to track how the model performed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7McYwfDUztC"
   },
   "source": [
    "**Upload the Latest Batch of Production Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zsmlbMIFUzAy"
   },
   "outputs": [],
   "source": [
    "dt = str(datetime.now())\n",
    "prod_s3_path = client.upload_file(\n",
    "    Path('ranking/data/test.csv'), \n",
    "    upload_path=upload_path,\n",
    ")\n",
    "prod_dataset_id = project.register_dataset_from_file(\n",
    "    f\"prod_dataset_{dt}\", \n",
    "    prod_s3_path, \n",
    "    data_params={\"timestamp_col\": \"timestamp\", \"protected_features\": [\"Director\", \"Cast1\"], **data_params},\n",
    "    agent_id=AGENT_ID\n",
    ")\n",
    "prod_preds_s3_path = client.upload_file(\n",
    "    Path('ranking/data/test_preds.csv'), \n",
    "    upload_path=upload_path,\n",
    ")\n",
    "project.register_predictions_from_file(\n",
    "    prod_dataset_id, \n",
    "    model_id,\n",
    "    prod_preds_s3_path, \n",
    "    pred_params=pred_params,\n",
    "    agent_id=AGENT_ID\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.update_ct_categories([\"TEST_CATEGORY_TYPE_MODEL_PERFORMANCE\",\n",
    "                                           \"TEST_CATEGORY_TYPE_SUBSET_PERFORMANCE_DEGRADATION\",\n",
    "                                           \"TEST_CATEGORY_TYPE_ABNORMAL_INPUTS\",\n",
    "                                           \"TEST_CATEGORY_TYPE_DRIFT\",\n",
    "                                           \"TEST_CATEGORY_TYPE_BIAS_AND_FAIRNESS\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-ooaBT-U4yr"
   },
   "source": [
    "**Run Continuous Testing over Batch of Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHxteK1gUklo"
   },
   "outputs": [],
   "source": [
    "ct_job = ct_instance.start_continuous_test(prod_dataset_id, agent_id=AGENT_ID)\n",
    "ct_job.get_status(verbose=True, wait_until_finish=True)\n",
    "ct_instance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifxKz_p6Uc63"
   },
   "source": [
    "**Wait for a couple minutes and your results will appear in the UI.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Querying Results from Continuous Testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Continuous Testing has been set up and data has been uploaded for processing, the user can query the results throughout the entire uploaded history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obtain All Detection Events**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = [d.to_dict() for m in ct_instance.list_monitors() for d in m.list_detected_events()]\n",
    "events_df = pd.DataFrame(events).drop([\"id\", \"project_id\", \"firewall_id\", \"event_object_id\", \"description_html\", \"last_update_time\"], axis=1)\n",
    "events_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzMea8BUFOT1"
   },
   "source": [
    "## **CT Results**\n",
    "\n",
    "The Continuous Tests operate at the batch level and provide a mechanism to monitor the health of ML deployments in production. They allow the user to understand when errors begin to occur and surface the underlying drivers of such errors. \n",
    "\n",
    "You can explore the results in the UI by running the below cell and redirecting to the generated link.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qndn2zUMLhJo"
   },
   "outputs": [],
   "source": [
    "ct_instance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycu422woQkoA",
    "tags": []
   },
   "source": [
    "### **Analyzing CT Results**\n",
    "\n",
    "**Failing Rows Rate stays steady (and low) over time** - In the below image, we can see that the Failing Rows Rate remains fairly low (<15%) over time, and does not trend upward significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFuGGg8uNtC2"
   },
   "source": [
    "![Image of model historical performance graph for movie ranking model](https://ri-documentation-public-release-images.s3.us-west-2.amazonaws.com/en/2.6-stable/_static/images/demo_notebooks/ri_ranking_walkthrough/production_perf_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPAgDrQkEA8d",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Summary:\n",
    "In this Notebook, RIME and the SDK helped with ingesting and investigating tabular pointwise ranking information which:\n",
    "\n",
    "‚úÖ Measured impact of failing tests on model performance\n",
    "\n",
    "‚úÖ Assisted with modeling and experiment tracking\n",
    "\n",
    "‚úÖ Identified root-cause analysis of underlying issues in data and model (e.g. Numerical Outliers and Bad Inputs)\n",
    "\n",
    "‚úÖ Continuously testing production data and model which enforced better ml integrity and posture (e.g. Highlighting changes in data schema, data malformations, cardinality changes, out of range values, missing values)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RI_Ranking_Walkthrough.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
